# TradingAgents-CN æ€§èƒ½ä¼˜åŒ–å»ºè®®

**æ—¥æœŸ**: 2026-01-18
**ç‰ˆæœ¬**: v1.0.0-preview
**ä¼˜åŒ–èŒƒå›´**: æ•°æ®è·å–ã€ç¼“å­˜ã€å¹¶å‘ã€æ•°æ®åº“

---

## ç›®å½•

1. [å½“å‰æ€§èƒ½é—®é¢˜](#1-å½“å‰æ€§èƒ½é—®é¢˜)
2. [ç¼“å­˜ä¼˜åŒ–](#2-ç¼“å­˜ä¼˜åŒ–)
3. [å¹¶å‘ä¼˜åŒ–](#3-å¹¶å‘ä¼˜åŒ–)
4. [æ•°æ®åº“ä¼˜åŒ–](#4-æ•°æ®åº“ä¼˜åŒ–)
5. [ä»£ç ä¼˜åŒ–](#5-ä»£ç ä¼˜åŒ–)
6. [æ¶æ„ä¼˜åŒ–](#6-æ¶æ„ä¼˜åŒ–)

---

## 1. å½“å‰æ€§èƒ½é—®é¢˜

### 1.1 å·²å‘ç°çš„é—®é¢˜

| é—®é¢˜ | ä¸¥é‡ç¨‹åº¦ | å½±å“ |
|------|---------|------|
| **æ•°æ®æºé‡å¤è°ƒç”¨** | ğŸ”´ é«˜ | åŒä¸€æ•°æ®å¤šæ¬¡è·å–ç›¸åŒæ•°æ®æº |
| **ç¼“å­˜æœªå……åˆ†åˆ©ç”¨** | ğŸŸ¡ ä¸­ | ç¼“å­˜å‘½ä¸­ç‡å¯èƒ½è¾ƒä½ |
| **ä¸²è¡Œæ•°æ®è·å–** | ğŸŸ¡ ä¸­ | å¤šä¸ªæ•°æ®æºä¸²è¡Œè°ƒç”¨è€Œéå¹¶è¡Œ |
| **æ•°æ®åº“æŸ¥è¯¢æœªä¼˜åŒ–** | ğŸŸ¡ ä¸­ | ç¼ºå°‘ç´¢å¼•å’ŒæŸ¥è¯¢ä¼˜åŒ– |
| **æ¨¡å—å¯¼å…¥é”™è¯¯** | ğŸ”´ é«˜ | integrated_cache æ¨¡å—å¯¼å…¥å¤±è´¥ |

### 1.2 æ€§èƒ½ç“¶é¢ˆåˆ†æ

```python
# å…¸å‹çš„æ€§èƒ½ç“¶é¢ˆ
1. åˆ†æå¸ˆä¸²è¡Œæ‰§è¡Œï¼ˆåº”è¯¥å¹¶è¡Œï¼‰
   å½“å‰ï¼šå¸‚åœºåˆ†æå¸ˆ â†’ åŸºæœ¬é¢åˆ†æå¸ˆ â†’ æ–°é—»åˆ†æå¸ˆ â†’ ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ
   ä¼˜åŒ–ï¼š4ä¸ªåˆ†æå¸ˆå¹¶è¡Œæ‰§è¡Œ

2. æ•°æ®æºé‡å¤è°ƒç”¨
   å½“å‰ï¼šæ¯ä¸ªåˆ†æå¸ˆç‹¬ç«‹è°ƒç”¨æ•°æ®æº
   ä¼˜åŒ–ï¼šå…±äº«æ•°æ®è·å–ç»“æœï¼Œé¿å…é‡å¤

3. ç¼“å­˜ç­–ç•¥ä¸å½“
   å½“å‰ï¼šå›ºå®š TTLï¼Œæœªæ ¹æ®æ•°æ®ç‰¹ç‚¹è°ƒæ•´
   ä¼˜åŒ–ï¼šåˆ†å±‚ TTLï¼Œæ™ºèƒ½å¤±æ•ˆ
```

---

## 2. ç¼“å­˜ä¼˜åŒ–

### 2.1 ä¿®å¤ integrated_cache æ¨¡å—å¯¼å…¥

**é—®é¢˜**: `ModuleNotFoundError: No module named 'tradingagents.dataflows.integrated_cache'`

**è§£å†³æ–¹æ¡ˆ**:

```python
# ä¿®å¤å¯¼å…¥è·¯å¾„
# æ–‡ä»¶ï¼štradingagents/dataflows/__init__.py

# æ·»åŠ  integrated_cache å¯¼å‡º
from .cache import get_cache

# ç¡®ä¿ cache/__init__.py æ­£ç¡®å¯¼å‡º
# æ–‡ä»¶ï¼štradingagents/dataflows/cache/__init__.py

from .integrated import IntegratedCache

__all__ = ['IntegratedCache']
```

### 2.2 æ™ºèƒ½ç¼“å­˜ç­–ç•¥

**å½“å‰é—®é¢˜**: å›ºå®š TTLï¼Œä¸è€ƒè™‘æ•°æ®æ›´æ–°é¢‘ç‡

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```python
# æ–‡ä»¶ï¼štradingagents/dataflows/cache/smart_cache.py

import time
from typing import Any, Optional
from datetime import timedelta, datetime

class SmartCache:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨ - æ ¹æ®æ•°æ®ç‰¹ç‚¹è°ƒæ•´ç¼“å­˜ç­–ç•¥"""
    
    # ä¸åŒç±»å‹æ•°æ®çš„é»˜è®¤ TTLï¼ˆç§’ï¼‰
    DEFAULT_TTLS = {
        'realtime_quote': 300,      # å®æ—¶è¡Œæƒ…ï¼š5åˆ†é’Ÿ
        'daily_kline': 3600,        # æ—¥Kçº¿ï¼š1å°æ—¶
        'fundamental': 86400,       # åŸºæœ¬é¢æ•°æ®ï¼š1å¤©
        'news': 1800,                # æ–°é—»æ•°æ®ï¼š30åˆ†é’Ÿ
        'sentiment': 3600,           # æƒ…ç»ªæ•°æ®ï¼š1å°æ—¶
    }
    
    def __init__(self, cache_manager):
        self.cache_manager = cache_manager
        self.cache_hit_stats = {}
    
    async def get(
        self,
        key: str,
        data_type: str,
        ttl: Optional[int] = None
    ) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®ï¼Œè®°å½•å‘½ä¸­ç‡"""
        
        # è‡ªåŠ¨ TTL
        if ttl is None:
            ttl = self.DEFAULT_TTLS.get(data_type, 3600)
        
        # è·å–ç¼“å­˜
        value = await self.cache_manager.get(key)
        
        # è®°å½•ç»Ÿè®¡
        if value is not None:
            self.cache_hit_stats[data_type] = self.cache_hit_stats.get(data_type, 0) + 1
            return value
        
        return None
    
    async def set(
        self,
        key: str,
        value: Any,
        data_type: str,
        ttl: Optional[int] = None
    ) -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        
        # è‡ªåŠ¨ TTL
        if ttl is None:
            ttl = self.DEFAULT_TTLS.get(data_type, 3600)
        
        return await self.cache_manager.set(key, value, ttl)
    
    def get_cache_hit_rate(self, data_type: str) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        total_hits = self.cache_hit_stats.get(data_type, 0)
        # TODO: éœ€è¦è®°å½•æ€»è®¿é—®æ¬¡æ•°
        return 0.0
```

### 2.3 ç¼“å­˜é¢„çƒ­æœºåˆ¶

**ä¼˜åŒ–æ–¹æ¡ˆ**: ç³»ç»Ÿå¯åŠ¨æ—¶é¢„çƒ­çƒ­ç‚¹æ•°æ®

```python
# æ–‡ä»¶ï¼šscripts/maintenance/cache_warmup.py

import asyncio
from datetime import datetime, timedelta
from tradingagents.dataflows.cache.smart_cache import SmartCache

async def warmup_cache():
    """ç¼“å­˜é¢„çƒ­ - é¢„åŠ è½½çƒ­ç‚¹æ•°æ®"""
    
    cache = SmartCache(get_cache())
    
    # é¢„çƒ­æ²ªæ·±300æˆåˆ†è‚¡
    hs300_stocks = [
        '600519.SH', '601318.SH', '601398.SH',  # èŒ…å°ã€å¹³å®‰ã€å·¥è¡Œ
        # ... æ›´å¤šè‚¡ç¥¨
    ]
    
    today = datetime.now().strftime('%Y-%m-%d')
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    
    tasks = []
    for stock in hs300_stocks:
        # é¢„çƒ­æ—¥Kçº¿æ•°æ®
        task = cache.set(
            f'daily_kline:{stock}:{yesterday}',
            None,  # å ä½å€¼
            'daily_kline',
            ttl=3600
        )
        tasks.append(task)
    
    # å¹¶è¡Œé¢„çƒ­
    await asyncio.gather(*tasks, return_exceptions=True)
    print(f"âœ… ç¼“å­˜é¢„çƒ­å®Œæˆï¼šé¢„çƒ­ {len(hs300_stocks)} åªè‚¡ç¥¨")

if __name__ == "__main__":
    asyncio.run(warmup_cache())
```

### 2.4 ç¼“å­˜ç›‘æ§å’Œæ¸…ç†

**ä¼˜åŒ–æ–¹æ¡ˆ**: ç›‘æ§ç¼“å­˜ä½¿ç”¨æƒ…å†µï¼Œå®šæœŸæ¸…ç†

```python
# æ–‡ä»¶ï¼štradingagents/dataflows/cache/cache_monitor.py

import time
from typing import Dict, List
from tradingagents.utils.logging_init import get_logger

logger = get_logger("cache_monitor")

class CacheMonitor:
    """ç¼“å­˜ç›‘æ§å™¨"""
    
    def __init__(self, cache_manager):
        self.cache_manager = cache_manager
        self.stats = {
            'hits': 0,
            'misses': 0,
            'size': 0,
            'evictions': 0,
        }
        self.start_time = time.time()
    
    def record_hit(self):
        """è®°å½•ç¼“å­˜å‘½ä¸­"""
        self.stats['hits'] += 1
    
    def record_miss(self):
        """è®°å½•ç¼“å­˜æœªå‘½ä¸­"""
        self.stats['misses'] += 1
    
    def get_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.stats['hits'] + self.stats['misses']
        if total == 0:
            return 0.0
        return self.stats['hits'] / total
    
    def get_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        runtime = time.time() - self.start_time
        
        return {
            'hit_rate': self.get_hit_rate(),
            'total_requests': self.stats['hits'] + self.stats['misses'],
            'cache_size': self.stats['size'],
            'evictions': self.stats['evictions'],
            'runtime_seconds': runtime,
        }
    
    async def cleanup_expired_keys(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜é”®"""
        
        # Redis è‡ªåŠ¨æ¸…ç†è¿‡æœŸé”®
        # MongoDB éœ€è¦æ‰‹åŠ¨æ¸…ç†
        
        try:
            from app.core.database import get_mongo_db_sync
            db = get_mongo_db_sync()
            cache_collection = db.cache_collection
            
            # æŸ¥æ‰¾è¿‡æœŸè®°å½•
            expired_threshold = datetime.now() - timedelta(days=7)
            result = cache_collection.delete_many({
                'created_at': {'$lt': expired_threshold}
            })
            
            logger.info(f"âœ… æ¸…ç† {result.deleted_count} æ¡è¿‡æœŸç¼“å­˜è®°å½•")
            return result.deleted_count
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
            return 0
```

---

## 3. å¹¶å‘ä¼˜åŒ–

### 3.1 åˆ†æå¸ˆå¹¶è¡Œæ‰§è¡Œ

**å½“å‰é—®é¢˜**: åˆ†æå¸ˆä¸²è¡Œæ‰§è¡Œ

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```python
# æ–‡ä»¶ï¼štradingagents/graph/parallel_analysts.py

import asyncio
from typing import List, Dict, Any
from langchain_core.messages import BaseMessage

async def run_analysts_parallel(
    market_analyst_fn,
    fundamentals_analyst_fn,
    news_analyst_fn,
    social_media_analyst_fn,
    state: Dict[str, Any]
) -> Dict[str, Any]:
    """å¹¶è¡Œæ‰§è¡Œ4ä¸ªåˆ†æå¸ˆ"""
    
    logger.info("ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œåˆ†æå¸ˆ...")
    start_time = time.time()
    
    # åˆ›å»º4ä¸ªå¹¶è¡Œä»»åŠ¡
    tasks = [
        asyncio.create_task(market_analyst_fn(state.copy())),
        asyncio.create_task(fundamentals_analyst_fn(state.copy())),
        asyncio.create_task(news_analyst_fn(state.copy())),
        asyncio.create_task(social_media_analyst_fn(state.copy())),
    ]
    
    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # å¤„ç†ç»“æœ
    market_result = results[0] if not isinstance(results[0], Exception) else None
    fundamentals_result = results[1] if not isinstance(results[1], Exception) else None
    news_result = results[2] if not isinstance(results[2], Exception) else None
    social_media_result = results[3] if not isinstance(results[3], Exception) else None
    
    # åˆå¹¶ç»“æœåˆ°çŠ¶æ€
    final_state = state.copy()
    if market_result:
        final_state.update(market_result)
    if fundamentals_result:
        final_state.update(fundamentals_result)
    if news_result:
        final_state.update(news_result)
    if social_media_result:
        final_state.update(social_media_result)
    
    elapsed = time.time() - start_time
    logger.info(f"âœ… å¹¶è¡Œæ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
    
    return final_state
```

### 3.2 æ•°æ®æºå¹¶è¡Œè°ƒç”¨

**ä¼˜åŒ–æ–¹æ¡ˆ**: ä¸åŒæ•°æ®æºå¹¶è¡Œå°è¯•

```python
# æ–‡ä»¶ï¼štradingagents/dataflows/parallel_data_fetch.py

import asyncio
from typing import List, Optional
import pandas as pd

async def fetch_data_from_multiple_sources(
    symbol: str,
    start_date: str,
    end_date: str,
    providers: List
) -> Optional[pd.DataFrame]:
    """å¹¶è¡Œä»å¤šä¸ªæ•°æ®æºè·å–æ•°æ®ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæˆåŠŸçš„ç»“æœ"""
    
    logger.info(f"ğŸš€ å¹¶è¡Œä» {len(providers)} ä¸ªæ•°æ®æºè·å–æ•°æ®...")
    
    # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
    tasks = []
    for provider in providers:
        task = asyncio.create_task(
            provider.get_historical_data(symbol, start_date, end_date),
            name=f"{provider.__class__.__name__}"
        )
        tasks.append(task)
    
    # ç­‰å¾…ç¬¬ä¸€ä¸ªæˆåŠŸçš„ç»“æœ
    done, pending = await asyncio.wait(
        tasks,
        return_when=asyncio.FIRST_COMPLETED
    )
    
    # å–æ¶ˆå…¶ä»–ä»»åŠ¡
    for task in pending:
        task.cancel()
    
    # è¿”å›ç¬¬ä¸€ä¸ªæˆåŠŸçš„ç»“æœ
    for task in done:
        try:
            result = await task
            if result is not None and not result.empty:
                logger.info(f"âœ… æ•°æ®æº {task.get_name()} æˆåŠŸè¿”å›æ•°æ®")
                return result
        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®æº {task.get_name()} å¤±è´¥: {e}")
            continue
    
    logger.error("âŒ æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥")
    return None
```

### 3.3 æ‰¹é‡æ“ä½œä¼˜åŒ–

**ä¼˜åŒ–æ–¹æ¡ˆ**: æ‰¹é‡è·å–å¤šä¸ªè‚¡ç¥¨æ•°æ®

```python
# æ–‡ä»¶ï¼štradingagents/dataflows/batch_operations.py

import asyncio
from typing import List, Dict
import pandas as pd

async def batch_get_stock_data(
    symbols: List[str],
    start_date: str,
    end_date: str,
    provider,
    batch_size: int = 10
) -> Dict[str, pd.DataFrame]:
    """æ‰¹é‡è·å–å¤šä¸ªè‚¡ç¥¨çš„æ•°æ®"""
    
    logger.info(f"ğŸš€ æ‰¹é‡è·å– {len(symbols)} åªè‚¡ç¥¨æ•°æ®ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
    
    results = {}
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(symbols), batch_size):
        batch = symbols[i:i+batch_size]
        logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {batch}")
        
        # å¹¶è¡Œè·å–æ‰¹æ¬¡æ•°æ®
        tasks = []
        for symbol in batch:
            task = asyncio.create_task(
                provider.get_historical_data(symbol, start_date, end_date)
            )
            tasks.append(task)
        
        # ç­‰å¾…æ‰¹æ¬¡å®Œæˆ
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for symbol, result in zip(batch, batch_results):
            if not isinstance(result, Exception) and result is not None:
                results[symbol] = result
            else:
                logger.warning(f"âš ï¸ è‚¡ç¥¨ {symbol} è·å–å¤±è´¥")
        
        # æ‰¹æ¬¡ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
        if i + batch_size < len(symbols):
            await asyncio.sleep(1)
    
    logger.info(f"âœ… æ‰¹é‡è·å–å®Œæˆï¼ŒæˆåŠŸ: {len(results)}/{len(symbols)}")
    return results
```

---

## 4. æ•°æ®åº“ä¼˜åŒ–

### 4.1 ç´¢å¼•ä¼˜åŒ–

**ä¼˜åŒ–æ–¹æ¡ˆ**: ä¸ºå¸¸ç”¨æŸ¥è¯¢å­—æ®µæ·»åŠ ç´¢å¼•

```python
# æ–‡ä»¶ï¼šscripts/database/create_indexes.py

from app.core.database import get_mongo_db_sync
from tradingagents.utils.logging_init import get_logger

logger = get_logger("database")

def create_indexes():
    """åˆ›å»ºæ•°æ®åº“ç´¢å¼•"""
    
    db = get_mongo_db_sync()
    
    # ç¼“å­˜é›†åˆç´¢å¼•
    cache_collection = db.cache_collection
    cache_indexes = [
        {'key': [('key', 1)], 'unique': True, 'name': 'key_unique'},
        {'key': [('created_at', -1)], 'name': 'created_at_idx'},
        {'key': [('ttl', 1)], 'name': 'ttl_idx'},
        {'key': [('data_type', 1), ('created_at', -1)], 'name': 'type_created_idx'},
    ]
    
    for index_spec in cache_indexes:
        try:
            cache_collection.create_index(**index_spec)
            logger.info(f"âœ… åˆ›å»ºç´¢å¼•: {index_spec['name']}")
        except Exception as e:
            logger.warning(f"âš ï¸ ç´¢å¼•å·²å­˜åœ¨æˆ–åˆ›å»ºå¤±è´¥: {index_spec['name']}: {e}")
    
    # Token ä½¿ç”¨é›†åˆç´¢å¼•
    token_usage_collection = db.token_usage
    token_indexes = [
        {'key': [('provider', 1)], 'name': 'provider_idx'},
        {'key': [('date', -1)], 'name': 'date_idx'},
        {'key': [('provider', 1), ('date', -1)], 'name': 'provider_date_idx'},
    ]
    
    for index_spec in token_indexes:
        try:
            token_usage_collection.create_index(**index_spec)
            logger.info(f"âœ… åˆ›å»ºç´¢å¼•: {index_spec['name']}")
        except Exception as e:
            logger.warning(f"âš ï¸ ç´¢å¼•å·²å­˜åœ¨æˆ–åˆ›å»ºå¤±è´¥: {index_spec['name']}: {e}")
    
    logger.info("âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ")

if __name__ == "__main__":
    create_indexes()
```

### 4.2 æŸ¥è¯¢ä¼˜åŒ–

**ä¼˜åŒ–æ–¹æ¡ˆ**: ä½¿ç”¨èšåˆç®¡é“å’ŒæŠ•å½±

```python
# æ–‡ä»¶ï¼štradingagents/dataflows/cache/optimized_cache.py

import asyncio
from datetime import datetime, timedelta
from typing import Optional, Any

async def get_cache_optimized(
    key: str,
    collection
) -> Optional[Any]:
    """ä¼˜åŒ–çš„ç¼“å­˜è·å– - ä½¿ç”¨æŠ•å½±å’Œç´¢å¼•"""
    
    # ä½¿ç”¨æŠ•å½±åªè¿”å›éœ€è¦çš„å­—æ®µ
    query = {'key': key}
    projection = {'value': 1, 'created_at': 1, '_id': 0}
    
    result = await collection.find_one(query, projection)
    
    if result:
        return result['value']
    return None

async def get_multiple_cache_optimized(
    keys: List[str],
    collection
) -> Dict[str, Any]:
    """æ‰¹é‡è·å–ç¼“å­˜ - ä½¿ç”¨ $in æ“ä½œç¬¦"""
    
    # ä½¿ç”¨ $in æ‰¹é‡æŸ¥è¯¢
    query = {'key': {'$in': keys}}
    projection = {'key': 1, 'value': 1, 'created_at': 1, '_id': 0}
    
    cursor = collection.find(query, projection)
    results = {doc['key']: doc['value'] for doc in await cursor.to_list(length=len(keys))}
    
    return results

async def cleanup_old_cache_optimized(collection, days: int = 7):
    """ä¼˜åŒ–çš„æ—§ç¼“å­˜æ¸…ç† - æ‰¹é‡åˆ é™¤"""
    
    threshold = datetime.now() - timedelta(days=days)
    
    # æ‰¹é‡åˆ é™¤ï¼ˆæ¯”é€æ¡åˆ é™¤å¿«å¾ˆå¤šï¼‰
    result = await collection.delete_many({'created_at': {'$lt': threshold}})
    
    return result.deleted_count
```

### 4.3 è¿æ¥æ± é…ç½®

**ä¼˜åŒ–æ–¹æ¡ˆ**: ä¼˜åŒ–æ•°æ®åº“è¿æ¥æ± 

```python
# æ–‡ä»¶ï¼šapp/core/database.py

from motor.motor_asyncio import AsyncIOMotorClient
from typing import Optional

# MongoDB è¿æ¥æ± é…ç½®
MONGODB_POOL_CONFIG = {
    'maxPoolSize': 100,          # æœ€å¤§è¿æ¥æ•°
    'minPoolSize': 10,           # æœ€å°è¿æ¥æ•°
    'maxIdleTimeMS': 30000,       # è¿æ¥ç©ºé—²è¶…æ—¶ï¼ˆ30ç§’ï¼‰
    'waitQueueTimeoutMS': 5000,   # ç­‰å¾…è¿æ¥è¶…æ—¶ï¼ˆ5ç§’ï¼‰
    'socketTimeoutMS': 30000,      # Socket è¶…æ—¶ï¼ˆ30ç§’ï¼‰
    'connectTimeoutMS': 10000,     # è¿æ¥è¶…æ—¶ï¼ˆ10ç§’ï¼‰
    'serverSelectionTimeoutMS': 5000,  # æœåŠ¡å™¨é€‰æ‹©è¶…æ—¶ï¼ˆ5ç§’ï¼‰
}

# Redis è¿æ¥æ± é…ç½®
REDIS_POOL_CONFIG = {
    'max_connections': 50,         # æœ€å¤§è¿æ¥æ•°
    'socket_timeout': 5,          # Socket è¶…æ—¶ï¼ˆ5ç§’ï¼‰
    'socket_connect_timeout': 3,  # è¿æ¥è¶…æ—¶ï¼ˆ3ç§’ï¼‰
    'retry_on_timeout': True,      # è¶…æ—¶é‡è¯•
    'health_check_interval': 30,   # å¥åº·æ£€æŸ¥é—´éš”ï¼ˆ30ç§’ï¼‰
}

async def get_mongo_db_pool():
    """è·å– MongoDB è¿æ¥æ± å®¢æˆ·ç«¯"""
    
    connection_string = os.getenv('MONGODB_CONNECTION_STRING')
    
    client = AsyncIOMotorClient(
        connection_string,
        **MONGODB_POOL_CONFIG
    )
    
    return client

async def get_redis_pool():
    """è·å– Redis è¿æ¥æ± å®¢æˆ·ç«¯"""
    
    import redis.asyncio as redis
    
    url = os.getenv('REDIS_URL')
    
    pool = redis.ConnectionPool.from_url(
        url,
        **REDIS_POOL_CONFIG
    )
    
    return redis.Redis(connection_pool=pool)
```

---

## 5. ä»£ç ä¼˜åŒ–

### 5.1 å‡å°‘é‡å¤ä»£ç 

**ä¼˜åŒ–æ–¹æ¡ˆ**: æå–å…¬å…±é€»è¾‘

```python
# æ–‡ä»¶ï¼štradingagents/agents/utils/analyst_base.py

from typing import Dict, Any, Callable
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

class BaseAnalyst:
    """åˆ†æå¸ˆåŸºç±» - æä¾›å…¬å…±é€»è¾‘"""
    
    def __init__(self, llm, toolkit):
        self.llm = llm
        self.toolkit = toolkit
    
    def _check_tool_call_limit(self, state: Dict[str, Any], counter_key: str, max_calls: int = 3) -> bool:
        """æ£€æŸ¥å·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶"""
        
        call_count = state.get(f"{counter_key}_tool_call_count", 0)
        
        if call_count >= max_calls:
            logger.warning(f"âš ï¸ å·¥å…·è°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™: {call_count}/{max_calls}")
            return False
        
        return True
    
    def _build_prompt(
        self,
        system_message: str,
        state: Dict[str, Any]
    ) -> ChatPromptTemplate:
        """æ„å»ºæç¤ºè¯"""
        
        return ChatPromptTemplate.from_messages([
            ("system", system_message),
            MessagesPlaceholder(variable_name="messages"),
        ])
    
    async def _invoke_with_retry(
        self,
        prompt: ChatPromptTemplate,
        state: Dict[str, Any],
        max_retries: int = 3
    ) -> Any:
        """å¸¦é‡è¯•çš„ LLM è°ƒç”¨"""
        
        for attempt in range(max_retries):
            try:
                response = await self.llm.ainvoke(prompt.invoke_messages(state))
                return response
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(f"âš ï¸ LLM è°ƒç”¨å¤±è´¥ï¼ˆç¬¬{attempt+1}æ¬¡å°è¯•ï¼‰: {e}")
                    await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    logger.error(f"âŒ LLM è°ƒç”¨å¤±è´¥ï¼ˆ{max_retries}æ¬¡å°è¯•åï¼‰: {e}")
                    raise
```

### 5.2 ä½¿ç”¨ LRU Cache

**ä¼˜åŒ–æ–¹æ¡ˆ**: ç¼“å­˜é¢‘ç¹è°ƒç”¨çš„å‡½æ•°ç»“æœ

```python
from functools import lru_cache
from typing import Optional

@lru_cache(maxsize=1000)
def get_market_info_cached(symbol: str) -> Optional[Dict]:
    """å¸¦ç¼“å­˜çš„å¸‚åœºä¿¡æ¯è·å–"""
    
    return StockUtils.get_market_info(symbol)

@lru_cache(maxsize=500)
def get_company_name_cached(symbol: str, market_info: Dict) -> str:
    """å¸¦ç¼“å­˜çš„å…¬å¸åç§°è·å–"""
    
    return get_company_name(symbol, market_info)

# ä½¿ç”¨ç¤ºä¾‹
market_info = get_market_info_cached('600765')
company_name = get_company_name_cached('600765', market_info)
```

### 5.3 ä½¿ç”¨å¼‚æ­¥æ•°æ®åº“é©±åŠ¨

**ä¼˜åŒ–æ–¹æ¡ˆ**: å°† MongoDB åŒæ­¥é©±åŠ¨æ”¹ä¸ºå¼‚æ­¥

```python
# å½“å‰ï¼ˆåŒæ­¥ï¼‰ï¼š
from pymongo import MongoClient
client = MongoClient('mongodb://localhost:27017/')
db = client.tradingagents

# ä¼˜åŒ–åï¼ˆå¼‚æ­¥ï¼‰ï¼š
from motor.motor_asyncio import AsyncIOMotorClient
client = AsyncIOMotorClient('mongodb://localhost:27017/')
db = client.tradingagents

# å¼‚æ­¥æŸ¥è¯¢ç¤ºä¾‹
async def get_data_async():
    data = await db.collection.find_one({'key': 'value'})
    return data
```

---

## 6. æ¶æ„ä¼˜åŒ–

### 6.1 å¼•å…¥æ¶ˆæ¯é˜Ÿåˆ—

**ä¼˜åŒ–æ–¹æ¡ˆ**: ä½¿ç”¨æ¶ˆæ¯é˜Ÿåˆ—å¼‚æ­¥å¤„ç†ä»»åŠ¡

```python
# æ–‡ä»¶ï¼štradingagents/workers/task_queue.py

import asyncio
from typing import Callable, Any
from collections import deque

class AsyncTaskQueue:
    """å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—"""
    
    def __init__(self, max_workers: int = 5):
        self.queue = deque()
        self.workers = []
        self.max_workers = max_workers
        self.running = False
    
    async def start(self):
        """å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
        
        self.running = True
        self.workers = [
            asyncio.create_task(self._worker())
            for _ in range(self.max_workers)
        ]
        
        logger.info(f"âœ… ä»»åŠ¡é˜Ÿåˆ—å·²å¯åŠ¨ï¼Œå·¥ä½œçº¿ç¨‹æ•°: {self.max_workers}")
    
    async def _worker(self):
        """å·¥ä½œçº¿ç¨‹"""
        
        while self.running:
            if self.queue:
                task_func, *args = self.queue.popleft()
                try:
                    await task_func(*args)
                except Exception as e:
                    logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            else:
                await asyncio.sleep(0.1)  # é¿å…å¿™ç­‰å¾…
    
    async def submit(self, func: Callable, *args):
        """æäº¤ä»»åŠ¡"""
        
        self.queue.append((func, *args))
        logger.info(f"ğŸ“ ä»»åŠ¡å·²æäº¤ï¼Œé˜Ÿåˆ—é•¿åº¦: {len(self.queue)}")
    
    async def stop(self):
        """åœæ­¢ä»»åŠ¡é˜Ÿåˆ—"""
        
        self.running = False
        await asyncio.gather(*self.workers, return_exceptions=True)
        logger.info("âœ… ä»»åŠ¡é˜Ÿåˆ—å·²åœæ­¢")

# ä½¿ç”¨ç¤ºä¾‹
task_queue = AsyncTaskQueue(max_workers=5)
await task_queue.start()

# æäº¤ä»»åŠ¡
await task_queue.submit(some_async_function, arg1, arg2)
```

### 6.2 ç»“æœç¼“å­˜ç­–ç•¥

**ä¼˜åŒ–æ–¹æ¡ˆ**: ç¼“å­˜åˆ†æå¸ˆç»“æœï¼Œé¿å…é‡å¤åˆ†æ

```python
# æ–‡ä»¶ï¼štradingagents/cache/analyst_result_cache.py

import hashlib
import json
from datetime import datetime, timedelta
from typing import Dict, Any

class AnalystResultCache:
    """åˆ†æå¸ˆç»“æœç¼“å­˜"""
    
    def __init__(self, cache_manager):
        self.cache_manager = cache_manager
        self.default_ttl = 86400  # 1å¤©
    
    def _generate_cache_key(
        self,
        symbol: str,
        date: str,
        analyst_type: str
    ) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        
        key_data = f"{symbol}:{date}:{analyst_type}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    async def get_result(
        self,
        symbol: str,
        date: str,
        analyst_type: str
    ) -> Optional[Dict[str, Any]]:
        """è·å–åˆ†æå¸ˆç»“æœ"""
        
        cache_key = self._generate_cache_key(symbol, date, analyst_type)
        
        result = await self.cache_manager.get(cache_key)
        
        if result:
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            created_at = result.get('created_at')
            if created_at:
                age = datetime.now() - created_at
                if age < timedelta(days=1):  # 1å¤©å†…çš„ç»“æœæœ‰æ•ˆ
                    logger.info(f"âœ… ä»ç¼“å­˜è·å– {analyst_type} ç»“æœ")
                    return result['data']
        
        return None
    
    async def set_result(
        self,
        symbol: str,
        date: str,
        analyst_type: str,
        data: Dict[str, Any]
    ) -> bool:
        """ç¼“å­˜åˆ†æå¸ˆç»“æœ"""
        
        cache_key = self._generate_cache_key(symbol, date, analyst_type)
        
        cache_data = {
            'symbol': symbol,
            'date': date,
            'analyst_type': analyst_type,
            'data': data,
            'created_at': datetime.now(),
        }
        
        return await self.cache_manager.set(
            cache_key,
            cache_data,
            ttl=self.default_ttl
        )
```

### 6.3 æ•°æ®é¢„åŠ è½½

**ä¼˜åŒ–æ–¹æ¡ˆ**: ç³»ç»Ÿå¯åŠ¨æ—¶é¢„åŠ è½½å¸¸ç”¨æ•°æ®

```python
# æ–‡ä»¶ï¼šscripts/maintenance/preload_data.py

import asyncio
from datetime import datetime, timedelta
from tradingagents.dataflows.preload_data import preload_common_stocks

async def preload_system_data():
    """é¢„åŠ è½½ç³»ç»Ÿå¸¸ç”¨æ•°æ®"""
    
    logger.info("ğŸš€ å¼€å§‹é¢„åŠ è½½ç³»ç»Ÿæ•°æ®...")
    
    # 1. é¢„åŠ è½½æ²ªæ·±300æˆåˆ†è‚¡åŸºæœ¬ä¿¡æ¯
    await preload_common_stocks()
    
    # 2. é¢„åŠ è½½æœ€æ–°è´¢åŠ¡æ•°æ®
    from tradingagents.dataflows.preload_fundamentals import preload_recent_fundamentals
    await preload_recent_fundamentals()
    
    # 3. é¢„åŠ è½½çƒ­é—¨è‚¡ç¥¨æ•°æ®
    from tradingagents.dataflows.preload_popular_stocks import preload_popular_stocks
    await preload_popular_stocks()
    
    logger.info("âœ… ç³»ç»Ÿæ•°æ®é¢„åŠ è½½å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(preload_system_data())
```

---

## ä¼˜åŒ–ä¼˜å…ˆçº§

| ä¼˜åŒ–é¡¹ | ä¼˜å…ˆçº§ | é¢„æœŸæ”¶ç›Š | å®æ–½éš¾åº¦ |
|--------|--------|---------|---------|
| ä¿®å¤ integrated_cache å¯¼å…¥ | ğŸ”´ P0 | é«˜ | ä½ |
| åˆ†æå¸ˆå¹¶è¡Œæ‰§è¡Œ | ğŸ”´ P0 | é«˜ï¼ˆ3xæå‡ï¼‰ | ä¸­ |
| æ•°æ®æºå¹¶è¡Œè°ƒç”¨ | ğŸ”´ P0 | ä¸­é«˜ | ä¸­ |
| ç¼“å­˜ç­–ç•¥ä¼˜åŒ– | ğŸŸ¡ P1 | ä¸­ | ä¸­ |
| æ•°æ®åº“ç´¢å¼•ä¼˜åŒ– | ğŸŸ¡ P1 | ä¸­ | ä½ |
| æ‰¹é‡æ“ä½œä¼˜åŒ– | ğŸŸ¡ P1 | ä¸­ | ä¸­ |
| è¿æ¥æ± é…ç½® | ğŸŸ¢ P2 | ä½ | ä½ |
| ç»“æœç¼“å­˜ç­–ç•¥ | ğŸŸ¢ P2 | ä½ | ä¸­ |
| æ•°æ®é¢„åŠ è½½ | ğŸŸ¢ P2 | ä½ | ä¸­ |
| æ¶ˆæ¯é˜Ÿåˆ—å¼•å…¥ | ğŸŸ¢ P3 | ä½ | é«˜ |

---

## å®æ–½è®¡åˆ’

### ç¬¬ä¸€é˜¶æ®µï¼ˆç«‹å³æ‰§è¡Œï¼‰

1. âœ… ä¿®å¤ integrated_cache æ¨¡å—å¯¼å…¥
2. âœ… åˆ›å»ºæ•°æ®åº“ç´¢å¼•
3. âœ… ä¼˜åŒ–ç¼“å­˜ç­–ç•¥

### ç¬¬äºŒé˜¶æ®µï¼ˆ1å‘¨å†…ï¼‰

1. å®ç°åˆ†æå¸ˆå¹¶è¡Œæ‰§è¡Œ
2. å®ç°æ•°æ®æºå¹¶è¡Œè°ƒç”¨
3. æ·»åŠ ç¼“å­˜ç›‘æ§

### ç¬¬ä¸‰é˜¶æ®µï¼ˆ2å‘¨å†…ï¼‰

1. å®ç°æ‰¹é‡æ“ä½œä¼˜åŒ–
2. å®ç°ç»“æœç¼“å­˜ç­–ç•¥
3. å®ç°æ•°æ®é¢„åŠ è½½

### ç¬¬å››é˜¶æ®µï¼ˆ1ä¸ªæœˆå†…ï¼‰

1. å¼•å…¥æ¶ˆæ¯é˜Ÿåˆ—
2. å…¨é¢æ€§èƒ½æµ‹è¯•
3. æ€§èƒ½è°ƒä¼˜

---

## ç›‘æ§å’ŒæŒ‡æ ‡

### å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å½“å‰å€¼ | ç›‘æ§æ–¹æ³• |
|------|--------|--------|---------|
| åˆ†æå¸ˆæ‰§è¡Œæ—¶é—´ | < 60ç§’ | æœªçŸ¥ | æ—¥å¿—ç»Ÿè®¡ |
| æ•°æ®æºå“åº”æ—¶é—´ | < 5ç§’ | æœªçŸ¥ | æ—¥å¿—ç»Ÿè®¡ |
| ç¼“å­˜å‘½ä¸­ç‡ | > 80% | æœªçŸ¥ | ç¼“å­˜ç›‘æ§ |
| æ•°æ®åº“æŸ¥è¯¢æ—¶é—´ | < 100ms | æœªçŸ¥ | æ…¢æŸ¥è¯¢æ—¥å¿— |
| API è°ƒç”¨æˆåŠŸç‡ | > 95% | æœªçŸ¥ | æ—¥å¿—ç»Ÿè®¡ |

### ç›‘æ§å·¥å…·

```python
# æ–‡ä»¶ï¼štradingagents/monitoring/performance_monitor.py

import time
from typing import Dict, List
from contextlib import asynccontextmanager
from tradingagents.utils.logging_init import get_logger

logger = get_logger("performance")

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {}
    
    @asynccontextmanager
    async def measure(self, name: str):
        """æµ‹é‡æ€§èƒ½çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        
        start_time = time.time()
        
        try:
            yield
        finally:
            elapsed = time.time() - start_time
            
            if name not in self.metrics:
                self.metrics[name] = []
            
            self.metrics[name].append(elapsed)
            logger.info(f"ğŸ“Š {name}: {elapsed:.3f}ç§’")
    
    def get_average(self, name: str) -> float:
        """è·å–å¹³å‡æ‰§è¡Œæ—¶é—´"""
        
        if name not in self.metrics:
            return 0.0
        
        values = self.metrics[name]
        return sum(values) / len(values)
    
    def get_p95(self, name: str) -> float:
        """è·å– P95 æ‰§è¡Œæ—¶é—´"""
        
        if name not in self.metrics:
            return 0.0
        
        values = sorted(self.metrics[name])
        index = int(len(values) * 0.95)
        return values[index]
    
    def report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        
        report = []
        for name, values in self.metrics.items():
            avg = sum(values) / len(values)
            p95 = sorted(values)[int(len(values) * 0.95)]
            report.append({
                'name': name,
                'avg': f"{avg:.3f}s",
                'p95': f"{p95:.3f}s",
                'count': len(values)
            })
        
        # æ‰“å°æŠ¥å‘Š
        print("\n" + "="*60)
        print("æ€§èƒ½æŠ¥å‘Š")
        print("="*60)
        for item in sorted(report, key=lambda x: float(x['avg'][:-1]), reverse=True):
            print(f"{item['name']}: å¹³å‡ {item['avg']}, P95 {item['p95']}, æ¬¡æ•° {item['count']}")
        print("="*60 + "\n")

# ä½¿ç”¨ç¤ºä¾‹
monitor = PerformanceMonitor()

async def some_operation():
    async with monitor.measure("operation_name"):
        # æ‰§è¡Œæ“ä½œ
        await asyncio.sleep(1)

# ç”ŸæˆæŠ¥å‘Š
monitor.report()
```

---

## æ€»ç»“

### å…³é”®ä¼˜åŒ–ç‚¹

1. **ç¼“å­˜ä¼˜åŒ–**
   - ä¿®å¤ integrated_cache å¯¼å…¥é—®é¢˜
   - å®ç°æ™ºèƒ½ç¼“å­˜ç­–ç•¥
   - æ·»åŠ ç¼“å­˜ç›‘æ§å’Œæ¸…ç†

2. **å¹¶å‘ä¼˜åŒ–**
   - åˆ†æå¸ˆå¹¶è¡Œæ‰§è¡Œï¼ˆé¢„æœŸ3xæå‡ï¼‰
   - æ•°æ®æºå¹¶è¡Œè°ƒç”¨
   - æ‰¹é‡æ“ä½œä¼˜åŒ–

3. **æ•°æ®åº“ä¼˜åŒ–**
   - æ·»åŠ æ•°æ®åº“ç´¢å¼•
   - ä¼˜åŒ–æŸ¥è¯¢è¯­å¥
   - é…ç½®è¿æ¥æ± 

4. **ä»£ç ä¼˜åŒ–**
   - å‡å°‘é‡å¤ä»£ç 
   - ä½¿ç”¨ LRU Cache
   - å¼‚æ­¥æ•°æ®åº“é©±åŠ¨

5. **æ¶æ„ä¼˜åŒ–**
   - å¼•å…¥æ¶ˆæ¯é˜Ÿåˆ—
   - ç»“æœç¼“å­˜ç­–ç•¥
   - æ•°æ®é¢„åŠ è½½

### é¢„æœŸæ•ˆæœ

- âš¡ æ€§èƒ½æå‡ï¼š3-5x
- ğŸ’° æˆæœ¬é™ä½ï¼š50-70%ï¼ˆå‡å°‘é‡å¤ API è°ƒç”¨ï¼‰
- ğŸ“ˆ å¯é æ€§æå‡ï¼šç¼“å­˜å‘½ä¸­ç‡ > 80%
- ğŸ¯ ç”¨æˆ·ä½“éªŒæå‡ï¼šå“åº”æ—¶é—´ < 60ç§’
