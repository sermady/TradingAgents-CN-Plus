# -*- coding: utf-8 -*-
"""
LLMå“åº”ç¼“å­˜å®ç°
æä¾›åŸºäºå†…å®¹çš„æ™ºèƒ½ç¼“å­˜,å‡å°‘LLMè°ƒç”¨å’ŒTokenæ¶ˆè€—
"""

import time
import hashlib
import json
from typing import Dict, Any, Optional, Tuple
from datetime import datetime

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import get_logger

logger = get_logger("default")


class LLMCache:
    """LLMå“åº”ç¼“å­˜"""

    def __init__(
        self,
        cache_backend: str = "memory",  # memory, redis, mongodb, file
        max_size: int = 10000,
        default_ttl: int = 3600,  # 1å°æ—¶
    ):
        """
        åˆå§‹åŒ–LLMç¼“å­˜

        Args:
            cache_backend: ç¼“å­˜åç«¯ (memory/redis/mongodb/file)
            max_size: æœ€å¤§ç¼“å­˜æ•°é‡
            default_ttl: é»˜è®¤TTL(ç§’)
        """
        self.cache_backend = cache_backend
        self.max_size = max_size
        self.default_ttl = default_ttl

        # å†…å­˜ç¼“å­˜
        self._memory_cache: Dict[str, Tuple[str, float, int]] = {}

        logger.info(
            f"ğŸ—„ï¸ [LLMç¼“å­˜] åˆå§‹åŒ–ç¼“å­˜: backend={cache_backend}, max_size={max_size}, default_ttl={default_ttl}s"
        )

    def _get_cache_key(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
    ) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®

        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°

        Returns:
            ç¼“å­˜é”®
        """
        key_string = f"{model}:{temperature}:{max_tokens}:{prompt}"
        return hashlib.sha256(key_string.encode("utf-8")).hexdigest()

    def get(
        self,
        prompt: str,
        model: str,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        ttl: Optional[int] = None,
    ) -> Optional[str]:
        """
        ä»ç¼“å­˜è·å–å“åº”

        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            ttl: TTL(ç§’),Noneåˆ™ä½¿ç”¨é»˜è®¤TTL

        Returns:
            ç¼“å­˜çš„å“åº”,å¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸåˆ™è¿”å›None
        """
        cache_key = self._get_cache_key(prompt, model, temperature, max_tokens)

        if self.cache_backend == "memory":
            return self._get_from_memory(cache_key, ttl)

        logger.debug(f"ğŸ” [LLMç¼“å­˜] ç¼“å­˜æœªå‘½ä¸­: key={cache_key[:16]}...")
        return None

    def set(
        self,
        prompt: str,
        response: str,
        model: str,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        ttl: Optional[int] = None,
    ):
        """
        ä¿å­˜å“åº”åˆ°ç¼“å­˜

        Args:
            prompt: æç¤ºè¯
            response: LLMå“åº”
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            ttl: TTL(ç§’),Noneåˆ™ä½¿ç”¨é»˜è®¤TTL
        """
        if ttl is None:
            ttl = self.default_ttl

        cache_key = self._get_cache_key(prompt, model, temperature, max_tokens)

        if self.cache_backend == "memory":
            self._save_to_memory(cache_key, response, ttl)

        logger.debug(f"ğŸ’¾ [LLMç¼“å­˜] ç¼“å­˜å‘½ä¸­: key={cache_key[:16]}...")

    def _get_from_memory(
        self,
        cache_key: str,
        ttl: int,
    ) -> Optional[str]:
        """ä»å†…å­˜ç¼“å­˜è·å–"""
        if cache_key not in self._memory_cache:
            return None

        response, timestamp, hit_count = self._memory_cache[cache_key]
        age = time.time() - timestamp

        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if age > ttl:
            del self._memory_cache[cache_key]
            logger.debug(f"â°ï¸ [LLMç¼“å­˜] ç¼“å­˜è¿‡æœŸ: age={age:.1f}s > {ttl}s")
            return None

        # æ›´æ–°å‘½ä¸­æ¬¡æ•°
        self._memory_cache[cache_key] = (response, timestamp, hit_count + 1)

        logger.info(
            f"âœ… [LLMç¼“å­˜] ç¼“å­˜å‘½ä¸­: key={cache_key[:16]}..., age={age:.1f}s, å‘½ä¸­æ¬¡æ•°={hit_count}"
        )
        return response

    def _save_to_memory(
        self,
        cache_key: str,
        response: str,
        ttl: int,
    ):
        """ä¿å­˜åˆ°å†…å­˜ç¼“å­˜"""
        # æ£€æŸ¥ç¼“å­˜å¤§å°,å¿…è¦æ—¶æ¸…ç†
        if len(self._memory_cache) >= self.max_size:
            self._evict_oldest()

        self._memory_cache[cache_key] = (response, time.time(), 1)

    def _evict_oldest(self):
        """æ·˜æ±°æœ€æ—§çš„ç¼“å­˜"""
        if not self._memory_cache:
            return

        # æ‰¾åˆ°æœ€æ—§çš„ç¼“å­˜
        oldest_key = min(
            self._memory_cache.items(),
            key=lambda x: x[1][1],
        )

        del self.memory_cache[oldest_key]
        logger.info(f"ğŸ—‘ï¸ [LLMç¼“å­˜] æ·˜æ±°æ—§ç¼“å­˜: key={oldest_key[:16]}...")

    def clear(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        cache_size = len(self._memory_cache)
        self._memory_cache.clear()
        logger.info(f"ğŸ—‘ï¸ [LLMç¼“å­˜] å·²æ¸…é™¤ç¼“å­˜: å…±{cache_size}æ¡")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return {
            "backend": self.cache_backend,
            "size": len(self.memory_cache),
            "max_size": self.max_size,
            "hit_rate": self._calculate_hit_rate(),
        }

    def _calculate_hit_rate(self) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        total_hits = sum(hit_count for _, _, hit_count in self._memory_cache.values())
        total_access = len(self.memory_cache)
        return (total_hits / total_access * 100) if total_access > 0 else 0


# å…¨å±€ç¼“å­˜å®ä¾‹
_llm_cache: Optional[LLMCache] = None


def get_llm_cache(
    cache_backend: str = "memory",
    max_size: int = 10000,
    default_ttl: int = 3600,
) -> LLMCache:
    """
    è·å–LLMç¼“å­˜å®ä¾‹(å•ä¾‹æ¨¡å¼)

    Args:
        cache_backend: ç¼“å­˜åç«¯
        max_size: æœ€å¤§ç¼“å­˜æ•°é‡
        default_ttl: é»˜è®¤TTL

    Returns:
        LLMç¼“å­˜å®ä¾‹
    """
    global _llm_cache

    if _llm_cache is None:
        _llm_cache = LLMCache(
            cache_backend=cache_backend,
            max_size=max_size,
            default_ttl=default_ttl,
        )

    return _llm_cache


def cache_llm_response(
    prompt: str,
    model: str,
    temperature: float = 0.7,
    max_tokens: int = 2000,
    ttl: Optional[int] = None,
) -> Optional[str]:
    """
    ç¼“å­˜LLMå“åº”(ä¾¿æ·å‡½æ•°)

    Args:
        prompt: æç¤ºè¯
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦
        max_tokens: æœ€å¤§tokenæ•°
        ttl: TTL(ç§’),Noneåˆ™ä½¿ç”¨é»˜è®¤TTL

    Returns:
        ç¼“å­˜çš„å“åº”,å¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸåˆ™è¿”å›None
    """
    cache = get_llm_cache()
    return cache.get(prompt, model, temperature, max_tokens, ttl)


def save_llm_response(
    prompt: str,
    response: str,
    model: str,
    temperature: float = 0.7,
    max_tokens: int = 2000,
    ttl: Optional[int] = None,
):
    """
    ä¿å­˜LLMå“åº”åˆ°ç¼“å­˜(ä¾¿æ·å‡½æ•°)

    Args:
        prompt: æç¤ºè¯
        response: LLMå“åº”
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦
        max_tokens: æœ€å¤§tokenæ•°
        ttl: TTL(ç§’),Noneåˆ™ä½¿ç”¨é»˜è®¤TTL
    """
    cache = get_llm_cache()
    cache.set(prompt, response, model, temperature, max_tokens, ttl)


def clear_llm_cache():
    """æ¸…é™¤LLMç¼“å­˜"""
    cache = get_llm_cache()
    cache.clear()
