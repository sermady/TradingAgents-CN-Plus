# -*- coding: utf-8 -*-
"""
æ•°æ®åè°ƒå™¨èŠ‚ç‚¹ - è´Ÿè´£é¢„è·å–æ‰€æœ‰å¿…è¦çš„æ•°æ®ï¼ˆä»…é™Aè‚¡ï¼‰
ç»•è¿‡ LLM å·¥å…·ç»‘å®šï¼Œç›´æ¥è°ƒç”¨ç»Ÿä¸€æ•°æ®è·å–æ–¹æ³•

ä¼˜åŒ–ç‰¹æ€§:
1. å¤šçº§é™çº§ç­–ç•¥: Tushare â†’ Baostock â†’ AkShare â†’ ç¼“å­˜
2. æ•°æ®éªŒè¯é›†æˆ: è‡ªåŠ¨éªŒè¯ä»·æ ¼ã€æˆäº¤é‡ã€åŸºæœ¬é¢æŒ‡æ ‡
3. æ•°æ®è´¨é‡è¯„åˆ†: åœ¨ state ä¸­æ·»åŠ  data_quality_score
4. å¹¶è¡Œæ•°æ®è·å–: ä½¿ç”¨ ThreadPoolExecutor æå‡æ€§èƒ½
5. ç»Ÿä¸€ç¼“å­˜ç­–ç•¥: æ”¯æŒåˆ†æçº§ç¼“å­˜ï¼ˆ5åˆ†é’ŸTTLï¼‰
6. PSæ¯”ç‡éªŒè¯: åœ¨é¢„å–é˜¶æ®µéªŒè¯å¹¶ä¿®æ­£PSè®¡ç®—
7. æˆäº¤é‡å•ä½ç»Ÿä¸€: ç»Ÿä¸€è½¬æ¢ä¸º"è‚¡"
8. æ•°æ®æºè¶…æ—¶ä¸é‡è¯•: æ¯ä¸ªæ•°æ®æºç‹¬ç«‹è¶…æ—¶å’ŒæŒ‡æ•°é€€é¿é‡è¯•
"""

import time
import json
import re
import asyncio
from typing import Dict, Any, Optional, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from dataclasses import dataclass, field
from functools import wraps

from tradingagents.agents.utils.agent_states import AgentState
from tradingagents.utils.logging_init import get_logger

logger = get_logger("data_coordinator")


# ==================== é‡è¯•æœºåˆ¶ ====================


def retry_with_backoff(
    max_retries: int = 3, base_delay: float = 1.0, max_delay: float = 10.0
):
    """æŒ‡æ•°é€€é¿é‡è¯•è£…é¥°å™¨"""

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        delay = min(base_delay * (2**attempt), max_delay)
                        logger.warning(
                            f"âš ï¸ {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}ï¼Œ{delay:.1f}s åé‡è¯•..."
                        )
                        time.sleep(delay)
                    else:
                        logger.error(
                            f"âŒ {func.__name__} æ‰€æœ‰ {max_retries} æ¬¡å°è¯•éƒ½å¤±è´¥"
                        )
            raise last_exception

        return wrapper

    return decorator


@dataclass
class DataFetchResult:
    """æ•°æ®è·å–ç»“æœå°è£…"""

    data: str
    source: str
    quality_score: float
    issues: List[Dict[str, Any]]
    fetch_time: float
    metadata: Dict[str, Any] = field(default_factory=dict)


class DataCoordinator:
    """
    æ•°æ®åè°ƒå™¨ - é›†ä¸­ç®¡ç†æ•°æ®é¢„å–å’ŒéªŒè¯

    åŠŸèƒ½:
    1. å¤šæ•°æ®æºè‡ªåŠ¨é™çº§ (Tushare â†’ Baostock â†’ AkShare)
    2. æ•°æ®éªŒè¯é›†æˆ (ä»·æ ¼ã€æˆäº¤é‡ã€åŸºæœ¬é¢)
    3. è´¨é‡è¯„åˆ†è®¡ç®—
    4. å¹¶è¡Œæ•°æ®è·å–
    5. PSæ¯”ç‡æºå¤´éªŒè¯
    6. æˆäº¤é‡å•ä½ç»Ÿä¸€
    7. è¶…æ—¶ä¸é‡è¯•æœºåˆ¶
    """

    # æ•°æ®æºä¼˜å…ˆçº§ (ç”¨äºé™çº§)
    DATA_SOURCE_PRIORITY = ["tushare", "baostock", "akshare"]

    # æ•°æ®ç±»å‹æ˜ å°„
    DATA_TYPES = {
        "market": {
            "name": "å¸‚åœºæ•°æ®",
            "validator": "price",
            "weight": 0.20,
        },
        "financial": {
            "name": "åŸºæœ¬é¢æ•°æ®",
            "validator": "fundamentals",
            "weight": 0.20,
        },
        "news": {
            "name": "æ–°é—»æ•°æ®",
            "validator": None,
            "weight": 0.20,
        },
        "sentiment": {
            "name": "èˆ†æƒ…æ•°æ®",
            "validator": None,
            "weight": 0.20,
        },
        "china_market": {
            "name": "Aè‚¡ç‰¹è‰²æ•°æ®",
            "validator": None,
            "weight": 0.20,
        },
    }

    # å„æ•°æ®æºè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    SOURCE_TIMEOUT = {
        "tushare": 10,
        "baostock": 15,
        "akshare": 15,
    }

    def __init__(self):
        self.validators = {}
        self._init_validators()
        self.cache = {}  # ç®€å•çš„å†…å­˜ç¼“å­˜
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜
        self.analysis_cache = {}  # åˆ†æçº§ç¼“å­˜ï¼ˆæŒ‰è‚¡ç¥¨ä»£ç +æ—¥æœŸï¼‰
        self.analysis_cache_ttl = 300  # 5åˆ†é’Ÿ

    def _init_validators(self):
        """åˆå§‹åŒ–æ•°æ®éªŒè¯å™¨"""
        try:
            from tradingagents.dataflows.validators.price_validator import (
                PriceValidator,
            )
            from tradingagents.dataflows.validators.volume_validator import (
                VolumeValidator,
            )
            from tradingagents.dataflows.validators.fundamentals_validator import (
                FundamentalsValidator,
            )

            self.validators["price"] = PriceValidator(tolerance=0.01)
            self.validators["volume"] = VolumeValidator(tolerance=0.05)
            self.validators["fundamentals"] = FundamentalsValidator(tolerance=0.05)
            logger.info("âœ… æ•°æ®éªŒè¯å™¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®éªŒè¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.validators = {}

    def _get_cache_key(self, symbol: str, data_type: str, date: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{symbol}_{data_type}_{date}"

    def _get_cached_data(self, key: str) -> Optional[str]:
        """è·å–ç¼“å­˜æ•°æ®"""
        if key in self.cache:
            cached_time, data = self.cache[key]
            if time.time() - cached_time < self.cache_ttl:
                logger.info(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜æ•°æ®: {key}")
                return data
            else:
                # ç¼“å­˜è¿‡æœŸ
                del self.cache[key]
        return None

    def _set_cached_data(self, key: str, data: str):
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        self.cache[key] = (time.time(), data)

    def _parse_market_data(self, data_str: str) -> Dict[str, Any]:
        """è§£æå¸‚åœºæ•°æ®å­—ç¬¦ä¸²ä¸ºç»“æ„åŒ–æ•°æ®"""
        result = {}
        if not data_str or "âŒ" in data_str:
            return result

        try:
            # æå–å…³é”®æŒ‡æ ‡
            patterns = {
                "current_price": r"æœ€æ–°ä»·[ï¼š:]\s*(\d+\.?\d*)",
                "open": r"ä»Šå¼€[ï¼š:]\s*(\d+\.?\d*)",
                "high": r"æœ€é«˜[ï¼š:]\s*(\d+\.?\d*)",
                "low": r"æœ€ä½[ï¼š:]\s*(\d+\.?\d*)",
                "volume": r"æˆäº¤é‡[ï¼š:]\s*(\d+\.?\d*)",
                "volume_unit": r"æˆäº¤é‡[ï¼š:]\s*\d+\.?\d*\s*(\w+)",
                "turnover_rate": r"æ¢æ‰‹ç‡[ï¼š:]\s*(\d+\.?\d*)",
                "MA5": r"MA5[ï¼š:]\s*(\d+\.?\d*)",
                "MA10": r"MA10[ï¼š:]\s*(\d+\.?\d*)",
                "MA20": r"MA20[ï¼š:]\s*(\d+\.?\d*)",
                "RSI": r"RSI\d*[ï¼š:]\s*(\d+\.?\d*)",
            }

            for key, pattern in patterns.items():
                matches = re.findall(pattern, data_str)
                if matches:
                    try:
                        result[key] = float(matches[0])
                    except (ValueError, TypeError):
                        result[key] = matches[0]

            # æ·»åŠ æ•°æ®æºä¿¡æ¯
            if "æ¥æº:" in data_str or "æ•°æ®æ¥æº" in data_str:
                source_match = re.search(r"æ¥æº[:ï¼š]\s*(\w+)", data_str)
                if source_match:
                    result["source"] = source_match.group(1).lower()

        except Exception as e:
            logger.debug(f"è§£æå¸‚åœºæ•°æ®å¤±è´¥: {e}")

        return result

    def _parse_fundamentals_data(self, data_str: str) -> Dict[str, Any]:
        """è§£æåŸºæœ¬é¢æ•°æ®å­—ç¬¦ä¸²ä¸ºç»“æ„åŒ–æ•°æ®"""
        result = {}
        if not data_str or "âŒ" in data_str:
            return result

        try:
            # æå–å…³é”®æŒ‡æ ‡
            patterns = {
                "PE": r"å¸‚ç›ˆ[ç‡\(]\w*[\)ï¼š]?\s*(\-?\d+\.?\d*)",
                "PB": r"å¸‚å‡€[ç‡\(]\w*[\)ï¼š]?\s*(\d+\.?\d*)",
                "PS": r"å¸‚é”€[ç‡\(]\w*[\)ï¼š]?\s*(\d+\.?\d*)",
                "market_cap": r"æ€»å¸‚å€¼[ï¼š:]\s*(\d+\.?\d*)",
                "revenue": r"æ€»è¥æ”¶[ï¼š:]\s*(\d+\.?\d*)",
                "ROE": r"ROE[ï¼ˆ\(]?å‡€èµ„äº§æ”¶ç›Šç‡[ï¼‰\)]?[ï¼š:]\s*(\-?\d+\.?\d*)",
                "ROA": r"ROA[ï¼š:]\s*(\-?\d+\.?\d*)",
                "gross_margin": r"æ¯›åˆ©ç‡[ï¼š:]\s*(\d+\.?\d*)",
                "net_margin": r"å‡€åˆ©ç‡[ï¼š:]\s*(\d+\.?\d*)",
                "debt_ratio": r"èµ„äº§è´Ÿå€ºç‡[ï¼š:]\s*(\d+\.?\d*)",
                "share_count": r"æ€»è‚¡æœ¬[ï¼š:]\s*(\d+\.?\d*)",
            }

            for key, pattern in patterns.items():
                matches = re.findall(pattern, data_str)
                if matches:
                    try:
                        value = float(matches[0])
                        # å¤„ç†å•ä½è½¬æ¢ï¼ˆå¦‚æœæ˜¯ä¸‡æˆ–äº¿ï¼‰
                        if (
                            "äº¿"
                            in data_str[
                                data_str.find(matches[0]) : data_str.find(matches[0])
                                + 10
                            ]
                        ):
                            result[key] = value  # å·²ç»æ˜¯äº¿
                        elif (
                            "ä¸‡"
                            in data_str[
                                data_str.find(matches[0]) : data_str.find(matches[0])
                                + 10
                            ]
                        ):
                            result[key] = value / 10000  # ä¸‡è½¬äº¿
                        else:
                            result[key] = value
                    except (ValueError, TypeError):
                        result[key] = matches[0]

            # æ·»åŠ æ•°æ®æºä¿¡æ¯
            if "æ¥æº:" in data_str or "æ•°æ®æ¥æº" in data_str:
                source_match = re.search(r"æ¥æº[:ï¼š]\s*(\w+)", data_str)
                if source_match:
                    result["source"] = source_match.group(1).lower()

        except Exception as e:
            logger.debug(f"è§£æåŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")

        return result

    def _validate_data(
        self, data_type: str, symbol: str, data: Dict[str, Any]
    ) -> Tuple[float, List[Dict]]:
        """
        éªŒè¯æ•°æ®å¹¶è¿”å›è´¨é‡è¯„åˆ†

        Returns:
            (quality_score, issues)
        """
        if data_type not in self.DATA_TYPES:
            return 1.0, []

        validator_type = self.DATA_TYPES[data_type].get("validator")
        if not validator_type or validator_type not in self.validators:
            return 1.0, []

        try:
            validator = self.validators[validator_type]
            result = validator.validate(symbol, data)

            # è½¬æ¢é—®é¢˜åˆ—è¡¨
            issues = []
            for issue in result.discrepancies:
                issues.append(
                    {
                        "severity": issue.severity.value,
                        "message": issue.message,
                        "field": issue.field,
                        "expected": issue.expected,
                        "actual": issue.actual,
                    }
                )

            return result.confidence, issues

        except Exception as e:
            logger.warning(f"éªŒè¯ {data_type} æ•°æ®å¤±è´¥: {e}")
            return 0.8, [
                {"severity": "warning", "message": f"éªŒè¯å¤±è´¥: {e}", "field": ""}
            ]

    def _get_market_data_with_fallback(
        self, symbol: str, trade_date: str
    ) -> DataFetchResult:
        """
        è·å–å¸‚åœºæ•°æ®ï¼ˆå¸¦é™çº§ç­–ç•¥ï¼‰

        é™çº§é¡ºåº: Tushare â†’ Baostock â†’ AkShare â†’ ç¼“å­˜
        """
        start_time = time.time()

        # 1. å…ˆæ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(symbol, "market", trade_date)
        cached = self._get_cached_data(cache_key)
        if cached:
            return DataFetchResult(
                data=cached,
                source="cache",
                quality_score=0.9,
                issues=[],
                fetch_time=time.time() - start_time,
            )

        # 2. å°è¯•å„ä¸ªæ•°æ®æº
        sources = self.DATA_SOURCE_PRIORITY
        last_error = None

        for source in sources:
            try:
                logger.info(f"ğŸ“ˆ å°è¯•ä» {source} è·å–å¸‚åœºæ•°æ®...")
                data = self._fetch_market_data_from_source(symbol, trade_date, source)

                if data and "âŒ" not in str(data):
                    # è§£æå¹¶éªŒè¯æ•°æ®
                    parsed = self._parse_market_data(data)
                    quality_score, issues = self._validate_data(
                        "market", symbol, parsed
                    )

                    # æ ‡è®°æ•°æ®æ¥æº
                    if parsed:
                        data += f"\næ•°æ®æ¥æº: {source}"

                    # ç¼“å­˜æˆåŠŸæ•°æ®
                    self._set_cached_data(cache_key, data)

                    fetch_time = time.time() - start_time
                    logger.info(
                        f"âœ… {source} å¸‚åœºæ•°æ®è·å–æˆåŠŸ (è´¨é‡åˆ†: {quality_score:.2f}, è€—æ—¶: {fetch_time:.2f}s)"
                    )

                    return DataFetchResult(
                        data=data,
                        source=source,
                        quality_score=quality_score,
                        issues=issues,
                        fetch_time=fetch_time,
                    )

            except Exception as e:
                logger.warning(f"âš ï¸ {source} å¸‚åœºæ•°æ®è·å–å¤±è´¥: {e}")
                last_error = e
                continue

        # 3. æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥
        error_msg = f"âŒ å¸‚åœºæ•°æ®è·å–å¤±è´¥ (å·²å°è¯•: {', '.join(sources)})"
        if last_error:
            error_msg += f": {last_error}"

        return DataFetchResult(
            data=error_msg,
            source="failed",
            quality_score=0.0,
            issues=[{"severity": "critical", "message": error_msg, "field": ""}],
            fetch_time=time.time() - start_time,
        )

    def _get_fundamentals_data_with_fallback(
        self, symbol: str, trade_date: str
    ) -> DataFetchResult:
        """
        è·å–åŸºæœ¬é¢æ•°æ®ï¼ˆå¸¦é™çº§ç­–ç•¥ï¼‰

        é™çº§é¡ºåº: Tushare â†’ Baostock â†’ AkShare â†’ ç¼“å­˜

        ä¼˜åŒ–:
        1. PSæ¯”ç‡æºå¤´éªŒè¯å’Œä¿®æ­£
        2. æˆäº¤é‡å•ä½ç»Ÿä¸€
        3. è¯¦ç»†æ•°æ®è´¨é‡æ ‡è®°
        """
        start_time = time.time()

        # 1. å…ˆæ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(symbol, "financial", trade_date)
        cached = self._get_cached_data(cache_key)
        if cached:
            return DataFetchResult(
                data=cached,
                source="cache",
                quality_score=0.9,
                issues=[],
                fetch_time=time.time() - start_time,
            )

        # 2. å°è¯•å„ä¸ªæ•°æ®æº
        sources = self.DATA_SOURCE_PRIORITY
        last_error = None

        for source in sources:
            try:
                logger.info(f"ğŸ’° å°è¯•ä» {source} è·å–åŸºæœ¬é¢æ•°æ®...")
                data = self._fetch_fundamentals_data_from_source(
                    symbol, trade_date, source
                )

                if data and "âŒ" not in str(data):
                    # è§£ææ•°æ®
                    parsed = self._parse_fundamentals_data(data)

                    # åŸºç¡€éªŒè¯
                    quality_score, issues = self._validate_data(
                        "financial", symbol, parsed
                    )

                    # ========== PS æ¯”ç‡æºå¤´éªŒè¯å’Œä¿®æ­£ ==========
                    ps_issues, corrected_ps = self._validate_and_fix_ps_ratio(
                        parsed, symbol
                    )
                    if ps_issues:
                        issues.extend(ps_issues)
                        if corrected_ps:
                            # å¦‚æœæœ‰ä¿®æ­£å€¼ï¼Œåœ¨æ•°æ®ä¸­æ ‡æ³¨
                            data = self._add_ps_correction_to_data(data, corrected_ps)
                            quality_score = max(0.3, quality_score - 0.1)  # è½»å¾®æ‰£åˆ†
                        else:
                            quality_score = max(0.3, quality_score - 0.2)  # ä¸¥é‡æ‰£åˆ†

                    # ========== æˆäº¤é‡å•ä½æ ‡å‡†åŒ– ==========
                    parsed, volume_unit_info = self._standardize_volume_unit(
                        parsed, data
                    )
                    if volume_unit_info in [
                        "converted_from_lots",
                        "inferred_lots_converted",
                    ]:
                        # æ·»åŠ å•ä½è½¬æ¢æ ‡è®°
                        data += f"\næˆäº¤é‡å•ä½: å·²ç»Ÿä¸€è½¬æ¢ä¸º'è‚¡'ï¼ˆåŸå§‹æ•°æ®å¯èƒ½æ˜¯'æ‰‹'ï¼‰"
                        logger.info(f"ğŸ“Š æˆäº¤é‡å•ä½è½¬æ¢: {symbol} {volume_unit_info}")

                    # æ ‡è®°æ•°æ®æ¥æº
                    if parsed:
                        data += f"\næ•°æ®æ¥æº: {source}"

                    # ç¼“å­˜æˆåŠŸæ•°æ®
                    self._set_cached_data(cache_key, data)

                    fetch_time = time.time() - start_time
                    logger.info(
                        f"âœ… {source} åŸºæœ¬é¢æ•°æ®è·å–æˆåŠŸ (è´¨é‡åˆ†: {quality_score:.2f}, è€—æ—¶: {fetch_time:.2f}s)"
                    )

                    return DataFetchResult(
                        data=data,
                        source=source,
                        quality_score=quality_score,
                        issues=issues,
                        fetch_time=fetch_time,
                        metadata={
                            "corrected_ps": corrected_ps,
                            "volume_unit_info": volume_unit_info,
                        },
                    )

            except Exception as e:
                logger.warning(f"âš ï¸ {source} åŸºæœ¬é¢æ•°æ®è·å–å¤±è´¥: {e}")
                last_error = e
                continue

        # 3. æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥
        error_msg = f"âŒ åŸºæœ¬é¢æ•°æ®è·å–å¤±è´¥ (å·²å°è¯•: {', '.join(sources)})"
        if last_error:
            error_msg += f": {last_error}"

        return DataFetchResult(
            data=error_msg,
            source="failed",
            quality_score=0.0,
            issues=[{"severity": "critical", "message": error_msg, "field": ""}],
            fetch_time=time.time() - start_time,
        )

    def _validate_and_fix_ps_ratio(
        self, data: Dict[str, Any], symbol: str
    ) -> Tuple[List[Dict], Optional[float]]:
        """
        éªŒè¯å¹¶ä¿®æ­£ PS æ¯”ç‡è®¡ç®—

        Returns:
            (issues, corrected_ps): é—®é¢˜åˆ—è¡¨å’Œä¿®æ­£åçš„PSå€¼ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        issues = []
        corrected_ps = None

        market_cap = data.get("market_cap")
        revenue = data.get("revenue")
        ps = data.get("PS") or data.get("ps_ratio")

        if not all([market_cap, revenue]) or revenue <= 0:
            return issues, corrected_ps

        try:
            # ç¡®ä¿æ•°å€¼ç±»å‹
            market_cap = float(market_cap)
            revenue = float(revenue)
            calculated_ps = market_cap / revenue

            if ps is not None:
                ps = float(ps)
                diff_pct = abs((calculated_ps - ps) / ps) * 100 if ps != 0 else 100

                if diff_pct > 20:  # å·®å¼‚è¶…è¿‡20%
                    issues.append(
                        {
                            "severity": "error",
                            "message": f"PSæ¯”ç‡è®¡ç®—é”™è¯¯! æŠ¥å‘Šå€¼={ps:.2f}, æ­£ç¡®å€¼åº”ä¸ºâ‰ˆ{calculated_ps:.2f} (å¸‚å€¼={market_cap:.2f}äº¿/è¥æ”¶={revenue:.2f}äº¿)",
                            "field": "PS",
                            "expected": round(calculated_ps, 2),
                            "actual": ps,
                        }
                    )
                    corrected_ps = calculated_ps
                elif diff_pct > 10:  # å·®å¼‚è¶…è¿‡10%
                    issues.append(
                        {
                            "severity": "warning",
                            "message": f"PSæ¯”ç‡å¯èƒ½å­˜åœ¨åå·®: æŠ¥å‘Šå€¼={ps:.2f}, è®¡ç®—å€¼={calculated_ps:.2f}",
                            "field": "PS",
                            "expected": round(calculated_ps, 2),
                            "actual": ps,
                        }
                    )
            else:
                # æ•°æ®ä¸­æ²¡æœ‰PSï¼Œä½†å¯ä»¥æ ¹æ®å¸‚å€¼å’Œè¥æ”¶è®¡ç®—
                corrected_ps = calculated_ps
                issues.append(
                    {
                        "severity": "info",
                        "message": f"å·²è‡ªåŠ¨è®¡ç®—PSæ¯”ç‡={calculated_ps:.2f} (å¸‚å€¼={market_cap:.2f}äº¿/è¥æ”¶={revenue:.2f}äº¿)",
                        "field": "PS",
                        "expected": round(calculated_ps, 2),
                    }
                )

            # æ£€æŸ¥PSæ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
            if calculated_ps < 0.1 or calculated_ps > 100:
                issues.append(
                    {
                        "severity": "warning",
                        "message": f"PSæ¯”ç‡={calculated_ps:.2f} è¶…å‡ºå¸¸è§„èŒƒå›´(0.1-100)",
                        "field": "PS",
                        "actual": round(calculated_ps, 2),
                    }
                )

        except (ValueError, TypeError, ZeroDivisionError) as e:
            issues.append(
                {
                    "severity": "warning",
                    "message": f"PSæ¯”ç‡éªŒè¯å¤±è´¥: {e}",
                    "field": "PS",
                }
            )

        return issues, corrected_ps

    def _standardize_volume_unit(
        self, data: Dict[str, Any], data_str: str
    ) -> Tuple[Dict[str, Any], str]:
        """
        æ ‡å‡†åŒ–æˆäº¤é‡å•ä½ä¸º"è‚¡"

        Returns:
            (updated_data, unit_info): æ›´æ–°åçš„æ•°æ®å’Œå•ä½ä¿¡æ¯
        """
        volume = data.get("volume")
        if volume is None:
            return data, "unknown"

        try:
            volume = float(volume)

            # æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦æ˜ç¡®æ ‡æ³¨äº†å•ä½
            if "æ‰‹" in data_str and "ä¸‡è‚¡" not in data_str:
                # æ•°æ®å•ä½æ˜¯"æ‰‹"ï¼Œè½¬æ¢ä¸º"è‚¡"
                converted_volume = volume * 100
                data["volume"] = converted_volume
                data["volume_unit"] = "shares"
                data["original_volume"] = volume
                data["original_unit"] = "lots"
                return data, "converted_from_lots"

            # å¦‚æœæ²¡æœ‰æ˜ç¡®æ ‡æ³¨ï¼Œæ ¹æ®æ•°å€¼å¤§å°æ¨æ–­
            # å¦‚æœæˆäº¤é‡å°äº10000ï¼Œå¯èƒ½æ˜¯"æ‰‹"ï¼›å¦‚æœå¤§äº100ä¸‡ï¼Œå¯èƒ½æ˜¯"è‚¡"
            if volume < 10000 and volume > 0:
                # å¯èƒ½æ˜¯"æ‰‹"ï¼Œä½†ä¹Ÿå¯èƒ½æ˜¯å°ç›˜è‚¡
                # æ£€æŸ¥æ˜¯å¦æœ‰æ¢æ‰‹ç‡å¯ä»¥éªŒè¯
                turnover_rate = data.get("turnover_rate")
                share_count = data.get("share_count")

                if turnover_rate and share_count:
                    # æ ¹æ®æ¢æ‰‹ç‡éªŒè¯
                    # æ¢æ‰‹ç‡ = æˆäº¤é‡(è‚¡) / æ€»è‚¡æœ¬(è‚¡) * 100
                    share_count_shares = share_count * 10000  # ä¸‡è‚¡è½¬è‚¡
                    calculated_turnover_as_shares = (volume / share_count_shares) * 100
                    calculated_turnover_as_lots = (
                        volume * 100 / share_count_shares
                    ) * 100

                    diff_as_shares = abs(calculated_turnover_as_shares - turnover_rate)
                    diff_as_lots = abs(calculated_turnover_as_lots - turnover_rate)

                    if diff_as_lots < diff_as_shares:
                        # å•ä½æ˜¯"æ‰‹"
                        converted_volume = volume * 100
                        data["volume"] = converted_volume
                        data["volume_unit"] = "shares"
                        data["original_volume"] = volume
                        data["original_unit"] = "lots"
                        return data, "inferred_lots_converted"

            # é»˜è®¤ä¸º"è‚¡"
            data["volume_unit"] = "shares"
            return data, "shares"

        except (ValueError, TypeError):
            return data, "unknown"

    def _add_ps_correction_to_data(self, data_str: str, corrected_ps: float) -> str:
        """
        åœ¨åŸºæœ¬é¢æ•°æ®å­—ç¬¦ä¸²ä¸­æ·»åŠ PSä¿®æ­£æ ‡è®°
        """
        if corrected_ps is None:
            return data_str

        correction_note = f"\n\nâš ï¸ **PSæ¯”ç‡ä¿®æ­£**: æ•°æ®æºæŠ¥å‘Šçš„PSæ¯”ç‡å¯èƒ½æœ‰è¯¯ï¼Œæ­£ç¡®è®¡ç®—å€¼åº”ä¸º {corrected_ps:.2f}\n"
        correction_note += f"ä¿®æ­£å…¬å¼: PS = æ€»å¸‚å€¼ / æ€»è¥æ”¶\n"

        return data_str + correction_note

    def _fetch_market_data_from_source(
        self, symbol: str, trade_date: str, source: str
    ) -> str:
        """ä»æŒ‡å®šæ•°æ®æºè·å–å¸‚åœºæ•°æ®"""
        from tradingagents.dataflows.interface import get_china_stock_data_unified

        # ä¸´æ—¶åˆ‡æ¢æ•°æ®æº
        import os

        original_source = os.environ.get("DEFAULT_CHINA_DATA_SOURCE", "akshare")

        try:
            os.environ["DEFAULT_CHINA_DATA_SOURCE"] = source
            return get_china_stock_data_unified(symbol, trade_date, trade_date)
        finally:
            os.environ["DEFAULT_CHINA_DATA_SOURCE"] = original_source

    def _fetch_fundamentals_data_from_source(
        self, symbol: str, trade_date: str, source: str
    ) -> str:
        """ä»æŒ‡å®šæ•°æ®æºè·å–åŸºæœ¬é¢æ•°æ®"""
        from tradingagents.agents.utils.agent_utils import Toolkit

        # ä¸´æ—¶åˆ‡æ¢æ•°æ®æº
        import os

        original_source = os.environ.get("DEFAULT_CHINA_DATA_SOURCE", "akshare")

        try:
            os.environ["DEFAULT_CHINA_DATA_SOURCE"] = source
            return Toolkit.get_stock_fundamentals_unified.func(
                ticker=symbol,
                start_date=trade_date,
                end_date=trade_date,
                curr_date=trade_date,
            )
        finally:
            os.environ["DEFAULT_CHINA_DATA_SOURCE"] = original_source

    def _get_news_data(self, symbol: str, trade_date: str) -> DataFetchResult:
        """è·å–æ–°é—»æ•°æ®"""
        start_time = time.time()

        try:
            logger.info(f"ğŸ“° æ­£åœ¨è·å–æ–°é—»æ•°æ®...")
            from tradingagents.agents.utils.agent_utils import Toolkit

            news_data = Toolkit.get_stock_news_unified.func(
                ticker=symbol, curr_date=trade_date
            )

            fetch_time = time.time() - start_time

            # è¯„ä¼°æ–°é—»æ•°æ®è´¨é‡ï¼ˆåŸºäºæ–°é—»æ•°é‡ï¼‰
            quality_score = 0.8
            if news_data and "âŒ" not in news_data:
                news_count = news_data.count("æ ‡é¢˜:") + news_data.count("æ ‡é¢˜ï¼š")
                if news_count >= 5:
                    quality_score = 1.0
                elif news_count >= 3:
                    quality_score = 0.9
                elif news_count >= 1:
                    quality_score = 0.7
                else:
                    quality_score = 0.5

            logger.info(
                f"âœ… æ–°é—»æ•°æ®è·å–æˆåŠŸ (è´¨é‡åˆ†: {quality_score:.2f}, è€—æ—¶: {fetch_time:.2f}s)"
            )

            return DataFetchResult(
                data=news_data if news_data else "æš‚æ— ç›¸å…³æ–°é—»æ•°æ®",
                source="unified",
                quality_score=quality_score,
                issues=[],
                fetch_time=fetch_time,
            )

        except Exception as e:
            error_msg = f"âŒ æ–°é—»æ•°æ®è·å–å¤±è´¥: {e}"
            logger.error(error_msg)
            return DataFetchResult(
                data=error_msg,
                source="failed",
                quality_score=0.0,
                issues=[{"severity": "error", "message": str(e), "field": ""}],
                fetch_time=time.time() - start_time,
            )

    def _get_sentiment_data(self, symbol: str, trade_date: str) -> DataFetchResult:
        """è·å–èˆ†æƒ…æ•°æ®"""
        start_time = time.time()

        try:
            logger.info(f"ğŸ˜Š æ­£åœ¨è·å–èˆ†æƒ…æ•°æ®...")
            from tradingagents.dataflows.interface import get_chinese_social_sentiment

            sentiment_data = get_chinese_social_sentiment(symbol, trade_date)

            fetch_time = time.time() - start_time

            # è¯„ä¼°èˆ†æƒ…æ•°æ®è´¨é‡
            quality_score = 0.7
            if sentiment_data and "âŒ" not in sentiment_data:
                if "æƒ…ç»ªæŒ‡æ•°" in sentiment_data or "èˆ†æƒ…" in sentiment_data:
                    quality_score = 0.9

            logger.info(
                f"âœ… èˆ†æƒ…æ•°æ®è·å–æˆåŠŸ (è´¨é‡åˆ†: {quality_score:.2f}, è€—æ—¶: {fetch_time:.2f}s)"
            )

            return DataFetchResult(
                data=sentiment_data if sentiment_data else "æš‚æ— èˆ†æƒ…æ•°æ®",
                source="unified",
                quality_score=quality_score,
                issues=[],
                fetch_time=fetch_time,
            )

        except Exception as e:
            error_msg = f"âŒ èˆ†æƒ…æ•°æ®è·å–å¤±è´¥: {e}"
            logger.error(error_msg)
            return DataFetchResult(
                data=error_msg,
                source="failed",
                quality_score=0.0,
                issues=[{"severity": "error", "message": str(e), "field": ""}],
                fetch_time=time.time() - start_time,
            )

    def _get_china_market_features_data(
        self, symbol: str, trade_date: str
    ) -> DataFetchResult:
        """
        è·å–Aè‚¡ç‰¹è‰²æ•°æ®ï¼ˆæ¶¨è·Œåœã€æ¢æ‰‹ç‡ã€é‡æ¯”ã€åŒ—å‘èµ„é‡‘ç­‰ï¼‰

        è¿™äº›æ•°æ®ä¸“é—¨ç”¨äºä¸­å›½å¸‚åœºåˆ†æå¸ˆï¼Œèšç„¦Aè‚¡å¸‚åœºç‰¹è‰²æŒ‡æ ‡
        """
        start_time = time.time()

        try:
            logger.info(f"ğŸ‡¨ğŸ‡³ æ­£åœ¨è·å–Aè‚¡ç‰¹è‰²æ•°æ®...")

            # æ„å»ºAè‚¡ç‰¹è‰²æ•°æ®å­—ç¬¦ä¸²
            china_features_data = []
            china_features_data.append(f"=== Aè‚¡å¸‚åœºç‰¹è‰²æ•°æ® ===")
            china_features_data.append(f"è‚¡ç¥¨ä»£ç : {symbol}")
            china_features_data.append(f"æ•°æ®æ—¥æœŸ: {trade_date}")
            china_features_data.append("")

            # å°è¯•è·å–æ¶¨è·Œåœæ•°æ®
            try:
                from tradingagents.dataflows.interface import (
                    get_china_stock_data_unified,
                )

                market_data = get_china_stock_data_unified(
                    symbol, trade_date, trade_date
                )

                # æå–å…³é”®æŒ‡æ ‡
                if market_data and "âŒ" not in market_data:
                    # è§£ææ¶¨è·ŒåœçŠ¶æ€
                    china_features_data.append("ã€æ¶¨è·Œåœåˆ†æã€‘")

                    # æå–ä»·æ ¼æ•°æ®
                    import re

                    price_match = re.search(r"æœ€æ–°ä»·[ï¼š:]\s*(\d+\.?\d*)", market_data)
                    high_match = re.search(r"æœ€é«˜[ï¼š:]\s*(\d+\.?\d*)", market_data)
                    low_match = re.search(r"æœ€ä½[ï¼š:]\s*(\d+\.?\d*)", market_data)
                    open_match = re.search(r"ä»Šå¼€[ï¼š:]\s*(\d+\.?\d*)", market_data)

                    if all([price_match, open_match]):
                        current_price = float(price_match.group(1))
                        open_price = float(open_match.group(1))
                        change_pct = ((current_price - open_price) / open_price) * 100

                        china_features_data.append(f"å½“å‰ä»·æ ¼: {current_price}")
                        china_features_data.append(f"ä»Šæ—¥å¼€ç›˜: {open_price}")
                        china_features_data.append(f"æ¶¨è·Œå¹…: {change_pct:.2f}%")

                        # åˆ¤æ–­æ˜¯å¦è§¦åŠæ¶¨è·Œåœ
                        if change_pct >= 9.5:
                            china_features_data.append("âš ï¸ è§¦åŠæ¶¨åœæ¿ï¼ˆæˆ–æ¥è¿‘æ¶¨åœï¼‰")
                        elif change_pct <= -9.5:
                            china_features_data.append("âš ï¸ è§¦åŠè·Œåœæ¿ï¼ˆæˆ–æ¥è¿‘è·Œåœï¼‰")
                        elif change_pct >= 5:
                            china_features_data.append("ğŸ“ˆ å¤§å¹…ä¸Šæ¶¨")
                        elif change_pct <= -5:
                            china_features_data.append("ğŸ“‰ å¤§å¹…ä¸‹è·Œ")
                        else:
                            china_features_data.append("â¡ï¸ æ­£å¸¸æ³¢åŠ¨")

                    china_features_data.append("")

                    # æå–æ¢æ‰‹ç‡
                    turnover_match = re.search(
                        r"æ¢æ‰‹ç‡[ï¼š:]\s*(\d+\.?\d*)", market_data
                    )
                    if turnover_match:
                        turnover = float(turnover_match.group(1))
                        china_features_data.append("ã€æ¢æ‰‹ç‡åˆ†æã€‘")
                        china_features_data.append(f"æ¢æ‰‹ç‡: {turnover:.2f}%")

                        if turnover < 1:
                            china_features_data.append(
                                "ğŸ’¤ æä½æ¢æ‰‹ï¼šäº¤æ˜“æ¸…æ·¡ï¼ŒæµåŠ¨æ€§å·®"
                            )
                        elif turnover < 3:
                            china_features_data.append(
                                "ğŸ”„ ä½æ¢æ‰‹ï¼šæ­£å¸¸èŒƒå›´ï¼Œäº¤æ˜“ä¸æ´»è·ƒ"
                            )
                        elif turnover < 7:
                            china_features_data.append("âš¡ ä¸­ç­‰æ¢æ‰‹ï¼šæ­£å¸¸æ´»è·ƒ")
                        elif turnover < 10:
                            china_features_data.append(
                                "ğŸ”¥ é«˜æ¢æ‰‹ï¼šé«˜åº¦æ´»è·ƒï¼Œå…³æ³¨èµ„é‡‘åŠ¨å‘"
                            )
                        elif turnover < 20:
                            china_features_data.append(
                                "ğŸš¨ æé«˜æ¢æ‰‹ï¼šå¼‚å¸¸æ´»è·ƒï¼Œå¯èƒ½æœ‰é‡å¤§æ¶ˆæ¯"
                            )
                        else:
                            china_features_data.append(
                                "âš ï¸ è¶…é«˜æ¢æ‰‹ï¼šæåº¦æ´»è·ƒï¼Œé«˜é£é™©é«˜æœºä¼š"
                            )

                        china_features_data.append("")

                    # æå–é‡æ¯”ï¼ˆå¦‚æœæœ‰ï¼‰
                    volume_ratio_match = re.search(
                        r"é‡æ¯”[ï¼š:]\s*(\d+\.?\d*)", market_data
                    )
                    if volume_ratio_match:
                        volume_ratio = float(volume_ratio_match.group(1))
                        china_features_data.append("ã€é‡æ¯”åˆ†æã€‘")
                        china_features_data.append(f"é‡æ¯”: {volume_ratio:.2f}")

                        if volume_ratio < 0.5:
                            china_features_data.append("ğŸ“‰ ä¸¥é‡ç¼©é‡ï¼šæˆäº¤æ¸…æ·¡")
                        elif volume_ratio < 0.8:
                            china_features_data.append("ğŸ“‰ ç¼©é‡ï¼šäº¤æ˜“æ´»è·ƒåº¦ä¸‹é™")
                        elif volume_ratio < 1.5:
                            china_features_data.append("â¡ï¸ æ­£å¸¸æ”¾é‡")
                        elif volume_ratio < 2.5:
                            china_features_data.append("ğŸ“ˆ æ˜æ˜¾æ”¾é‡ï¼šèµ„é‡‘å…³æ³¨åº¦æå‡")
                        elif volume_ratio < 5:
                            china_features_data.append("ğŸ”¥ æ˜¾è‘—æ”¾é‡ï¼šå¤§é‡èµ„é‡‘ä»‹å…¥")
                        else:
                            china_features_data.append("ğŸš¨ å¼‚å¸¸æ”¾é‡ï¼šéœ€å…³æ³¨æ¶ˆæ¯é¢")

                        china_features_data.append("")

                    # æå–æŒ¯å¹…
                    amplitude_match = re.search(r"æŒ¯å¹…[ï¼š:]\s*(\d+\.?\d*)", market_data)
                    if amplitude_match:
                        amplitude = float(amplitude_match.group(1))
                        china_features_data.append("ã€æŒ¯å¹…åˆ†æã€‘")
                        china_features_data.append(f"æŒ¯å¹…: {amplitude:.2f}%")

                        if amplitude < 2:
                            china_features_data.append("ğŸ’¤ çª„å¹…æ³¢åŠ¨")
                        elif amplitude < 5:
                            china_features_data.append("ğŸ“Š æ­£å¸¸æ³¢åŠ¨")
                        elif amplitude < 10:
                            china_features_data.append("âš¡ å®½å¹…æ³¢åŠ¨")
                        else:
                            china_features_data.append("ğŸš¨ å‰§çƒˆæ³¢åŠ¨")

                        china_features_data.append("")

                    # æ ‡è®°æ•°æ®æ¥æº
                    china_features_data.append(f"æ•°æ®æ¥æº: å¸‚åœºæ•°æ®æ¥å£")

            except Exception as e:
                logger.warning(f"âš ï¸ è·å–Aè‚¡ç‰¹è‰²æ•°æ®å¤±è´¥: {e}")
                china_features_data.append(f"âš ï¸ éƒ¨åˆ†æ•°æ®è·å–å¤±è´¥: {e}")

            final_data = "\n".join(china_features_data)
            fetch_time = time.time() - start_time

            # è¯„ä¼°æ•°æ®è´¨é‡
            quality_score = 0.8
            if "è§¦åŠæ¶¨åœæ¿" in final_data or "æ¢æ‰‹ç‡" in final_data:
                quality_score = 0.95
            elif "æ¶¨è·Œå¹…" in final_data:
                quality_score = 0.85

            logger.info(
                f"âœ… Aè‚¡ç‰¹è‰²æ•°æ®è·å–æˆåŠŸ (è´¨é‡åˆ†: {quality_score:.2f}, è€—æ—¶: {fetch_time:.2f}s)"
            )

            return DataFetchResult(
                data=final_data,
                source="unified",
                quality_score=quality_score,
                issues=[],
                fetch_time=fetch_time,
            )

        except Exception as e:
            error_msg = f"âŒ Aè‚¡ç‰¹è‰²æ•°æ®è·å–å¤±è´¥: {e}"
            logger.error(error_msg)
            return DataFetchResult(
                data=error_msg,
                source="failed",
                quality_score=0.0,
                issues=[{"severity": "error", "message": str(e), "field": ""}],
                fetch_time=time.time() - start_time,
            )

    def fetch_all_data(
        self,
        symbol: str,
        trade_date: str,
        parallel: bool = True,
        use_cache: bool = True,
    ) -> Dict[str, Any]:
        """
        è·å–æ‰€æœ‰ç±»å‹çš„æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            trade_date: äº¤æ˜“æ—¥æœŸ
            parallel: æ˜¯å¦å¹¶è¡Œè·å–
            use_cache: æ˜¯å¦ä½¿ç”¨åˆ†æçº§ç¼“å­˜

        Returns:
            åŒ…å«æ‰€æœ‰æ•°æ®å’Œè´¨é‡è¯„åˆ†çš„å­—å…¸
        """
        # æ£€æŸ¥åˆ†æçº§ç¼“å­˜
        cache_key = f"analysis:{symbol}:{trade_date}"
        if use_cache:
            cached_result = self._get_analysis_cache(cache_key)
            if cached_result:
                logger.info(
                    f"ğŸ’¾ [Data Coordinator] ä½¿ç”¨åˆ†æçº§ç¼“å­˜: {symbol} (å‰©ä½™TTL: {self._get_cache_ttl(cache_key):.0f}s)"
                )
                return cached_result

        logger.info(
            f"ğŸ”„ [Data Coordinator] å¼€å§‹è·å– {symbol} çš„æ‰€æœ‰æ•°æ® (å¹¶è¡Œ={parallel})"
        )
        start_time = time.time()

        results = {}

        if parallel:
            # å¹¶è¡Œè·å–æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬Aè‚¡ç‰¹è‰²æ•°æ®ï¼‰
            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = {
                    executor.submit(
                        self._get_market_data_with_fallback, symbol, trade_date
                    ): "market",
                    executor.submit(
                        self._get_fundamentals_data_with_fallback, symbol, trade_date
                    ): "financial",
                    executor.submit(self._get_news_data, symbol, trade_date): "news",
                    executor.submit(
                        self._get_sentiment_data, symbol, trade_date
                    ): "sentiment",
                    executor.submit(
                        self._get_china_market_features_data, symbol, trade_date
                    ): "china_market",
                }

                for future in as_completed(futures):
                    data_type = futures[future]
                    try:
                        result = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                        results[data_type] = result
                    except Exception as e:
                        logger.error(f"âŒ {data_type} æ•°æ®è·å–è¶…æ—¶æˆ–å¤±è´¥: {e}")
                        results[data_type] = DataFetchResult(
                            data=f"âŒ {data_type} æ•°æ®è·å–å¤±è´¥: {e}",
                            source="failed",
                            quality_score=0.0,
                            issues=[
                                {"severity": "critical", "message": str(e), "field": ""}
                            ],
                            fetch_time=0,
                        )
        else:
            # ä¸²è¡Œè·å–
            results["market"] = self._get_market_data_with_fallback(symbol, trade_date)
            results["financial"] = self._get_fundamentals_data_with_fallback(
                symbol, trade_date
            )
            results["news"] = self._get_news_data(symbol, trade_date)
            results["sentiment"] = self._get_sentiment_data(symbol, trade_date)
            results["china_market"] = self._get_china_market_features_data(
                symbol, trade_date
            )

        # è®¡ç®—æ€»ä½“è´¨é‡è¯„åˆ†
        total_weight = sum(
            self.DATA_TYPES[dt]["weight"] for dt in results if dt in self.DATA_TYPES
        )
        if total_weight > 0:
            overall_quality = (
                sum(
                    results[dt].quality_score * self.DATA_TYPES[dt]["weight"]
                    for dt in results
                    if dt in self.DATA_TYPES
                )
                / total_weight
            )
        else:
            overall_quality = 0.0

        total_time = time.time() - start_time
        logger.info(
            f"âœ… [Data Coordinator] æ‰€æœ‰æ•°æ®è·å–å®Œæˆ (æ€»ä½“è´¨é‡åˆ†: {overall_quality:.2f}, æ€»è€—æ—¶: {total_time:.2f}s)"
        )

        # æ”¶é›† metadataï¼ˆå¦‚ PS ä¿®æ­£å€¼ã€æˆäº¤é‡å•ä½ç­‰ï¼‰
        financial_metadata = results.get(
            "financial", DataFetchResult("", "", 0.0, [], 0)
        ).metadata

        # æ„å»ºè¿”å›ç»“æœ
        result = {
            "market_data": results.get(
                "market", DataFetchResult("", "", 0.0, [], 0)
            ).data,
            "financial_data": results.get(
                "financial", DataFetchResult("", "", 0.0, [], 0)
            ).data,
            "news_data": results.get("news", DataFetchResult("", "", 0.0, [], 0)).data,
            "sentiment_data": results.get(
                "sentiment", DataFetchResult("", "", 0.0, [], 0)
            ).data,
            "china_market_data": results.get(
                "china_market", DataFetchResult("", "", 0.0, [], 0)
            ).data,
            "data_quality_score": overall_quality,
            "data_sources": {
                "market": results.get(
                    "market", DataFetchResult("", "", 0.0, [], 0)
                ).source,
                "financial": results.get(
                    "financial", DataFetchResult("", "", 0.0, [], 0)
                ).source,
                "news": results.get("news", DataFetchResult("", "", 0.0, [], 0)).source,
                "sentiment": results.get(
                    "sentiment", DataFetchResult("", "", 0.0, [], 0)
                ).source,
                "china_market": results.get(
                    "china_market", DataFetchResult("", "", 0.0, [], 0)
                ).source,
            },
            "data_issues": {
                "market": results.get(
                    "market", DataFetchResult("", "", 0.0, [], 0)
                ).issues,
                "financial": results.get(
                    "financial", DataFetchResult("", "", 0.0, [], 0)
                ).issues,
                "news": results.get("news", DataFetchResult("", "", 0.0, [], 0)).issues,
                "sentiment": results.get(
                    "sentiment", DataFetchResult("", "", 0.0, [], 0)
                ).issues,
                "china_market": results.get(
                    "china_market", DataFetchResult("", "", 0.0, [], 0)
                ).issues,
            },
            "data_metadata": {
                "corrected_ps": financial_metadata.get("corrected_ps"),
                "volume_unit_info": financial_metadata.get("volume_unit_info"),
            },
            "fetch_time": total_time,
        }

        # ç¼“å­˜ç»“æœ
        if use_cache:
            self._set_analysis_cache(cache_key, result)

        return result

    # ==================== åˆ†æçº§ç¼“å­˜æ–¹æ³• ====================

    def _get_analysis_cache(self, key: str) -> Optional[Dict[str, Any]]:
        """è·å–åˆ†æçº§ç¼“å­˜"""
        if key not in self.analysis_cache:
            return None

        cached_data, expires_at = self.analysis_cache[key]
        if time.time() > expires_at:
            # ç¼“å­˜è¿‡æœŸ
            del self.analysis_cache[key]
            return None

        return cached_data

    def _set_analysis_cache(self, key: str, data: Dict[str, Any]) -> None:
        """è®¾ç½®åˆ†æçº§ç¼“å­˜"""
        expires_at = time.time() + self.analysis_cache_ttl
        self.analysis_cache[key] = (data, expires_at)
        logger.info(
            f"ğŸ’¾ [Data Coordinator] åˆ†æçº§ç¼“å­˜å·²è®¾ç½®: {key} (TTL: {self.analysis_cache_ttl}s)"
        )

    def _get_cache_ttl(self, key: str) -> float:
        """è·å–ç¼“å­˜å‰©ä½™æ—¶é—´"""
        if key not in self.analysis_cache:
            return 0
        _, expires_at = self.analysis_cache[key]
        return max(0, expires_at - time.time())

    def clear_analysis_cache(self) -> None:
        """æ¸…é™¤æ‰€æœ‰åˆ†æçº§ç¼“å­˜"""
        self.analysis_cache.clear()
        logger.info("ğŸ—‘ï¸ [Data Coordinator] åˆ†æçº§ç¼“å­˜å·²æ¸…é™¤")


# å…¨å±€ DataCoordinator å®ä¾‹
_data_coordinator = None


def get_data_coordinator() -> DataCoordinator:
    """è·å– DataCoordinator å®ä¾‹"""
    global _data_coordinator
    if _data_coordinator is None:
        _data_coordinator = DataCoordinator()
    return _data_coordinator


def data_coordinator_node(state: AgentState):
    """
    Data Coordinator Node - é›†ä¸­å¼æ•°æ®é¢„å–èŠ‚ç‚¹

    è´Ÿè´£é¢„è·å–æ‰€æœ‰ A è‚¡å¿…è¦çš„æ•°æ®ï¼ˆMarket, Fundamentals, News, Sentimentï¼‰
    å¹¶å­˜å‚¨åœ¨ AgentState ä¸­ä¾›ä¸‹æ¸¸åˆ†æå¸ˆä½¿ç”¨ã€‚

    è¿™ç§é›†ä¸­å¼æ–¹æ³•å¯ä»¥é¿å…ï¼š
    1. é‡å¤çš„ API è°ƒç”¨
    2. åˆ†æå¸ˆèŠ‚ç‚¹æ— é™å¾ªç¯å°è¯•è°ƒç”¨å·¥å…·
    3. å·¥å…·å¤±è´¥æ—¶äº§ç”Ÿå¹»è§‰

    âš¡ å…³é”®æ”¹è¿›ï¼š
    - å¤šçº§é™çº§ç­–ç•¥ (Tushare â†’ Baostock â†’ AkShare)
    - æ•°æ®éªŒè¯é›†æˆ
    - è´¨é‡è¯„åˆ†æœºåˆ¶
    - å¹¶è¡Œæ•°æ®è·å–
    """
    logger.info("ğŸ”„ [Data Coordinator] å¼€å§‹é›†ä¸­å¼æ•°æ®é¢„å–...")

    company = state.get("company_of_interest", "")
    trade_date = state.get("trade_date", "")

    # å°†åˆ†ææ—¥æœŸè®¾ç½®åˆ° Toolkit._configï¼Œç¡®ä¿å·¥å…·å‡½æ•°èƒ½è·å–åˆ°
    if trade_date:
        from tradingagents.agents.utils.agent_utils import Toolkit

        Toolkit.update_config({"trade_date": trade_date})
        logger.info(f"ğŸ“… [Data Coordinator] å·²è®¾ç½®åˆ†ææ—¥æœŸåˆ° Toolkit: {trade_date}")

    if not company:
        logger.error("âŒ [Data Coordinator] è‚¡ç¥¨ä»£ç ä¸ºç©º")
        return {
            "market_data": "âŒ é”™è¯¯ï¼šè‚¡ç¥¨ä»£ç ä¸ºç©º",
            "financial_data": "âŒ é”™è¯¯ï¼šè‚¡ç¥¨ä»£ç ä¸ºç©º",
            "news_data": "âŒ é”™è¯¯ï¼šè‚¡ç¥¨ä»£ç ä¸ºç©º",
            "sentiment_data": "âŒ é”™è¯¯ï¼šè‚¡ç¥¨ä»£ç ä¸ºç©º",
            "data_quality_score": 0.0,
            "data_sources": {},
        }

    # ğŸ”§ æ£€æµ‹è‚¡ç¥¨å¸‚åœºç±»å‹
    from tradingagents.utils.stock_utils import StockUtils

    market_info = StockUtils.get_market_info(company)
    is_china = market_info.get("is_china", False)

    if not is_china:
        logger.warning(
            f"âš ï¸ [Data Coordinator] éAè‚¡å¸‚åœºï¼ˆ{market_info.get('market_name', 'Unknown')}ï¼‰ï¼Œå½“å‰ä»…æ”¯æŒAè‚¡"
        )
        logger.info(f"ğŸ’¡ æç¤ºï¼šæ¸¯è‚¡/ç¾è‚¡åˆ†æå½“å‰ä¸å¯ç”¨ï¼Œä»…æ”¯æŒ A è‚¡åˆ†æ")
        return {
            "market_data": f"âš ï¸ ä¸æ”¯æŒçš„å¸‚åœº: {market_info.get('market_name', 'Unknown')}ï¼Œå½“å‰ä»…æ”¯æŒ A è‚¡",
            "financial_data": f"âš ï¸ ä¸æ”¯æŒçš„å¸‚åœº: {market_info.get('market_name', 'Unknown')}ï¼Œå½“å‰ä»…æ”¯æŒ A è‚¡",
            "news_data": f"âš ï¸ ä¸æ”¯æŒçš„å¸‚åœº: {market_info.get('market_name', 'Unknown')}ï¼Œå½“å‰ä»…æ”¯æŒ A è‚¡",
            "sentiment_data": f"âš ï¸ ä¸æ”¯æŒçš„å¸‚åœº: {market_info.get('market_name', 'Unknown')}ï¼Œå½“å‰ä»…æ”¯æŒ A è‚¡",
            "data_quality_score": 0.0,
            "data_sources": {
                "market": "unsupported",
                "financial": "unsupported",
                "news": "unsupported",
                "sentiment": "unsupported",
            },
        }

    # ä»…æ”¯æŒ A è‚¡æ•°æ®é¢„å–
    logger.info(f"ğŸ“Š ç›®æ ‡: {company}, äº¤æ˜“æ—¥æœŸ: {trade_date} (A è‚¡)")

    # ä½¿ç”¨äº¤æ˜“æ—¥ç®¡ç†å™¨ç¡®ä¿æ—¥æœŸæ­£ç¡®
    from tradingagents.utils.trading_date_manager import get_trading_date_manager

    date_mgr = get_trading_date_manager()
    adjusted_date = date_mgr.get_latest_trading_date(trade_date)
    if adjusted_date != trade_date:
        logger.info(f"ğŸ“… æ—¥æœŸè°ƒæ•´: {trade_date} â†’ {adjusted_date} (æœ€è¿‘äº¤æ˜“æ—¥)")
        trade_date = adjusted_date

    # ä½¿ç”¨ DataCoordinator è·å–æ‰€æœ‰æ•°æ®
    coordinator = get_data_coordinator()
    results = coordinator.fetch_all_data(company, trade_date, parallel=True)

    logger.info(f"âœ… [Data Coordinator] æ•°æ®é¢„å–å®Œæˆ")
    logger.info(
        f"   å¸‚åœºæ•°æ®è´¨é‡: {results.get('data_sources', {}).get('market', 'unknown')}"
    )
    logger.info(
        f"   åŸºæœ¬é¢æ•°æ®è´¨é‡: {results.get('data_sources', {}).get('financial', 'unknown')}"
    )
    logger.info(f"   æ€»ä½“è´¨é‡è¯„åˆ†: {results.get('data_quality_score', 0):.2f}")

    return {
        "market_data": results["market_data"],
        "financial_data": results["financial_data"],
        "news_data": results["news_data"],
        "sentiment_data": results["sentiment_data"],
        "china_market_data": results["china_market_data"],
        "data_quality_score": results["data_quality_score"],
        "data_sources": results["data_sources"],
        "data_issues": results.get("data_issues", {}),
        "data_metadata": results.get("data_metadata", {}),
    }
