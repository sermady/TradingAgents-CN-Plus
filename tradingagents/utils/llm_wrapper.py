# -*- coding: utf-8 -*-
"""
LLMè°ƒç”¨ç»Ÿä¸€è£…é¥°å™¨
æä¾›ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£,åŒ…å«è‡ªåŠ¨é‡è¯•ã€Tokenç»Ÿè®¡ã€é”™è¯¯å¤„ç†ç­‰åŠŸèƒ½
"""

import time
import functools
import hashlib
from typing import Optional, Callable, Any, Dict
from datetime import datetime

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import get_logger

logger = get_logger("default")


class LLMCallConfig:
    """LLMè°ƒç”¨é…ç½®"""

    def __init__(
        self,
        max_retries: int = 3,
        retry_delay: float = 2.0,
        validate_response: bool = True,
        min_response_length: int = 10,
        log_tokens: bool = True,
        log_performance: bool = True,
        cache_enabled: bool = True,
        cache_ttl: int = 3600,
    ):
        """
        åˆå§‹åŒ–LLMè°ƒç”¨é…ç½®

        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•å»¶è¿Ÿ(ç§’)
            validate_response: æ˜¯å¦éªŒè¯å“åº”
            min_response_length: æœ€å°å“åº”é•¿åº¦
            log_tokens: æ˜¯å¦è®°å½•Tokenä½¿ç”¨
            log_performance: æ˜¯å¦è®°å½•æ€§èƒ½
            cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
            cache_ttl: ç¼“å­˜æœ‰æ•ˆæœŸ(ç§’)
        """
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.validate_response = validate_response
        self.min_response_length = min_response_length
        self.log_tokens = log_tokens
        self.log_performance = log_performance
        self.cache_enabled = cache_enabled
        self.cache_ttl = cache_ttl


class LLMCallResult:
    """LLMè°ƒç”¨ç»“æœ"""

    def __init__(
        self,
        success: bool,
        content: str = "",
        error: Optional[str] = None,
        retry_count: int = 0,
        duration: float = 0.0,
        input_tokens: int = 0,
        output_tokens: int = 0,
        cached: bool = False,
    ):
        """
        åˆå§‹åŒ–LLMè°ƒç”¨ç»“æœ

        Args:
            success: æ˜¯å¦æˆåŠŸ
            content: å“åº”å†…å®¹
            error: é”™è¯¯ä¿¡æ¯
            retry_count: é‡è¯•æ¬¡æ•°
            duration: è€—æ—¶(ç§’)
            input_tokens: è¾“å…¥Tokenæ•°
            output_tokens: è¾“å‡ºTokenæ•°
            cached: æ˜¯å¦æ¥è‡ªç¼“å­˜
        """
        self.success = success
        self.content = content
        self.error = error
        self.retry_count = retry_count
        self.duration = duration
        self.input_tokens = input_tokens
        self.output_tokens = output_tokens
        self.cached = cached


# LLMå“åº”ç¼“å­˜(ç®€å•å®ç°)
_llm_response_cache: Dict[str, tuple] = {}


def _get_cache_key(prompt: str, llm_model: str, **kwargs) -> str:
    """
    ç”Ÿæˆç¼“å­˜é”®

    Args:
        prompt: æç¤ºè¯
        llm_model: LLMæ¨¡å‹åç§°
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        ç¼“å­˜é”®
    """
    key_string = f"{llm_model}:{prompt}:{str(sorted(kwargs.items()))}"
    return hashlib.md5(key_string.encode("utf-8")).hexdigest()


def _get_from_cache(cache_key: str, config: LLMCallConfig) -> Optional[LLMCallResult]:
    """
    ä»ç¼“å­˜è·å–ç»“æœ

    Args:
        cache_key: ç¼“å­˜é”®
        config: LLMè°ƒç”¨é…ç½®

    Returns:
        ç¼“å­˜ç»“æœ,å¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸåˆ™è¿”å›None
    """
    global _llm_response_cache

    if not config.cache_enabled:
        return None

    if cache_key in _llm_response_cache:
        content, timestamp = _llm_response_cache[cache_key]
        age = time.time() - timestamp

        if age < config.cache_ttl:
            logger.debug(
                f"ğŸ“¦ [LLMç¼“å­˜] å‘½ä¸­ç¼“å­˜ (TTLå‰©ä½™: {config.cache_ttl - age:.1f}ç§’)"
            )
            return LLMCallResult(
                success=True,
                content=content,
                cached=True,
            )
        else:
            # ç¼“å­˜è¿‡æœŸ,åˆ é™¤
            del _llm_response_cache[cache_key]
            logger.debug(f"ğŸ“¦ [LLMç¼“å­˜] ç¼“å­˜è¿‡æœŸ (å·²è¿‡æœŸ{age:.1f}ç§’)")

    return None


def _save_to_cache(cache_key: str, content: str, config: LLMCallConfig):
    """
    ä¿å­˜ç»“æœåˆ°ç¼“å­˜

    Args:
        cache_key: ç¼“å­˜é”®
        content: å“åº”å†…å®¹
        config: LLMè°ƒç”¨é…ç½®
    """
    global _llm_response_cache

    if not config.cache_enabled:
        return

    _llm_response_cache[cache_key] = (content, time.time())
    logger.debug(f"ğŸ“¦ [LLMç¼“å­˜] ç¼“å­˜ç»“æœ (TTL: {config.cache_ttl}ç§’)")


def llm_call(
    max_retries: int = 3,
    retry_delay: float = 2.0,
    validate_response: bool = True,
    min_response_length: int = 10,
    log_tokens: bool = True,
    log_performance: bool = True,
    cache_enabled: bool = True,
    cache_ttl: int = 3600,
    llm_name: str = "LLM",
    agent_name: str = "Agent",
):
    """
    LLMè°ƒç”¨è£…é¥°å™¨

    æä¾›ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£,åŒ…å«:
    - è‡ªåŠ¨é‡è¯•æœºåˆ¶
    - Tokenç»Ÿè®¡
    - é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
    - å“åº”éªŒè¯
    - æ€§èƒ½è®¡æ—¶
    - å“åº”ç¼“å­˜

    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•å»¶è¿Ÿ(ç§’)
        validate_response: æ˜¯å¦éªŒè¯å“åº”
        min_response_length: æœ€å°å“åº”é•¿åº¦
        log_tokens: æ˜¯å¦è®°å½•Tokenä½¿ç”¨
        log_performance: æ˜¯å¦è®°å½•æ€§èƒ½
        cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
        cache_ttl: ç¼“å­˜æœ‰æ•ˆæœŸ(ç§’)
        llm_name: LLMåç§°(ç”¨äºæ—¥å¿—)
        agent_name: Agentåç§°(ç”¨äºæ—¥å¿—)

    Returns:
        è£…é¥°åçš„å‡½æ•°

    Examples:
        >>> @llm_call(max_retries=3, llm_name="Google", agent_name="Market Analyst")
        >>> def call_market_llm(llm, prompt):
        >>>     return llm.invoke(prompt)
    """

    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(llm, *args, **kwargs) -> LLMCallResult:
            # åˆ›å»ºé…ç½®
            config = LLMCallConfig(
                max_retries=max_retries,
                retry_delay=retry_delay,
                validate_response=validate_response,
                min_response_length=min_response_length,
                log_tokens=log_tokens,
                log_performance=log_performance,
                cache_enabled=cache_enabled,
                cache_ttl=cache_ttl,
            )

            # å°è¯•ä»ç¼“å­˜è·å–
            # æå–promptä½œä¸ºç¼“å­˜é”®(å‡è®¾ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯prompt)
            prompt = args[0] if args else kwargs.get("prompt", "")
            llm_model = getattr(llm, "model", "unknown")

            cache_key = _get_cache_key(prompt, llm_model, **kwargs)
            cached_result = _get_from_cache(cache_key, config)
            if cached_result:
                return cached_result

            # æ‰§è¡ŒLLMè°ƒç”¨
            retry_count = 0
            last_error = None
            result_content = ""
            input_tokens = 0
            output_tokens = 0
            start_time = time.time()

            while retry_count < config.max_retries:
                try:
                    retry_count += 1
                    logger.info(
                        f"ğŸ”„ [{agent_name}] è°ƒç”¨{llm_name} (å°è¯• {retry_count}/{config.max_retries})"
                    )

                    # è°ƒç”¨LLM
                    response = func(llm, *args, **kwargs)

                    # æå–å†…å®¹
                    if hasattr(response, "content"):
                        result_content = response.content
                    elif isinstance(response, str):
                        result_content = response
                    else:
                        result_content = str(response)

                    # éªŒè¯å“åº”
                    if config.validate_response:
                        content_length = len(result_content)
                        if content_length < config.min_response_length:
                            logger.warning(
                                f"âš ï¸ [{agent_name}] å“åº”è¿‡çŸ­: {content_length}å­—ç¬¦ < {config.min_response_length}å­—ç¬¦"
                            )
                            # ç»§ç»­é‡è¯•
                            last_error = f"å“åº”è¿‡çŸ­: {content_length}å­—ç¬¦"
                            continue

                    # æå–Tokenä½¿ç”¨æƒ…å†µ
                    if hasattr(response, "response_metadata"):
                        metadata = response.response_metadata
                        if isinstance(metadata, dict) and "token_usage" in metadata:
                            token_usage = metadata["token_usage"]
                            input_tokens = token_usage.get("prompt_tokens", 0)
                            output_tokens = token_usage.get("completion_tokens", 0)

                            if config.log_tokens:
                                logger.info(
                                    f"ğŸ“Š [{agent_name}] Tokenä½¿ç”¨: è¾“å…¥={input_tokens}, "
                                    f"è¾“å‡º={output_tokens}, æ€»è®¡={input_tokens + output_tokens}"
                                )

                    # æˆåŠŸè°ƒç”¨
                    duration = time.time() - start_time

                    if config.log_performance:
                        logger.info(f"â±ï¸ [{agent_name}] LLMè°ƒç”¨è€—æ—¶: {duration:.2f}ç§’")
                        logger.info(
                            f"ğŸ“ [{agent_name}] å“åº”é•¿åº¦: {len(result_content)}å­—ç¬¦"
                        )

                    # ä¿å­˜åˆ°ç¼“å­˜
                    _save_to_cache(cache_key, result_content, config)

                    return LLMCallResult(
                        success=True,
                        content=result_content,
                        retry_count=retry_count,
                        duration=duration,
                        input_tokens=input_tokens,
                        output_tokens=output_tokens,
                        cached=False,
                    )

                except Exception as e:
                    last_error = str(e)
                    logger.error(
                        f"âŒ [{agent_name}] LLMè°ƒç”¨å¤±è´¥ (å°è¯• {retry_count}): {e}"
                    )

                    if retry_count < config.max_retries:
                        logger.info(
                            f"ğŸ”„ [{agent_name}] ç­‰å¾…{config.retry_delay}ç§’åé‡è¯•..."
                        )
                        time.sleep(config.retry_delay)

            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
            duration = time.time() - start_time
            logger.error(f"âŒ [{agent_name}] æ‰€æœ‰LLMè°ƒç”¨å°è¯•å¤±è´¥")

            # ç”Ÿæˆé»˜è®¤å“åº”
            default_response = f"""**é»˜è®¤å“åº”**

ç”±äºæŠ€æœ¯åŸå› ,{agent_name}æ— æ³•ç”Ÿæˆè¯¦ç»†åˆ†æã€‚

**é”™è¯¯ä¿¡æ¯:**
{last_error}

**å»ºè®®:**
1. æ£€æŸ¥LLM APIé…ç½®
2. æ£€æŸ¥ç½‘ç»œè¿æ¥
3. æ£€æŸ¥API Keyæ˜¯å¦æœ‰æ•ˆ
4. ç¨åé‡è¯•åˆ†æ

æ³¨æ„: æ­¤ä¸ºç³»ç»Ÿé»˜è®¤å“åº”,å»ºè®®ç»“åˆäººå·¥åˆ†æåšå‡ºæœ€ç»ˆå†³ç­–ã€‚"""

            return LLMCallResult(
                success=False,
                content=default_response,
                error=last_error,
                retry_count=retry_count,
                duration=duration,
            )

        return wrapper

    return decorator


def clear_llm_cache():
    """æ¸…é™¤LLMå“åº”ç¼“å­˜"""
    global _llm_response_cache
    cache_size = len(_llm_response_cache)
    _llm_response_cache.clear()
    logger.info(f"ğŸ—‘ï¸ [LLMç¼“å­˜] å·²æ¸…é™¤ç¼“å­˜ (å…±{cache_size}æ¡)")


def get_cache_stats() -> Dict[str, Any]:
    """
    è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

    Returns:
        ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
    """
    global _llm_response_cache

    return {
        "cache_size": len(_llm_response_cache),
        "cache_keys": list(_llm_response_cache.keys()),
    }
