# -*- coding: utf-8 -*-
"""
LLMé€‚é…å™¨ç»Ÿä¸€å·¥å‚
æä¾›ç»Ÿä¸€çš„LLMåˆ›å»ºæ¥å£,å‡å°‘LLMé…ç½®ä»£ç é‡å¤
"""

from typing import Optional, Dict, Any
from abc import ABC, abstractmethod

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import get_logger

logger = get_logger("default")


class BaseLLMProvider(ABC):
    """LLM ProvideråŸºç±»"""

    @abstractmethod
    def create_llm(
        self,
        model: str,
        api_key: str,
        temperature: float,
        max_tokens: int,
        timeout: int,
        base_url: Optional[str] = None,
        **kwargs,
    ) -> Any:
        """
        åˆ›å»ºLLMå®ä¾‹

        Args:
            model: æ¨¡å‹åç§°
            api_key: API Key
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¶…æ—¶æ—¶é—´
            base_url: è‡ªå®šä¹‰ç«¯ç‚¹
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMå®ä¾‹
        """
        pass

    @abstractmethod
    def validate_config(
        self, model: str, api_key: str, base_url: Optional[str]
    ) -> Dict[str, Any]:
        """
        éªŒè¯é…ç½®

        Args:
            model: æ¨¡å‹åç§°
            api_key: API Key
            base_url: è‡ªå®šä¹‰ç«¯ç‚¹

        Returns:
            éªŒè¯ç»“æœ {"valid": bool, "errors": list}
        """
        pass

    @abstractmethod
    def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨

        Returns:
            æ¨¡å‹å­—å…¸
        """
        pass


class GoogleProvider(BaseLLMProvider):
    """Google AI Provider"""

    def create_llm(
        self,
        model: str,
        api_key: str,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        timeout: int = 180,
        base_url: Optional[str] = None,
        **kwargs,
    ) -> Any:
        """åˆ›å»ºGoogle AI LLMå®ä¾‹"""
        from tradingagents.llm_adapters.google_openai_adapter import ChatGoogleOpenAI

        # éªŒè¯é…ç½®
        validation = self.validate_config(model, api_key, base_url)
        if not validation["valid"]:
            raise ValueError(f"Googleé…ç½®æ— æ•ˆ: {validation['errors']}")

        return ChatGoogleOpenAI(
            model=model,
            google_api_key=api_key,
            base_url=base_url,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
        )

    def validate_config(
        self, model: str, api_key: str, base_url: Optional[str]
    ) -> Dict[str, Any]:
        """éªŒè¯Googleé…ç½®"""
        errors = []

        if not api_key or len(api_key) < 10:
            errors.append("API Keyæ— æ•ˆæˆ–è¿‡çŸ­")

        return {
            "valid": len(errors) == 0,
            "errors": errors,
        }

    def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """è·å–Googleå¯ç”¨æ¨¡å‹"""
        from tradingagents.llm_adapters.google_openai_adapter import (
            GOOGLE_OPENAI_MODELS,
        )

        return GOOGLE_OPENAI_MODELS


class DashScopeProvider(BaseLLMProvider):
    """DashScope (é˜¿é‡Œç™¾ç‚¼) Provider"""

    def create_llm(
        self,
        model: str,
        api_key: str,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        timeout: int = 180,
        base_url: Optional[str] = None,
        **kwargs,
    ) -> Any:
        """åˆ›å»ºDashScope LLMå®ä¾‹"""
        from tradingagents.llm_adapters.dashscope_openai_adapter import (
            ChatDashScopeOpenAI,
        )

        # éªŒè¯é…ç½®
        validation = self.validate_config(model, api_key, base_url)
        if not validation["valid"]:
            raise ValueError(f"DashScopeé…ç½®æ— æ•ˆ: {validation['errors']}")

        return ChatDashScopeOpenAI(
            model=model,
            api_key=api_key,
            base_url=base_url,
            temperature=temperature,
            max_tokens=max_tokens,
            request_timeout=timeout,
        )

    def validate_config(
        self, model: str, api_key: str, base_url: Optional[str]
    ) -> Dict[str, Any]:
        """éªŒè¯DashScopeé…ç½®"""
        errors = []

        if not api_key or len(api_key) < 10:
            errors.append("API Keyæ— æ•ˆæˆ–è¿‡çŸ­")

        return {
            "valid": len(errors) == 0,
            "errors": errors,
        }

    def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """è·å–DashScopeå¯ç”¨æ¨¡å‹"""
        # DashScopeæ¨¡å‹åˆ—è¡¨(ä»æ–‡æ¡£è·å–)
        return {
            "qwen-turbo": {
                "description": "Qwen Turbo - å¿«é€Ÿæ¨¡å‹",
                "context_length": 8192,
                "recommended_for": ["å¿«é€Ÿå“åº”", "æ—¥å¸¸å¯¹è¯", "ç®€å•åˆ†æ"],
            },
            "qwen-plus": {
                "description": "Qwen Plus - å¢å¼ºæ¨¡å‹",
                "context_length": 32768,
                "recommended_for": ["å¤æ‚åˆ†æ", "ä¸“ä¸šä»»åŠ¡", "æ·±åº¦æ€è€ƒ"],
            },
            "qwen-max": {
                "description": "Qwen Max - æ——èˆ°æ¨¡å‹",
                "context_length": 32768,
                "recommended_for": ["å¤æ‚æ¨ç†", "ä¸“ä¸šåˆ†æ", "é«˜è´¨é‡è¾“å‡º"],
            },
        }


class DeepSeekProvider(BaseLLMProvider):
    """DeepSeek Provider"""

    def create_llm(
        self,
        model: str,
        api_key: str,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        timeout: int = 180,
        base_url: Optional[str] = None,
        **kwargs,
    ) -> Any:
        """åˆ›å»ºDeepSeek LLMå®ä¾‹"""
        from tradingagents.llm_adapters.deepseek_adapter import ChatDeepSeek

        # éªŒè¯é…ç½®
        validation = self.validate_config(model, api_key, base_url)
        if not validation["valid"]:
            raise ValueError(f"DeepSeeké…ç½®æ— æ•ˆ: {validation['errors']}")

        return ChatDeepSeek(
            model=model,
            api_key=api_key,
            base_url=base_url or "https://api.deepseek.com",
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
        )

    def validate_config(
        self, model: str, api_key: str, base_url: Optional[str]
    ) -> Dict[str, Any]:
        """éªŒè¯DeepSeeké…ç½®"""
        errors = []

        if not api_key or len(api_key) < 10:
            errors.append("API Keyæ— æ•ˆæˆ–è¿‡çŸ­")

        return {
            "valid": len(errors) == 0,
            "errors": errors,
        }

    def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """è·å–DeepSeekå¯ç”¨æ¨¡å‹"""
        return {
            "deepseek-chat": {
                "description": "DeepSeek Chat - é€šç”¨å¯¹è¯æ¨¡å‹",
                "context_length": 128000,
                "recommended_for": ["é€šç”¨å¯¹è¯", "ä»£ç ç†è§£", "ç®€å•åˆ†æ"],
            },
            "deepseek-coder": {
                "description": "DeepSeek Coder - ä»£ç ç”Ÿæˆæ¨¡å‹",
                "context_length": 128000,
                "recommended_for": ["ä»£ç ç”Ÿæˆ", "ä»£ç å®¡æŸ¥", "æŠ€æœ¯æ–‡æ¡£"],
            },
        }


class OpenAIProvider(BaseLLMProvider):
    """OpenAI Provider (åŒ…æ‹¬å…¼å®¹ç«¯ç‚¹)"""

    def create_llm(
        self,
        model: str,
        api_key: str,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        timeout: int = 180,
        base_url: Optional[str] = None,
        **kwargs,
    ) -> Any:
        """åˆ›å»ºOpenAI LLMå®ä¾‹"""
        from langchain_openai import ChatOpenAI

        # éªŒè¯é…ç½®
        validation = self.validate_config(model, api_key, base_url)
        if not validation["valid"]:
            raise ValueError(f"OpenAIé…ç½®æ— æ•ˆ: {validation['errors']}")

        return ChatOpenAI(
            model=model,
            api_key=api_key,
            base_url=base_url,
            temperature=tokenperature,
            max_tokens=max_tokens,
            timeout=timeout,
        )

    def validate_config(
        self, model: str, api_key: str, base_url: Optional[str]
    ) -> Dict[str, Any]:
        """éªŒè¯OpenAIé…ç½®"""
        errors = []

        if not api_key or len(api_key) < 10:
            errors.append("API Keyæ— æ•ˆæˆ–è¿‡çŸ­")

        return {
            "valid": len(errors) == 0,
            "errors": errors,
        }

    def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """è·å–OpenAIå¯ç”¨æ¨¡å‹"""
        return {
            "gpt-4o-mini": {
                "description": "GPT-4o Mini - å¿«é€Ÿæ¨¡å‹",
                "context_length": 128000,
                "recommended_for": ["å¿«é€Ÿå“åº”", "æ—¥å¸¸å¯¹è¯", "ç®€å•åˆ†æ"],
            },
            "gpt-4o": {
                "description": "GPT-4o - æ——èˆ°æ¨¡å‹",
                "context_length": 128000,
                "recommended_for": ["å¤æ‚åˆ†æ", "ä¸“ä¸šä»»åŠ¡", "æ·±åº¦æ¨ç†"],
            },
        }


class AnthropicProvider(BaseLLMProvider):
    """Anthropic Provider"""

    def create_llm(
        self,
        model: str,
        api_key: str,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        timeout: int = 180,
        base_url: Optional[str] = None,
        **kwargs,
    ) -> Any:
        """åˆ›å»ºAnthropic LLMå®ä¾‹"""
        from langchain_anthropic import ChatAnthropic

        # éªŒè¯é…ç½®
        validation = self.validate_config(model, api_key, base_url)
        if not validation["valid"]:
            raise ValueError(f"Anthropicé…ç½®æ— æ•ˆ: {validation['errors']}")

        return ChatAnthropic(
            model=model,
            api_key=api_key,
            base_url=base_url,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
        )

    def validate_config(
        self, model: str, api_key: str, base_url: Optional[str]
    ) -> Dict[str, Any]:
        """éªŒè¯Anthropicé…ç½®"""
        errors = []

        if not api_key or len(api_key) < 10:
            errors.append("API Keyæ— æ•ˆæˆ–è¿‡çŸ­")

        return {
            "valid": len(errors) == 0,
            "errors": errors,
        }

    def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """è·å–Anthropicå¯ç”¨æ¨¡å‹"""
        return {
            "claude-3-sonnet": {
                "description": "Claude 3 Sonnet - å¹³è¡¡æ¨¡å‹",
                "context_length": 200000,
                "recommended_for": ["å¤æ‚åˆ†æ", "ä¸“ä¸šä»»åŠ¡", "é«˜è´¨é‡è¾“å‡º"],
            },
            "claude-3-opus": {
                "description": "Claude 3 Opus - å¼ºå¤§æ¨¡å‹",
                "context_length": 200000,
                "recommended_for": ["å¤æ‚æ¨ç†", "ä¸“ä¸šåˆ†æ", "é«˜çº§åˆ›æ„"],
            },
        }


class CustomProvider(BaseLLMProvider):
    """è‡ªå®šä¹‰Provider(OpenAIå…¼å®¹ç«¯ç‚¹)"""

    def create_llm(
        self,
        model: str,
        api_key: str,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        timeout: int = 180,
        base_url: Optional[str] = None,
        provider_name: str = "custom",
        **kwargs,
    ) -> Any:
        """åˆ›å»ºè‡ªå®šä¹‰LLMå®ä¾‹"""
        from langchain_openai import ChatOpenAI

        # éªŒè¯é…ç½®
        validation = self.validate_config(model, api_key, base_url)
        if not validation["valid"]:
            raise ValueError(f"è‡ªå®šä¹‰Provideré…ç½®æ— æ•ˆ: {validation['errors']}")

        return ChatOpenAI(
            model=model,
            api_key=api_key,
            base_url=base_url,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
        )

    def validate_config(
        self, model: str, api_key: str, base_url: Optional[str]
    ) -> Dict[str, Any]:
        """éªŒè¯è‡ªå®šä¹‰Provideré…ç½®"""
        errors = []

        if not base_url:
            errors.append("å¿…é¡»æä¾›base_url")

        if not api_key or len(api_key) < 10:
            errors.append("API Keyæ— æ•ˆæˆ–è¿‡çŸ­")

        return {
            "valid": len(errors) == 0,
            "errors": errors,
        }

    def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """è·å–è‡ªå®šä¹‰Providerå¯ç”¨æ¨¡å‹"""
        # è¿”å›ç©ºå­—å…¸,éœ€è¦ç”¨æˆ·é…ç½®
        return {}


class LLMFactory:
    """LLMå·¥å‚"""

    def __init__(self):
        """åˆå§‹åŒ–LLMå·¥å‚"""
        self._providers: Dict[str, BaseLLMProvider] = {
            "google": GoogleProvider(),
            "dashscope": DashScopeProvider(),
            "alibaba": DashScopeProvider(),  # é˜¿é‡Œç™¾ç‚¼
            "deepseek": DeepSeekProvider(),
            "openai": OpenAIProvider(),
            "siliconflow": OpenAIProvider(),
            "openrouter": OpenAIProvider(),
            "ollama": OpenAIProvider(),
            "anthropic": AnthropicProvider(),
            "zhipu": CustomProvider(),
            "qianfan": CustomProvider(),
            "custom": CustomProvider(),
        }

        logger.info(f"ğŸ­ï¸ [LLMå·¥å‚] å·²æ³¨å†Œçš„Provider: {list(self._providers.keys())}")

    def register_provider(
        self,
        name: str,
        provider: BaseLLMProvider,
    ):
        """
        æ³¨å†Œæ–°çš„LLM Provider

        Args:
            name: Provideråç§°
            provider: Providerå®ä¾‹
        """
        self._providers[name] = provider
        logger.info(f"ğŸ“ [LLMå·¥å‚] æ³¨å†ŒProvider: {name}")

    def create_llm(
        self,
        provider_name: str,
        model: str,
        api_key: str,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        timeout: int = 180,
        base_url: Optional[str] = None,
        **kwargs,
    ) -> Any:
        """
        åˆ›å»ºLLMå®ä¾‹

        Args:
            provider_name: Provideråç§°
            model: æ¨¡å‹åç§°
            api_key: API Key
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¶…æ—¶æ—¶é—´
            base_url: è‡ªå®šä¹‰ç«¯ç‚¹
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMå®ä¾‹

        Raises:
            ValueError: ä¸æ”¯æŒçš„Provideræˆ–é…ç½®æ— æ•ˆ
        """
        # æ ‡å‡†åŒ–provideråç§°
        provider_name = provider_name.lower()

        # æŸ¥æ‰¾provider
        if provider_name not in self._providers:
            available = ", ".join(self._providers.keys())
            raise ValueError(
                f"ä¸æ”¯æŒçš„Provider: {provider_name}. å¯ç”¨Provider: {available}"
            )

        # è·å–provider
        provider = self._providers[provider_name]

        logger.info(f"ğŸ­ï¸ [LLMå·¥å‚] åˆ›å»ºLLM: Provider={provider_name}, Model={model}")

        # åˆ›å»ºLLM
        llm = provider.create_llm(
            model=model,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
            base_url=base_url,
            **kwargs,
        )

        logger.info(f"âœ… [LLMå·¥å‚] LLMåˆ›å»ºæˆåŠŸ")

        return llm

    def validate_config(
        self,
        provider_name: str,
        model: str,
        api_key: str,
        base_url: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        éªŒè¯é…ç½®

        Args:
            provider_name: Provideråç§°
            model: æ¨¡å‹åç§°
            api_key: API Key
            base_url: è‡ªå®šä¹‰ç«¯ç‚¹

        Returns:
            éªŒè¯ç»“æœ {"valid": bool, "errors": list, "provider": BaseLLMProvider}
        """
        provider_name = provider_name.lower()

        if provider_name not in self._providers:
            raise ValueError(f"ä¸æ”¯æŒçš„Provider: {provider_name}")

        provider = self._providers[provider_name]
        validation = provider.validate_config(model, api_key, base_url)

        return {
            "valid": validation["valid"],
            "errors": validation["errors"],
            "provider": provider,
        }

    def get_available_models(self, provider_name: str) -> Dict[str, Dict[str, Any]]:
        """
        è·å–Providerçš„å¯ç”¨æ¨¡å‹

        Args:
            provider_name: Provideråç§°

        Returns:
            æ¨¡å‹å­—å…¸
        """
        provider_name = provider_name.lower()

        if provider_name not in self._providers:
            raise ValueError(f"ä¸æ”¯æŒçš„Provider: {provider_name}")

        return self._providers[provider_name].get_available_models()

    def list_providers(self) -> list[str]:
        """
        åˆ—å‡ºæ‰€æœ‰Provider

        Returns:
            Provideråç§°åˆ—è¡¨
        """
        return list(self._providers.keys())

    def get_provider_info(self, provider_name: str) -> Dict[str, Any]:
        """
        è·å–Providerä¿¡æ¯

        Args:
            provider_name: Provideråç§°

        Returns:
            Providerä¿¡æ¯å­—å…¸
        """
        provider_name = provider_name.lower()

        if provider_name not in self._providers:
            raise ValueError(f"ä¸æ”¯æŒçš„Provider: {provider_name}")

        provider = self._providers[provider_name]

        return {
            "name": provider_name,
            "provider_class": provider.__class__.__name__,
            "available_models": provider.get_available_models(),
        }


# å…¨å±€LLMå·¥å‚å®ä¾‹
_llm_factory: Optional[LLMFactory] = None


def get_llm_factory() -> LLMFactory:
    """
    è·å–LLMå·¥å‚å®ä¾‹(å•ä¾‹æ¨¡å¼)

    Returns:
        LLMå·¥å‚å®ä¾‹
    """
    global _llm_factory

    if _llm_factory is None:
        _llm_factory = LLMFactory()

    return _llm_factory


def create_llm_by_factory(
    provider: str,
    model: str,
    api_key: str,
    temperature: float = 0.7,
    max_tokens: int = 2000,
    timeout: int = 180,
    base_url: Optional[str] = None,
    **kwargs,
) -> Any:
    """
    é€šè¿‡å·¥å‚åˆ›å»ºLLMå®ä¾‹(æ›¿ä»£trading_graph.pyä¸­çš„create_llm_by_provider)

    Args:
        provider: Provideråç§°
        model: æ¨¡å‹åç§°
        api_key: API Key
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        timeout: è¶…æ—¶æ—¶é—´
        base_url: è‡ªå®šä¹‰ç«¯ç‚¹
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        LLMå®ä¾‹
    """
    factory = get_llm_factory()
    return factory.create_llm(
        provider_name=provider,
        model=model,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout,
        base_url=base_url,
        **kwargs,
    )


def validate_llm_config(
    provider: str,
    model: str,
    api_key: str,
    base_url: Optional[str] = None,
) -> Dict[str, Any]:
    """
    éªŒè¯LLMé…ç½®

    Args:
        provider: Provideråç§°
        model: æ¨¡å‹åç§°
        api_key: API Key
        base_url: è‡ªå®šä¹‰ç«¯ç‚¹

    Returns:
        éªŒè¯ç»“æœ
    """
    factory = get_llm_factory()
    return factory.validate_config(provider, model, api_key, base_url)
