# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨åˆ†ææœåŠ¡
å°†ç°æœ‰çš„TradingAgentsåˆ†æåŠŸèƒ½åŒ…è£…æˆAPIæœåŠ¡
"""

import asyncio
import uuid
import json
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional, Callable
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# åˆå§‹åŒ–TradingAgentsæ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import init_logging

init_logging()

from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG
from app.services.simple_analysis_service import (
    create_analysis_config,
    get_provider_by_model_name,
)
from app.models.analysis import (
    AnalysisParameters,
    AnalysisResult,
    AnalysisTask,
    AnalysisBatch,
    AnalysisStatus,
    BatchStatus,
    SingleAnalysisRequest,
    BatchAnalysisRequest,
)
from app.models.user import PyObjectId
from bson import ObjectId
from app.core.database import get_mongo_db
from app.core.redis_client import get_redis_service, RedisKeys
from app.services.queue_service import QueueService
from app.core.database import get_redis_client
from app.services.redis_progress_tracker import RedisProgressTracker
from app.services.config_provider import provider as config_provider
from app.services.queue import (
    DEFAULT_USER_CONCURRENT_LIMIT,
    GLOBAL_CONCURRENT_LIMIT,
    VISIBILITY_TIMEOUT_SECONDS,
)
from app.services.usage_statistics_service import UsageStatisticsService
from app.services.progress_manager import get_progress_manager
from app.services.billing_service import get_billing_service
from app.models.config import UsageRecord
from app.core.config import settings
from app.core.unified_config_service import get_config_manager

import logging

logger = logging.getLogger(__name__)


class AnalysisService:
    """è‚¡ç¥¨åˆ†ææœåŠ¡ç±»"""

    def __init__(self):
        # è·å–Rediså®¢æˆ·ç«¯
        redis_client = get_redis_client()
        self.queue_service = QueueService(redis_client)
        # åˆå§‹åŒ–æœåŠ¡
        self.usage_service = UsageStatisticsService()
        self.progress_manager = get_progress_manager()
        self.billing_service = get_billing_service()
        self._trading_graph_cache = {}

    def _convert_user_id(self, user_id: str) -> PyObjectId:
        """å°†å­—ç¬¦ä¸²ç”¨æˆ·IDè½¬æ¢ä¸ºPyObjectId"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢ç”¨æˆ·ID: {user_id} (ç±»å‹: {type(user_id)})")

            # å¦‚æœæ˜¯adminç”¨æˆ·ï¼Œä½¿ç”¨é…ç½®çš„ObjectId
            if user_id == "admin":
                # ä½¿ç”¨é…ç½®ä¸­çš„ObjectIdä½œä¸ºadminç”¨æˆ·ID
                admin_object_id = ObjectId(settings.ADMIN_USER_ID)
                logger.info(f"ğŸ”„ è½¬æ¢adminç”¨æˆ·ID: {user_id} -> {admin_object_id}")
                return PyObjectId(admin_object_id)
            else:
                # å°è¯•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºObjectId
                object_id = ObjectId(user_id)
                logger.info(f"ğŸ”„ è½¬æ¢ç”¨æˆ·ID: {user_id} -> {object_id}")
                return PyObjectId(object_id)
        except Exception as e:
            logger.error(f"âŒ ç”¨æˆ·IDè½¬æ¢å¤±è´¥: {user_id} -> {e}")
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ObjectId
            new_object_id = ObjectId()
            logger.warning(f"âš ï¸ ç”Ÿæˆæ–°çš„ç”¨æˆ·ID: {new_object_id}")
            return PyObjectId(new_object_id)

    def _get_trading_graph(self, config: Dict[str, Any]) -> TradingAgentsGraph:
        """è·å–æˆ–åˆ›å»ºTradingAgentså›¾å®ä¾‹ï¼ˆå¸¦ç¼“å­˜ï¼‰- ä¸å•è‚¡åˆ†æä¿æŒä¸€è‡´"""
        config_key = json.dumps(config, sort_keys=True)

        if config_key not in self._trading_graph_cache:
            # ç›´æ¥ä½¿ç”¨å®Œæ•´é…ç½®ï¼Œä¸å†åˆå¹¶DEFAULT_CONFIGï¼ˆå› ä¸ºcreate_analysis_configå·²ç»å¤„ç†äº†ï¼‰
            # è¿™ä¸å•è‚¡åˆ†ææœåŠ¡å’Œwebç›®å½•çš„æ–¹å¼ä¸€è‡´
            self._trading_graph_cache[config_key] = TradingAgentsGraph(
                selected_analysts=config.get(
                    "selected_analysts", ["market", "fundamentals"]
                ),
                debug=config.get("debug", False),
                config=config,
            )

            logger.info(
                f"åˆ›å»ºæ–°çš„TradingAgentså®ä¾‹: {config.get('llm_provider', 'default')}"
            )

        return self._trading_graph_cache[config_key]

    def _execute_analysis_sync_with_progress(
        self, task: AnalysisTask, progress_tracker: RedisProgressTracker
    ) -> AnalysisResult:
        """åŒæ­¥æ‰§è¡Œåˆ†æä»»åŠ¡ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼Œå¸¦è¿›åº¦è·Ÿè¸ªï¼‰"""
        try:
            # åœ¨çº¿ç¨‹ä¸­é‡æ–°åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
            from tradingagents.utils.logging_init import init_logging, get_logger

            init_logging()
            thread_logger = get_logger("analysis_thread")

            thread_logger.info(
                f"ğŸ”„ [çº¿ç¨‹æ± ] å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.symbol}"
            )
            logger.info(f"ğŸ”„ [çº¿ç¨‹æ± ] å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.symbol}")

            # ç¯å¢ƒæ£€æŸ¥
            progress_tracker.update_progress("ğŸ”§ æ£€æŸ¥ç¯å¢ƒé…ç½®")

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½®
            # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨
            config_mgr = get_config_manager()
            quick_model = (
                getattr(task.parameters, "quick_analysis_model", None)
                or config_mgr.get_quick_analysis_model()
            )
            deep_model = (
                getattr(task.parameters, "deep_analysis_model", None)
                or config_mgr.get_deep_analysis_model()
            )

            # ğŸ”§ ä» MongoDB æ•°æ®åº“è¯»å–æ¨¡å‹çš„å®Œæ•´é…ç½®å‚æ•°ï¼ˆè€Œä¸æ˜¯ä» JSON æ–‡ä»¶ï¼‰
            quick_model_config = None
            deep_model_config = None

            try:
                from pymongo import MongoClient
                from app.core.config import settings

                # ä½¿ç”¨åŒæ­¥ MongoDB å®¢æˆ·ç«¯
                client = MongoClient(settings.MONGO_URI)
                db = client[settings.MONGO_DB]
                collection = db.system_configs

                # æŸ¥è¯¢æœ€æ–°çš„æ´»è·ƒé…ç½®
                doc = collection.find_one({"is_active": True}, sort=[("version", -1)])

                if doc and "llm_configs" in doc:
                    llm_configs = doc["llm_configs"]
                    logger.info(f"âœ… ä» MongoDB è¯»å–åˆ° {len(llm_configs)} ä¸ªæ¨¡å‹é…ç½®")

                    for llm_config in llm_configs:
                        if llm_config.get("model_name") == quick_model:
                            quick_model_config = {
                                "max_tokens": llm_config.get("max_tokens", 4000),
                                "temperature": llm_config.get("temperature", 0.7),
                                "timeout": llm_config.get("timeout", 180),
                                "retry_times": llm_config.get("retry_times", 3),
                                "api_base": llm_config.get("api_base"),
                            }
                            logger.info(f"âœ… è¯»å–å¿«é€Ÿæ¨¡å‹é…ç½®: {quick_model}")
                            logger.info(
                                f"   max_tokens={quick_model_config['max_tokens']}, temperature={quick_model_config['temperature']}"
                            )
                            logger.info(
                                f"   timeout={quick_model_config['timeout']}, retry_times={quick_model_config['retry_times']}"
                            )
                            logger.info(f"   api_base={quick_model_config['api_base']}")

                        if llm_config.get("model_name") == deep_model:
                            deep_model_config = {
                                "max_tokens": llm_config.get("max_tokens", 4000),
                                "temperature": llm_config.get("temperature", 0.7),
                                "timeout": llm_config.get("timeout", 180),
                                "retry_times": llm_config.get("retry_times", 3),
                                "api_base": llm_config.get("api_base"),
                            }
                            logger.info(
                                f"âœ… è¯»å–æ·±åº¦æ¨¡å‹é…ç½®: {deep_model} - {deep_model_config}"
                            )
                else:
                    logger.warning("âš ï¸ MongoDB ä¸­æ²¡æœ‰æ‰¾åˆ°ç³»ç»Ÿé…ç½®ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°")
            except Exception as e:
                logger.warning(f"âš ï¸ ä» MongoDB è¯»å–æ¨¡å‹é…ç½®å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°")

            # æˆæœ¬ä¼°ç®—
            progress_tracker.update_progress("ğŸ’° é¢„ä¼°åˆ†ææˆæœ¬")

            # æ ¹æ®æ¨¡å‹åç§°åŠ¨æ€æŸ¥æ‰¾ä¾›åº”å•†ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
            llm_provider = "dashscope"  # é»˜è®¤ä½¿ç”¨dashscope

            # å‚æ•°é…ç½®
            progress_tracker.update_progress("âš™ï¸ é…ç½®åˆ†æå‚æ•°")

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½®
            from app.services.simple_analysis_service import create_analysis_config

            config = create_analysis_config(
                research_depth=task.parameters.research_depth,
                selected_analysts=task.parameters.selected_analysts
                or ["market", "fundamentals"],
                quick_model=quick_model,
                deep_model=deep_model,
                llm_provider=llm_provider,
                market_type=getattr(task.parameters, "market_type", "Aè‚¡"),
                quick_model_config=quick_model_config,  # ä¼ é€’æ¨¡å‹é…ç½®
                deep_model_config=deep_model_config,  # ä¼ é€’æ¨¡å‹é…ç½®
            )

            # å¯åŠ¨å¼•æ“
            progress_tracker.update_progress("ğŸš€ åˆå§‹åŒ–AIåˆ†æå¼•æ“")

            # è·å–TradingAgentså®ä¾‹
            trading_graph = self._get_trading_graph(config)

            # æ‰§è¡Œåˆ†æ
            from datetime import timezone

            start_time = datetime.now(timezone.utc)
            analysis_date = task.parameters.analysis_date or datetime.now().strftime(
                "%Y-%m-%d"
            )

            # åˆ›å»ºè¿›åº¦å›è°ƒå‡½æ•°
            def progress_callback(message: str):
                progress_tracker.update_progress(message)

            # è°ƒç”¨ç°æœ‰çš„åˆ†ææ–¹æ³•ï¼ˆåŒæ­¥è°ƒç”¨ï¼Œä¼ é€’è¿›åº¦å›è°ƒï¼‰
            _, decision = trading_graph.propagate(
                task.symbol, analysis_date, progress_callback
            )

            execution_time = (datetime.now(timezone.utc) - start_time).total_seconds()

            # ç”ŸæˆæŠ¥å‘Š
            progress_tracker.update_progress("ğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š")

            # ä»å†³ç­–ä¸­æå–æ¨¡å‹ä¿¡æ¯
            model_info = (
                decision.get("model_info", "Unknown")
                if isinstance(decision, dict)
                else "Unknown"
            )

            # æ„å»ºç»“æœ
            result = AnalysisResult(
                analysis_id=str(uuid.uuid4()),
                summary=decision.get("summary", ""),
                recommendation=decision.get("recommendation", ""),
                confidence_score=decision.get("confidence_score", 0.0),
                risk_level=decision.get("risk_level", "ä¸­ç­‰"),
                key_points=decision.get("key_points", []),
                detailed_analysis=decision,
                execution_time=execution_time,
                tokens_used=decision.get("tokens_used", 0),
                model_info=model_info,  # ğŸ”¥ æ·»åŠ æ¨¡å‹ä¿¡æ¯å­—æ®µ
            )

            logger.info(
                f"âœ… [çº¿ç¨‹æ± ] åˆ†æä»»åŠ¡å®Œæˆ: {task.task_id} - è€—æ—¶{execution_time:.2f}ç§’"
            )
            return result

        except Exception as e:
            logger.error(f"âŒ [çº¿ç¨‹æ± ] æ‰§è¡Œåˆ†æä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")
            raise

    def _execute_analysis_sync(self, task: AnalysisTask) -> AnalysisResult:
        """åŒæ­¥æ‰§è¡Œåˆ†æä»»åŠ¡ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
        try:
            logger.info(f"ğŸ”„ [çº¿ç¨‹æ± ] å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.symbol}")

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½®
            # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨
            config_mgr = get_config_manager()
            quick_model = (
                getattr(task.parameters, "quick_analysis_model", None)
                or config_mgr.get_quick_analysis_model()
            )
            deep_model = (
                getattr(task.parameters, "deep_analysis_model", None)
                or config_mgr.get_deep_analysis_model()
            )

            # ğŸ”§ ä» MongoDB æ•°æ®åº“è¯»å–æ¨¡å‹çš„å®Œæ•´é…ç½®å‚æ•°ï¼ˆè€Œä¸æ˜¯ä» JSON æ–‡ä»¶ï¼‰
            quick_model_config = None
            deep_model_config = None

            try:
                from pymongo import MongoClient
                from app.core.config import settings

                # ä½¿ç”¨åŒæ­¥ MongoDB å®¢æˆ·ç«¯
                client = MongoClient(settings.MONGO_URI)
                db = client[settings.MONGO_DB]
                collection = db.system_configs

                # æŸ¥è¯¢æœ€æ–°çš„æ´»è·ƒé…ç½®
                doc = collection.find_one({"is_active": True}, sort=[("version", -1)])

                if doc and "llm_configs" in doc:
                    llm_configs = doc["llm_configs"]
                    logger.info(f"âœ… ä» MongoDB è¯»å–åˆ° {len(llm_configs)} ä¸ªæ¨¡å‹é…ç½®")

                    for llm_config in llm_configs:
                        if llm_config.get("model_name") == quick_model:
                            quick_model_config = {
                                "max_tokens": llm_config.get("max_tokens", 4000),
                                "temperature": llm_config.get("temperature", 0.7),
                                "timeout": llm_config.get("timeout", 180),
                                "retry_times": llm_config.get("retry_times", 3),
                                "api_base": llm_config.get("api_base"),
                            }
                            logger.info(f"âœ… è¯»å–å¿«é€Ÿæ¨¡å‹é…ç½®: {quick_model}")
                            logger.info(
                                f"   max_tokens={quick_model_config['max_tokens']}, temperature={quick_model_config['temperature']}"
                            )
                            logger.info(
                                f"   timeout={quick_model_config['timeout']}, retry_times={quick_model_config['retry_times']}"
                            )
                            logger.info(f"   api_base={quick_model_config['api_base']}")

                        if llm_config.get("model_name") == deep_model:
                            deep_model_config = {
                                "max_tokens": llm_config.get("max_tokens", 4000),
                                "temperature": llm_config.get("temperature", 0.7),
                                "timeout": llm_config.get("timeout", 180),
                                "retry_times": llm_config.get("retry_times", 3),
                                "api_base": llm_config.get("api_base"),
                            }
                            logger.info(
                                f"âœ… è¯»å–æ·±åº¦æ¨¡å‹é…ç½®: {deep_model} - {deep_model_config}"
                            )
                else:
                    logger.warning("âš ï¸ MongoDB ä¸­æ²¡æœ‰æ‰¾åˆ°ç³»ç»Ÿé…ç½®ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°")
            except Exception as e:
                logger.warning(f"âš ï¸ ä» MongoDB è¯»å–æ¨¡å‹é…ç½®å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°")

            # æ ¹æ®æ¨¡å‹åç§°åŠ¨æ€æŸ¥æ‰¾ä¾›åº”å•†ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
            llm_provider = "dashscope"  # é»˜è®¤ä½¿ç”¨dashscope

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½®
            from app.services.simple_analysis_service import create_analysis_config

            config = create_analysis_config(
                research_depth=task.parameters.research_depth,
                selected_analysts=task.parameters.selected_analysts
                or ["market", "fundamentals"],
                quick_model=quick_model,
                deep_model=deep_model,
                llm_provider=llm_provider,
                market_type=getattr(task.parameters, "market_type", "Aè‚¡"),
                quick_model_config=quick_model_config,  # ä¼ é€’æ¨¡å‹é…ç½®
                deep_model_config=deep_model_config,  # ä¼ é€’æ¨¡å‹é…ç½®
            )

            # è·å–TradingAgentså®ä¾‹
            trading_graph = self._get_trading_graph(config)

            # æ‰§è¡Œåˆ†æ
            from datetime import timezone

            start_time = datetime.now(timezone.utc)
            analysis_date = task.parameters.analysis_date or datetime.now().strftime(
                "%Y-%m-%d"
            )

            # è°ƒç”¨ç°æœ‰çš„åˆ†ææ–¹æ³•ï¼ˆåŒæ­¥è°ƒç”¨ï¼‰
            _, decision = trading_graph.propagate(task.symbol, analysis_date)

            execution_time = (datetime.now(timezone.utc) - start_time).total_seconds()

            # ä»å†³ç­–ä¸­æå–æ¨¡å‹ä¿¡æ¯
            model_info = (
                decision.get("model_info", "Unknown")
                if isinstance(decision, dict)
                else "Unknown"
            )

            # æ„å»ºç»“æœ
            result = AnalysisResult(
                analysis_id=str(uuid.uuid4()),
                summary=decision.get("summary", ""),
                recommendation=decision.get("recommendation", ""),
                confidence_score=decision.get("confidence_score", 0.0),
                risk_level=decision.get("risk_level", "ä¸­ç­‰"),
                key_points=decision.get("key_points", []),
                detailed_analysis=decision,
                execution_time=execution_time,
                tokens_used=decision.get("tokens_used", 0),
                model_info=model_info,  # ğŸ”¥ æ·»åŠ æ¨¡å‹ä¿¡æ¯å­—æ®µ
            )

            logger.info(
                f"âœ… [çº¿ç¨‹æ± ] åˆ†æä»»åŠ¡å®Œæˆ: {task.task_id} - è€—æ—¶{execution_time:.2f}ç§’"
            )
            return result

        except Exception as e:
            logger.error(f"âŒ [çº¿ç¨‹æ± ] æ‰§è¡Œåˆ†æä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")
            raise

    async def _execute_single_analysis_async(self, task: AnalysisTask):
        """å¼‚æ­¥æ‰§è¡Œå•è‚¡åˆ†æä»»åŠ¡ï¼ˆåœ¨åå°è¿è¡Œï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹ï¼‰"""
        progress_tracker = None
        try:
            logger.info(f"ğŸ”„ å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.symbol}")

            # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
            progress_tracker = RedisProgressTracker(
                task_id=task.task_id,
                analysts=task.parameters.selected_analysts
                or ["market", "fundamentals"],
                research_depth=task.parameters.research_depth or "æ ‡å‡†",
                llm_provider="dashscope",
            )

            # ç¼“å­˜è¿›åº¦è·Ÿè¸ªå™¨
            self._progress_trackers[task.task_id] = progress_tracker

            # åˆå§‹åŒ–è¿›åº¦
            progress_tracker.update_progress("ğŸš€ å¼€å§‹è‚¡ç¥¨åˆ†æ")
            await self._update_task_status_with_tracker(
                task.task_id, AnalysisStatus.PROCESSING, progress_tracker
            )

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œåˆ†æï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            import asyncio
            import concurrent.futures

            loop = asyncio.get_event_loop()

            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨è¿è¡ŒåŒæ­¥çš„åˆ†æä»£ç 
            with concurrent.futures.ThreadPoolExecutor() as executor:
                result = await loop.run_in_executor(
                    executor,
                    self._execute_analysis_sync_with_progress,
                    task,
                    progress_tracker,
                )

            # æ ‡è®°å®Œæˆ
            progress_tracker.mark_completed("âœ… åˆ†æå®Œæˆ")
            await self._update_task_status_with_tracker(
                task.task_id, AnalysisStatus.COMPLETED, progress_tracker, result
            )

            # è®°å½• token ä½¿ç”¨
            try:
                # è·å–ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
                quick_model = getattr(task.parameters, "quick_analysis_model", None)
                deep_model = getattr(task.parameters, "deep_analysis_model", None)

                # ä¼˜å…ˆä½¿ç”¨æ·±åº¦åˆ†ææ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å¿«é€Ÿåˆ†ææ¨¡å‹
                model_name = deep_model or quick_model or "qwen-plus"

                # æ ¹æ®æ¨¡å‹åç§°ç¡®å®šä¾›åº”å•†
                from app.services.simple_analysis_service import (
                    get_provider_by_model_name,
                )

                provider = get_provider_by_model_name(model_name)

                # è®°å½•ä½¿ç”¨æƒ…å†µ
                await self._record_token_usage(task, result, provider, model_name)
            except Exception as e:
                logger.error(f"âš ï¸  è®°å½• token ä½¿ç”¨å¤±è´¥: {e}")

            logger.info(f"âœ… åˆ†æä»»åŠ¡å®Œæˆ: {task.task_id}")

        except Exception as e:
            logger.error(f"âŒ åˆ†æä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")

            # æ ‡è®°å¤±è´¥
            if progress_tracker:
                progress_tracker.mark_failed(str(e))
                await self._update_task_status_with_tracker(
                    task.task_id, AnalysisStatus.FAILED, progress_tracker
                )
            else:
                await self._update_task_status(
                    task.task_id, AnalysisStatus.FAILED, 0, str(e)
                )
        finally:
            # æ¸…ç†è¿›åº¦è·Ÿè¸ªå™¨ç¼“å­˜
            if task.task_id in self._progress_trackers:
                del self._progress_trackers[task.task_id]

    async def submit_single_analysis(
        self, user_id: str, request: SingleAnalysisRequest
    ) -> Dict[str, Any]:
        """æäº¤å•è‚¡åˆ†æä»»åŠ¡"""
        try:
            logger.info(f"ğŸ“ å¼€å§‹æäº¤å•è‚¡åˆ†æä»»åŠ¡")
            logger.info(f"ğŸ‘¤ ç”¨æˆ·ID: {user_id} (ç±»å‹: {type(user_id)})")

            # è·å–è‚¡ç¥¨ä»£ç  (å…¼å®¹æ—§å­—æ®µ)
            stock_symbol = request.get_symbol()
            logger.info(f"ğŸ“Š è‚¡ç¥¨ä»£ç : {stock_symbol}")
            logger.info(f"âš™ï¸ åˆ†æå‚æ•°: {request.parameters}")

            # ç”Ÿæˆä»»åŠ¡ID
            task_id = str(uuid.uuid4())
            logger.info(f"ğŸ†” ç”Ÿæˆä»»åŠ¡ID: {task_id}")

            # è½¬æ¢ç”¨æˆ·ID
            converted_user_id = self._convert_user_id(user_id)
            logger.info(
                f"ğŸ”„ è½¬æ¢åçš„ç”¨æˆ·ID: {converted_user_id} (ç±»å‹: {type(converted_user_id)})"
            )

            # åˆ›å»ºåˆ†æä»»åŠ¡
            logger.info(f"ğŸ—ï¸ å¼€å§‹åˆ›å»ºAnalysisTaskå¯¹è±¡...")

            # è¯»å–åˆå¹¶åçš„ç³»ç»Ÿè®¾ç½®ï¼ˆENV ä¼˜å…ˆ â†’ DBï¼‰ï¼Œç”¨äºå¡«å……æ¨¡å‹ä¸å¹¶å‘/è¶…æ—¶é…ç½®
            try:
                effective_settings = (
                    await config_provider.get_effective_system_settings()
                )
            except Exception:
                effective_settings = {}

            # å¡«å……åˆ†æå‚æ•°ä¸­çš„æ¨¡å‹ï¼ˆè‹¥è¯·æ±‚æœªæ˜¾å¼æä¾›ï¼‰
            params = request.parameters or AnalysisParameters()
            if not getattr(params, "quick_analysis_model", None):
                params.quick_analysis_model = effective_settings.get(
                    "quick_analysis_model", "qwen-turbo"
                )
            if not getattr(params, "deep_analysis_model", None):
                params.deep_analysis_model = effective_settings.get(
                    "deep_analysis_model", "qwen-max"
                )

            # åº”ç”¨ç³»ç»Ÿçº§å¹¶å‘ä¸å¯è§æ€§è¶…æ—¶ï¼ˆè‹¥æä¾›ï¼‰
            try:
                self.queue_service.user_concurrent_limit = int(
                    effective_settings.get(
                        "max_concurrent_tasks", DEFAULT_USER_CONCURRENT_LIMIT
                    )
                )
                self.queue_service.global_concurrent_limit = int(
                    effective_settings.get(
                        "max_concurrent_tasks", GLOBAL_CONCURRENT_LIMIT
                    )
                )
                self.queue_service.visibility_timeout = int(
                    effective_settings.get(
                        "default_analysis_timeout", VISIBILITY_TIMEOUT_SECONDS
                    )
                )
            except Exception:
                # ä½¿ç”¨é»˜è®¤å€¼å³å¯
                pass

            task = AnalysisTask(
                task_id=task_id,
                user_id=converted_user_id,
                symbol=stock_symbol,
                stock_code=stock_symbol,  # å…¼å®¹å­—æ®µ
                parameters=params,
                status=AnalysisStatus.PENDING,
            )
            logger.info(f"âœ… AnalysisTaskå¯¹è±¡åˆ›å»ºæˆåŠŸ")

            # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“
            logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“...")
            db = get_mongo_db()
            task_dict = task.model_dump(by_alias=True)
            logger.info(f"ğŸ“„ ä»»åŠ¡å­—å…¸: {task_dict}")
            await db.analysis_tasks.insert_one(task_dict)
            logger.info(f"âœ… ä»»åŠ¡å·²ä¿å­˜åˆ°æ•°æ®åº“")

            # å•è‚¡åˆ†æï¼šç›´æ¥åœ¨åå°æ‰§è¡Œï¼ˆä¸é˜»å¡APIå“åº”ï¼‰
            logger.info(f"ğŸš€ å¼€å§‹åœ¨åå°æ‰§è¡Œåˆ†æä»»åŠ¡...")

            # åˆ›å»ºåå°ä»»åŠ¡ï¼Œä¸ç­‰å¾…å®Œæˆ
            import asyncio

            background_task = asyncio.create_task(
                self._execute_single_analysis_async(task)
            )

            # ä¸ç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œè®©å®ƒåœ¨åå°è¿è¡Œ
            logger.info(f"âœ… åå°ä»»åŠ¡å·²å¯åŠ¨ï¼Œä»»åŠ¡ID: {task_id}")

            logger.info(f"ğŸ‰ å•è‚¡åˆ†æä»»åŠ¡æäº¤å®Œæˆ: {task_id} - {stock_symbol}")

            return {
                "task_id": task_id,
                "symbol": stock_symbol,
                "stock_code": stock_symbol,  # å…¼å®¹å­—æ®µ
                "status": AnalysisStatus.PENDING,
                "message": "ä»»åŠ¡å·²åœ¨åå°å¯åŠ¨",
            }

        except Exception as e:
            logger.error(f"æäº¤å•è‚¡åˆ†æä»»åŠ¡å¤±è´¥: {e}")
            raise

    async def submit_batch_analysis(
        self, user_id: str, request: BatchAnalysisRequest
    ) -> Dict[str, Any]:
        """æäº¤æ‰¹é‡åˆ†æä»»åŠ¡"""
        try:
            # ç”Ÿæˆæ‰¹æ¬¡ID
            batch_id = str(uuid.uuid4())

            # è½¬æ¢ç”¨æˆ·ID
            converted_user_id = self._convert_user_id(user_id)

            # è¯»å–ç³»ç»Ÿè®¾ç½®ï¼Œå¡«å……æ¨¡å‹å‚æ•°å¹¶åº”ç”¨å¹¶å‘/è¶…æ—¶é…ç½®
            try:
                effective_settings = (
                    await config_provider.get_effective_system_settings()
                )
            except Exception:
                effective_settings = {}

            params = request.parameters or AnalysisParameters()
            if not getattr(params, "quick_analysis_model", None):
                params.quick_analysis_model = effective_settings.get(
                    "quick_analysis_model", "qwen-turbo"
                )
            if not getattr(params, "deep_analysis_model", None):
                params.deep_analysis_model = effective_settings.get(
                    "deep_analysis_model", "qwen-max"
                )

            try:
                self.queue_service.user_concurrent_limit = int(
                    effective_settings.get(
                        "max_concurrent_tasks", DEFAULT_USER_CONCURRENT_LIMIT
                    )
                )
                self.queue_service.global_concurrent_limit = int(
                    effective_settings.get(
                        "max_concurrent_tasks", GLOBAL_CONCURRENT_LIMIT
                    )
                )
                self.queue_service.visibility_timeout = int(
                    effective_settings.get(
                        "default_analysis_timeout", VISIBILITY_TIMEOUT_SECONDS
                    )
                )
            except Exception:
                pass

            # åˆ›å»ºæ‰¹æ¬¡è®°å½•
            # è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨ (å…¼å®¹æ—§å­—æ®µ)
            stock_symbols = request.get_symbols()

            batch = AnalysisBatch(
                batch_id=batch_id,
                user_id=converted_user_id,
                title=request.title,
                description=request.description,
                total_tasks=len(stock_symbols),
                parameters=params,
                status=BatchStatus.PENDING,
            )

            # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
            tasks = []
            for symbol in stock_symbols:
                task_id = str(uuid.uuid4())
                task = AnalysisTask(
                    task_id=task_id,
                    batch_id=batch_id,
                    user_id=converted_user_id,
                    symbol=symbol,
                    stock_code=symbol,  # å…¼å®¹å­—æ®µ
                    parameters=batch.parameters,
                    status=AnalysisStatus.PENDING,
                )
                tasks.append(task)

            # ä¿å­˜åˆ°æ•°æ®åº“
            db = get_mongo_db()
            await db.analysis_batches.insert_one(batch.dict(by_alias=True))
            await db.analysis_tasks.insert_many(
                [task.dict(by_alias=True) for task in tasks]
            )

            # æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—
            for task in tasks:
                # å‡†å¤‡é˜Ÿåˆ—å‚æ•°ï¼ˆç›´æ¥ä¼ é€’åˆ†æå‚æ•°ï¼Œä¸åµŒå¥—ï¼‰
                queue_params = task.parameters.dict() if task.parameters else {}

                # æ·»åŠ ä»»åŠ¡å…ƒæ•°æ®
                queue_params.update(
                    {
                        "task_id": task.task_id,
                        "symbol": task.symbol,
                        "stock_code": task.symbol,  # å…¼å®¹å­—æ®µ
                        "user_id": str(task.user_id),
                        "batch_id": task.batch_id,
                        "created_at": task.created_at.isoformat()
                        if task.created_at
                        else None,
                    }
                )

                # è°ƒç”¨é˜Ÿåˆ—æœåŠ¡
                await self.queue_service.enqueue_task(
                    user_id=str(converted_user_id),
                    symbol=task.symbol,
                    params=queue_params,
                    batch_id=task.batch_id,
                )

            logger.info(f"æ‰¹é‡åˆ†æä»»åŠ¡å·²æäº¤: {batch_id} - {len(tasks)}ä¸ªè‚¡ç¥¨")

            return {
                "batch_id": batch_id,
                "total_tasks": len(tasks),
                "status": BatchStatus.PENDING,
                "message": f"å·²æäº¤{len(tasks)}ä¸ªåˆ†æä»»åŠ¡åˆ°é˜Ÿåˆ—",
            }

        except Exception as e:
            logger.error(f"æäº¤æ‰¹é‡åˆ†æä»»åŠ¡å¤±è´¥: {e}")
            raise

    async def execute_analysis_task(
        self,
        task: AnalysisTask,
        progress_callback: Optional[Callable[[int, str], None]] = None,
    ) -> AnalysisResult:
        """æ‰§è¡Œå•ä¸ªåˆ†æä»»åŠ¡"""
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œåˆ†æä»»åŠ¡: {task.task_id} - {task.symbol}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            await self._update_task_status(task.task_id, AnalysisStatus.PROCESSING, 0)

            if progress_callback:
                progress_callback(10, "åˆå§‹åŒ–åˆ†æå¼•æ“...")

            # ä½¿ç”¨æ ‡å‡†é…ç½®å‡½æ•°åˆ›å»ºå®Œæ•´é…ç½® - ä¸å•è‚¡åˆ†æä¿æŒä¸€è‡´
            # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨
            config_mgr = get_config_manager()
            quick_model = (
                getattr(task.parameters, "quick_analysis_model", None)
                or config_mgr.get_quick_analysis_model()
            )
            deep_model = (
                getattr(task.parameters, "deep_analysis_model", None)
                or config_mgr.get_deep_analysis_model()
            )

            # ğŸ”§ ä»æ•°æ®åº“è¯»å–æ¨¡å‹çš„å®Œæ•´é…ç½®å‚æ•°
            quick_model_config = None
            deep_model_config = None
            # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨è·å–æ¨¡å‹é…ç½®
            config_mgr = get_config_manager()
            model_config = config_mgr.get_model_config(quick_model)
            quick_model_config = {
                "max_tokens": model_config.get("max_tokens"),
                "temperature": model_config.get("temperature"),
                "timeout": model_config.get("timeout"),
            }
            model_config = config_mgr.get_model_config(deep_model)
            deep_model_config = {
                "max_tokens": model_config.get("max_tokens"),
                "temperature": model_config.get("temperature"),
                "timeout": model_config.get("timeout"),
            }

            # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨è®¡ç®—æˆæœ¬
            config_mgr = get_config_manager()
            cost = 0.0
            currency = "CNY"  # é»˜è®¤è´§å¸å•ä½
                input_price = llm_config.input_price_per_1k or 0.0
                output_price = llm_config.output_price_per_1k or 0.0
                cost = (input_tokens / 1000 * input_price) + (
                    output_tokens / 1000 * output_price
                )
                currency = llm_config.currency or "CNY"

            # åˆ›å»ºä½¿ç”¨è®°å½•
            usage_record = UsageRecord(
                timestamp=datetime.now().isoformat(),
                provider=provider,
                model_name=model_name,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                cost=cost,
                currency=currency,
                session_id=task.task_id,
                analysis_type="stock_analysis",
                stock_code=task.symbol,
            )

            # ä¿å­˜åˆ°æ•°æ®åº“
            success = await self.usage_service.add_usage_record(usage_record)

            if success:
                logger.info(f"ğŸ’° è®°å½•ä½¿ç”¨æˆæœ¬: {provider}/{model_name} - Â¥{cost:.4f}")
            else:
                logger.warning(f"âš ï¸  è®°å½•ä½¿ç”¨æˆæœ¬å¤±è´¥")

        except Exception as e:
            logger.error(f"âŒ è®°å½• token ä½¿ç”¨å¤±è´¥: {e}")


# å…¨å±€åˆ†ææœåŠ¡å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
analysis_service: Optional[AnalysisService] = None


def get_analysis_service() -> AnalysisService:
    """è·å–åˆ†ææœåŠ¡å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global analysis_service
    if analysis_service is None:
        analysis_service = AnalysisService()
    return analysis_service
