# -*- coding: utf-8 -*-
"""
æ¨¡å‹èƒ½åŠ›ç®¡ç†æœåŠ¡

æä¾›æ¨¡å‹èƒ½åŠ›è¯„ä¼°ã€éªŒè¯å’Œæ¨èåŠŸèƒ½ã€‚
"""

from typing import Tuple, Dict, Optional, List, Any
from app.constants.model_capabilities import (
    ANALYSIS_DEPTH_REQUIREMENTS,
    DEFAULT_MODEL_CAPABILITIES,
    CAPABILITY_DESCRIPTIONS,
    ModelRole,
    ModelFeature,
)
from app.core.unified_config_service import get_config_manager
import logging
import re

logger = logging.getLogger(__name__)


class ModelCapabilityService:
    """æ¨¡å‹èƒ½åŠ›ç®¡ç†æœåŠ¡"""

    def _parse_aggregator_model_name(
        self, model_name: str
    ) -> Tuple[Optional[str], str]:
        """
        è§£æèšåˆæ¸ é“çš„æ¨¡å‹åç§°

        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¯èƒ½åŒ…å«å‰ç¼€ï¼ˆå¦‚ openai/gpt-4, anthropic/claude-3-sonnetï¼‰

        Returns:
            (åŸå‚å•†, åŸæ¨¡å‹å) å…ƒç»„
        """
        # å¸¸è§çš„èšåˆæ¸ é“æ¨¡å‹åç§°æ ¼å¼ï¼š
        # - openai/gpt-4
        # - anthropic/claude-3-sonnet
        # - google/gemini-pro

        if "/" in model_name:
            parts = model_name.split("/", 1)
            if len(parts) == 2:
                provider_hint = parts[0].lower()
                original_model = parts[1]

                # æ˜ å°„æä¾›å•†æç¤ºåˆ°æ ‡å‡†åç§°
                provider_map = {
                    "openai": "openai",
                    "anthropic": "anthropic",
                    "google": "google",
                    "deepseek": "deepseek",
                    "alibaba": "qwen",
                    "qwen": "qwen",
                    "zhipu": "zhipu",
                    "baidu": "baidu",
                    "moonshot": "moonshot",
                }

                provider = provider_map.get(provider_hint)
                return provider, original_model

        return None, model_name

    def _get_model_capability_with_mapping(
        self, model_name: str
    ) -> Tuple[int, Optional[str]]:
        """
        è·å–æ¨¡å‹èƒ½åŠ›ç­‰çº§ï¼ˆæ”¯æŒèšåˆæ¸ é“æ˜ å°„ï¼‰

        Returns:
            (èƒ½åŠ›ç­‰çº§, æ˜ å°„çš„åŸæ¨¡å‹å) å…ƒç»„
        """
        # ä»é»˜è®¤æ˜ å°„è¡¨è¯»å–ï¼ˆç›´æ¥åŒ¹é…å­—å…¸ä¸­çš„é…ç½®ï¼‰
        if model_name in DEFAULT_MODEL_CAPABILITIES:
            logger.info(f"âœ… ä»é»˜è®¤æ˜ å°„æ‰¾åˆ°æ¨¡å‹ {model_name} çš„é…ç½®")
            default_config = DEFAULT_MODEL_CAPABILITIES[model_name]
            return default_config["capability_level"], None

        # å°è¯•è§£æèšåˆæ¸ é“æ¨¡å‹å
        provider, original_model = self._parse_aggregator_model_name(model_name)
        if original_model and original_model != model_name:
            # å°è¯•ç”¨åŸæ¨¡å‹åæŸ¥æ‰¾
            if original_model in DEFAULT_MODEL_CAPABILITIES:
                logger.info(f"ğŸ”„ èšåˆæ¸ é“æ¨¡å‹æ˜ å°„: {model_name} -> {original_model}")
                return DEFAULT_MODEL_CAPABILITIES[original_model][
                    "capability_level"
                ], original_model

        # è¿”å›é»˜è®¤å€¼
        return 2, None

    def get_model_capability(self, model_name: str) -> int:
        """
        è·å–æ¨¡å‹çš„èƒ½åŠ›ç­‰çº§ï¼ˆæ”¯æŒèšåˆæ¸ é“æ¨¡å‹æ˜ å°„ï¼‰

        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆå¯èƒ½åŒ…å«èšåˆæ¸ é“å‰ç¼€ï¼Œå¦‚ openai/gpt-4ï¼‰

        Returns:
            èƒ½åŠ›ç­‰çº§ (1-5)
        """
        # 1. ä¼˜å…ˆä»æ•°æ®åº“é…ç½®è¯»å–
        try:
            llm_configs = get_config_manager().get_llm_configs()
            for config in llm_configs:
                if config.model_name == model_name:
                    return getattr(config, "capability_level", 2)
        except Exception as e:
            logger.warning(f"ä»é…ç½®è¯»å–æ¨¡å‹èƒ½åŠ›å¤±è´¥: {e}")

        # 2. ä»é»˜è®¤æ˜ å°„è¡¨è¯»å–ï¼ˆæ”¯æŒèšåˆæ¸ é“æ˜ å°„ï¼‰
        capability, mapped_model = self._get_model_capability_with_mapping(model_name)
        if mapped_model:
            logger.info(f"âœ… ä½¿ç”¨æ˜ å°„æ¨¡å‹ {mapped_model} çš„èƒ½åŠ›ç­‰çº§: {capability}")

        return capability

    def get_model_config(self, model_name: str) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹çš„å®Œæ•´é…ç½®ä¿¡æ¯ï¼ˆæ”¯æŒèšåˆæ¸ é“æ¨¡å‹æ˜ å°„ï¼‰

        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆå¯èƒ½åŒ…å«èšåˆæ¸ é“å‰ç¼€ï¼Œå¦‚ openai/gpt-4ï¼‰

        Returns:
            æ¨¡å‹é…ç½®å­—å…¸
        """
        # 1. ä¼˜å…ˆä»ç»Ÿä¸€é…ç½®ç®¡ç†å™¨è¯»å–
        try:
            config_manager = get_config_manager()
            db_config = config_manager._get_mongodb_config()

            if db_config and "llm_configs" in db_config:
                llm_configs = db_config["llm_configs"]
                logger.info(f"ğŸ” [MongoDB] llm_configs æ•°é‡: {len(llm_configs)}")

                for config_dict in llm_configs:
                    if config_dict.get("model_name") == model_name:
                        logger.info(f"ğŸ” [MongoDB] æ‰¾åˆ°æ¨¡å‹é…ç½®: {model_name}")
                        # ğŸ”§ å°†å­—ç¬¦ä¸²åˆ—è¡¨è½¬æ¢ä¸ºæšä¸¾åˆ—è¡¨
                        features_str = config_dict.get("features", [])
                        features_enum = []
                        for feature_str in features_str:
                            try:
                                # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º ModelFeature æšä¸¾
                                features_enum.append(ModelFeature(feature_str))
                            except ValueError:
                                logger.warning(f"âš ï¸ æœªçŸ¥çš„ç‰¹æ€§å€¼: {feature_str}")

                        # ğŸ”§ å°†å­—ç¬¦ä¸²åˆ—è¡¨è½¬æ¢ä¸ºæšä¸¾åˆ—è¡¨
                        roles_str = config_dict.get("suitable_roles", ["both"])
                        roles_enum = []
                        for role_str in roles_str:
                            try:
                                # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º ModelRole æšä¸¾
                                roles_enum.append(ModelRole(role_str))
                            except ValueError:
                                logger.warning(f"âš ï¸ æœªçŸ¥çš„è§’è‰²å€¼: {role_str}")

                        # å¦‚æœæ²¡æœ‰è§’è‰²ï¼Œé»˜è®¤ä¸º both
                        if not roles_enum:
                            roles_enum = [ModelRole.BOTH]

                        # ğŸ”§ å°† enabled å±æ€§è½¬æ¢ä¸ºå¸ƒå°”å€¼ï¼ˆå…¼å®¹å­—å…¸æ ¼å¼ï¼‰
                        is_enabled = config_dict.get("enabled", True)
                        capability_level = config_dict.get("capability_level", 2)

                        logger.info(
                            f"ğŸ“Š [MongoDBé…ç½®] {model_name}: features={features_enum}, roles={roles_enum}, enabled={is_enabled}"
                        )

                        return {
                            "model_name": config_dict.get("model_name"),
                            "capability_level": config_dict.get("capability_level", 2),
                            "suitable_roles": [str(r) for r in roles_enum],
                            "features": [
                                f.value for f in features_enum
                            ],  # ä½¿ç”¨ .value è·å–æšä¸¾å€¼
                            "recommended_depths": config_dict.get(
                                "recommended_depths", ["å¿«é€Ÿ", "åŸºç¡€", "æ ‡å‡†"]
                            ),
                            "performance_metrics": config_dict.get(
                                "performance_metrics", None
                            ),
                            "enabled": is_enabled,  # æ·»åŠ  enabled å±æ€§
                        }

            logger.warning(f"æœªä» MongoDB æ‰¾åˆ°æ¨¡å‹ {model_name} çš„é…ç½®ï¼Œå°è¯•å…¶ä»–æ–¹æ³•")
        except Exception as e:
            logger.warning(f"ä» MongoDB è¯»å–æ¨¡å‹é…ç½®å¤±è´¥: {e}", exc_info=True)

            if doc and "llm_configs" in doc:
                llm_configs = doc["llm_configs"]
                logger.info(f"ğŸ” [MongoDB] llm_configs æ•°é‡: {len(llm_configs)}")

                for config_dict in llm_configs:
                    if config_dict.get("model_name") == model_name:
                        logger.info(f"ğŸ” [MongoDB] æ‰¾åˆ°æ¨¡å‹é…ç½®: {model_name}")
                        # ğŸ”§ å°†å­—ç¬¦ä¸²åˆ—è¡¨è½¬æ¢ä¸ºæšä¸¾åˆ—è¡¨
                        features_str = config_dict.get("features", [])
                        features_enum = []
                        for feature_str in features_str:
                            try:
                                # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º ModelFeature æšä¸¾
                                features_enum.append(ModelFeature(feature_str))
                            except ValueError:
                                logger.warning(f"âš ï¸ æœªçŸ¥çš„ç‰¹æ€§å€¼: {feature_str}")

                        # ğŸ”§ å°†å­—ç¬¦ä¸²åˆ—è¡¨è½¬æ¢ä¸ºæšä¸¾åˆ—è¡¨
                        roles_str = config_dict.get("suitable_roles", ["both"])
                        roles_enum = []
                        for role_str in roles_str:
                            try:
                                # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º ModelRole æšä¸¾
                                roles_enum.append(ModelRole(role_str))
                            except ValueError:
                                logger.warning(f"âš ï¸ æœªçŸ¥çš„è§’è‰²å€¼: {role_str}")

                        # å¦‚æœæ²¡æœ‰è§’è‰²ï¼Œé»˜è®¤ä¸º both
                        if not roles_enum:
                            roles_enum = [ModelRole.BOTH]

                        logger.info(
                            f"ğŸ“Š [MongoDBé…ç½®] {model_name}: features={features_enum}, roles={roles_enum}"
                        )

                        # å…³é—­è¿æ¥
                        client.close()

                        return {
                            "model_name": config_dict.get("model_name"),
                            "capability_level": config_dict.get("capability_level", 2),
                            "suitable_roles": [
                                r.value for r in roles_enum
                            ],  # ä½¿ç”¨ .value è·å–å­—ç¬¦ä¸²å€¼
                            "features": [
                                f.value for f in features_enum
                            ],  # ä½¿ç”¨ .value è·å–å­—ç¬¦ä¸²å€¼
                            "recommended_depths": config_dict.get(
                                "recommended_depths", ["å¿«é€Ÿ", "åŸºç¡€", "æ ‡å‡†"]
                            ),
                            "performance_metrics": config_dict.get(
                                "performance_metrics", None
                            ),
                        }

            # å…³é—­è¿æ¥
            client.close()

        except Exception as e:
            logger.warning(f"ä» MongoDB è¯»å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)

        # 2. ä»é»˜è®¤æ˜ å°„è¡¨è¯»å–ï¼ˆç›´æ¥åŒ¹é…ï¼‰
        if model_name in DEFAULT_MODEL_CAPABILITIES:
            return DEFAULT_MODEL_CAPABILITIES[model_name]

        # 3. å°è¯•èšåˆæ¸ é“æ¨¡å‹æ˜ å°„
        provider, original_model = self._parse_aggregator_model_name(model_name)
        if original_model and original_model != model_name:
            if original_model in DEFAULT_MODEL_CAPABILITIES:
                logger.info(f"ğŸ”„ èšåˆæ¸ é“æ¨¡å‹æ˜ å°„: {model_name} -> {original_model}")
                config = DEFAULT_MODEL_CAPABILITIES[original_model].copy()
                config["model_name"] = model_name  # ä¿æŒåŸå§‹æ¨¡å‹å
                config["_mapped_from"] = original_model  # è®°å½•æ˜ å°„æ¥æº
                return config

        # 4. è¿”å›é»˜è®¤é…ç½®
        logger.warning(f"æœªæ‰¾åˆ°æ¨¡å‹ {model_name} çš„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return {
            "model_name": model_name,
            "capability_level": 2,
            "suitable_roles": [ModelRole.BOTH.value],  # ä½¿ç”¨ .value è·å–å­—ç¬¦ä¸²å€¼
            "features": [ModelFeature.TOOL_CALLING.value],  # ä½¿ç”¨ .value è·å–å­—ç¬¦ä¸²å€¼
            "recommended_depths": ["å¿«é€Ÿ", "åŸºç¡€", "æ ‡å‡†"],
            "performance_metrics": {"speed": 3, "cost": 3, "quality": 3},
        }

    def validate_model_pair(
        self, quick_model: str, deep_model: str, research_depth: str
    ) -> Dict[str, Any]:
        """
        éªŒè¯æ¨¡å‹å¯¹æ˜¯å¦é€‚åˆå½“å‰åˆ†ææ·±åº¦

        Args:
            quick_model: å¿«é€Ÿåˆ†ææ¨¡å‹åç§°
            deep_model: æ·±åº¦åˆ†ææ¨¡å‹åç§°
            research_depth: ç ”ç©¶æ·±åº¦ï¼ˆå¿«é€Ÿ/åŸºç¡€/æ ‡å‡†/æ·±åº¦/å…¨é¢ï¼‰

        Returns:
            éªŒè¯ç»“æœå­—å…¸ï¼ŒåŒ…å« valid, warnings, recommendations
        """
        logger.info(
            f"ğŸ” å¼€å§‹éªŒè¯æ¨¡å‹å¯¹: quick={quick_model}, deep={deep_model}, depth={research_depth}"
        )

        requirements = ANALYSIS_DEPTH_REQUIREMENTS.get(
            research_depth, ANALYSIS_DEPTH_REQUIREMENTS["æ ‡å‡†"]
        )
        logger.info(f"ğŸ” åˆ†ææ·±åº¦è¦æ±‚: {requirements}")

        quick_config = self.get_model_config(quick_model)
        deep_config = self.get_model_config(deep_model)

        logger.info(f"ğŸ” å¿«é€Ÿæ¨¡å‹é…ç½®: {quick_config}")
        logger.info(f"ğŸ” æ·±åº¦æ¨¡å‹é…ç½®: {deep_config}")

        result = {"valid": True, "warnings": [], "recommendations": []}

        # æ£€æŸ¥å¿«é€Ÿæ¨¡å‹
        quick_level = quick_config["capability_level"]
        logger.info(
            f"ğŸ” æ£€æŸ¥å¿«é€Ÿæ¨¡å‹èƒ½åŠ›ç­‰çº§: {quick_level} >= {requirements['quick_model_min']}?"
        )
        if quick_level < requirements["quick_model_min"]:
            warning = f"âš ï¸ å¿«é€Ÿæ¨¡å‹ {quick_model} (èƒ½åŠ›ç­‰çº§{quick_level}) ä½äº {research_depth} åˆ†æçš„å»ºè®®ç­‰çº§({requirements['quick_model_min']})"
            result["warnings"].append(warning)
            logger.warning(warning)

        # æ£€æŸ¥å¿«é€Ÿæ¨¡å‹è§’è‰²é€‚é…
        quick_roles = quick_config.get("suitable_roles", [])
        logger.info(f"ğŸ” æ£€æŸ¥å¿«é€Ÿæ¨¡å‹è§’è‰²: {quick_roles}")

        # å…¼å®¹å­—ç¬¦ä¸²å’Œæšä¸¾æ ¼å¼çš„è§’è‰²æ£€æŸ¥
        def has_role(roles, required_role):
            """æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€éœ€è§’è‰²ï¼ˆå…¼å®¹å­—ç¬¦ä¸²å’Œæšä¸¾ï¼‰"""
            for role in roles:
                if isinstance(role, str):
                    # å­—ç¬¦ä¸²æ ¼å¼æ¯”è¾ƒ
                    if role == required_role.value or role == str(required_role):
                        return True
                else:
                    # æšä¸¾æ ¼å¼æ¯”è¾ƒ
                    if role == required_role:
                        return True
            return False

        if not has_role(quick_roles, ModelRole.QUICK_ANALYSIS) and not has_role(
            quick_roles, ModelRole.BOTH
        ):
            warning = (
                f"ğŸ’¡ æ¨¡å‹ {quick_model} ä¸æ˜¯ä¸ºå¿«é€Ÿåˆ†æä¼˜åŒ–çš„ï¼Œå¯èƒ½å½±å“æ•°æ®æ”¶é›†æ•ˆç‡"
            )
            result["warnings"].append(warning)
            logger.warning(warning)

        # æ£€æŸ¥å¿«é€Ÿæ¨¡å‹æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
        quick_features = quick_config.get("features", [])
        logger.info(f"ğŸ” æ£€æŸ¥å¿«é€Ÿæ¨¡å‹ç‰¹æ€§: {quick_features}")

        # å…¼å®¹å­—ç¬¦ä¸²å’Œæšä¸¾æ ¼å¼çš„ç‰¹æ€§æ£€æŸ¥
        def has_feature(features, required_feature):
            """æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€éœ€ç‰¹æ€§ï¼ˆå…¼å®¹å­—ç¬¦ä¸²å’Œæšä¸¾ï¼‰"""
            required_value = required_feature.value
            required_str = str(required_feature)
            for feature in features:
                if isinstance(feature, str):
                    # å­—ç¬¦ä¸²æ ¼å¼æ¯”è¾ƒï¼š'TOOL_CALLING' æˆ– 'ModelFeature.TOOL_CALLING'
                    if feature == required_value or feature == required_str:
                        return True
                else:
                    # æšä¸¾æ ¼å¼æ¯”è¾ƒ
                    if feature == required_feature:
                        return True
            return False

        if not has_feature(quick_features, ModelFeature.TOOL_CALLING):
            result["valid"] = False
            warning = f"âŒ å¿«é€Ÿæ¨¡å‹ {quick_model} ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œæ— æ³•å®Œæˆæ•°æ®æ”¶é›†ä»»åŠ¡"
            result["warnings"].append(warning)
            logger.error(warning)

        # æ£€æŸ¥æ·±åº¦æ¨¡å‹
        deep_level = deep_config["capability_level"]
        logger.info(
            f"ğŸ” æ£€æŸ¥æ·±åº¦æ¨¡å‹èƒ½åŠ›ç­‰çº§: {deep_level} >= {requirements['deep_model_min']}?"
        )
        if deep_level < requirements["deep_model_min"]:
            result["valid"] = False
            warning = f"âŒ æ·±åº¦æ¨¡å‹ {deep_model} (èƒ½åŠ›ç­‰çº§{deep_level}) ä¸æ»¡è¶³ {research_depth} åˆ†æçš„æœ€ä½è¦æ±‚(ç­‰çº§{requirements['deep_model_min']})"
            result["warnings"].append(warning)
            logger.error(warning)
            result["recommendations"].append(
                self._recommend_model("deep", requirements["deep_model_min"])
            )

        # æ£€æŸ¥æ·±åº¦æ¨¡å‹è§’è‰²é€‚é…
        deep_roles = deep_config.get("suitable_roles", [])
        logger.info(f"ğŸ” æ£€æŸ¥æ·±åº¦æ¨¡å‹è§’è‰²: {deep_roles}")
        if not has_role(deep_roles, ModelRole.DEEP_ANALYSIS) and not has_role(
            deep_roles, ModelRole.BOTH
        ):
            warning = f"ğŸ’¡ æ¨¡å‹ {deep_model} ä¸æ˜¯ä¸ºæ·±åº¦æ¨ç†ä¼˜åŒ–çš„ï¼Œå¯èƒ½å½±å“åˆ†æè´¨é‡"
            result["warnings"].append(warning)
            logger.warning(warning)

        # æ£€æŸ¥å¿…éœ€ç‰¹æ€§
        logger.info(f"ğŸ” æ£€æŸ¥å¿…éœ€ç‰¹æ€§: {requirements['required_features']}")
        for feature in requirements["required_features"]:
            if feature == ModelFeature.REASONING:
                deep_features = deep_config.get("features", [])
                logger.info(f"ğŸ” æ£€æŸ¥æ·±åº¦æ¨¡å‹æ¨ç†èƒ½åŠ›: {deep_features}")
                if not has_feature(deep_features, feature):
                    warning = (
                        f"ğŸ’¡ {research_depth} åˆ†æå»ºè®®ä½¿ç”¨å…·æœ‰å¼ºæ¨ç†èƒ½åŠ›çš„æ·±åº¦æ¨¡å‹"
                    )
                    result["warnings"].append(warning)
                    logger.warning(warning)

        logger.info(
            f"ğŸ” éªŒè¯ç»“æœ: valid={result['valid']}, warnings={len(result['warnings'])}æ¡"
        )
        logger.info(f"ğŸ” è­¦å‘Šè¯¦æƒ…: {result['warnings']}")

        return result

    def recommend_models_for_depth(self, research_depth: str) -> Tuple[str, str]:
        """
        æ ¹æ®åˆ†ææ·±åº¦æ¨èåˆé€‚çš„æ¨¡å‹å¯¹

        Args:
            research_depth: ç ”ç©¶æ·±åº¦ï¼ˆå¿«é€Ÿ/åŸºç¡€/æ ‡å‡†/æ·±åº¦/å…¨é¢ï¼‰

        Returns:
            (quick_model, deep_model) å…ƒç»„
        """
        requirements = ANALYSIS_DEPTH_REQUIREMENTS.get(
            research_depth, ANALYSIS_DEPTH_REQUIREMENTS["æ ‡å‡†"]
        )

        # è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
        try:
            llm_configs = get_config_manager().get_llm_configs()
            # å…¼å®¹å­—å…¸æ ¼å¼å’Œå¯¹è±¡æ ¼å¼
            enabled_models = []
            for c in llm_configs:
                if isinstance(c, dict):
                    is_enabled = c.get("enabled", True)
                    model_name = c.get("model_name", "unknown")
                else:
                    is_enabled = getattr(c, "enabled", True)
                    model_name = getattr(c, "model_name", "unknown")

                if is_enabled:
                    enabled_models.append(c)
                    logger.debug(f"âœ… æ¨¡å‹å·²å¯ç”¨: {model_name}")
                else:
                    logger.debug(f"â¸ï¸ æ¨¡å‹å·²ç¦ç”¨: {model_name}")

        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹é…ç½®å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹
            return self._get_default_models()

        if not enabled_models:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_models()

        # ç­›é€‰é€‚åˆå¿«é€Ÿåˆ†æçš„æ¨¡å‹
        quick_candidates = []
        for m in enabled_models:
            # å…¼å®¹å­—å…¸æ ¼å¼å’Œå¯¹è±¡æ ¼å¼
            if isinstance(m, dict):
                roles = m.get("suitable_roles", ["both"])
                level = m.get("capability_level", 2)
                features = m.get("features", [])
            else:
                roles = getattr(m, "suitable_roles", ["both"])
                level = getattr(m, "capability_level", 2)
                features = getattr(m, "features", [])

            # å°†å­—ç¬¦ä¸²è§’è‰²è½¬æ¢ä¸ºæšä¸¾
            roles_enum = []
            for role_str in roles:
                try:
                    roles_enum.append(ModelRole(role_str))
                except ValueError:
                    roles_enum.append(ModelRole.BOTH)

            # å°†å­—ç¬¦ä¸²ç‰¹æ€§è½¬æ¢ä¸ºæšä¸¾
            features_enum = []
            for feature_str in features:
                try:
                    features_enum.append(ModelFeature(feature_str))
                except ValueError:
                    pass

            if (
                (ModelRole.QUICK_ANALYSIS in roles_enum or ModelRole.BOTH in roles_enum)
                and level >= requirements["quick_model_min"]
                and ModelFeature.TOOL_CALLING in features_enum
            ):
                quick_candidates.append(m)

        # ç­›é€‰é€‚åˆæ·±åº¦åˆ†æçš„æ¨¡å‹
        deep_candidates = []
        for m in enabled_models:
            # å…¼å®¹å­—å…¸æ ¼å¼å’Œå¯¹è±¡æ ¼å¼
            if isinstance(m, dict):
                roles = m.get("suitable_roles", ["both"])
                level = m.get("capability_level", 2)
            else:
                roles = getattr(m, "suitable_roles", ["both"])
                level = getattr(m, "capability_level", 2)

            # å°†å­—ç¬¦ä¸²è§’è‰²è½¬æ¢ä¸ºæšä¸¾
            roles_enum = []
            for role_str in roles:
                try:
                    roles_enum.append(ModelRole(role_str))
                except ValueError:
                    roles_enum.append(ModelRole.BOTH)

            if (
                ModelRole.DEEP_ANALYSIS in roles_enum or ModelRole.BOTH in roles_enum
            ) and level >= requirements["deep_model_min"]:
                deep_candidates.append(m)

        # æŒ‰æ€§ä»·æ¯”æ’åºï¼ˆèƒ½åŠ›ç­‰çº§ vs æˆæœ¬ï¼‰
        def get_sort_key(x):
            if isinstance(x, dict):
                level = x.get("capability_level", 2)
                perf = x.get("performance_metrics") or {}
                cost = perf.get("cost", 3)
                quality = perf.get("quality", 3)
            else:
                level = getattr(x, "capability_level", 2)
                perf = getattr(x, "performance_metrics") or {}
                cost = perf.get("cost", 3)
                quality = perf.get("quality", 3)
            return level, cost, quality

        quick_candidates.sort(key=get_sort_key, reverse=True)
        deep_candidates.sort(key=get_sort_key, reverse=True)

        # é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆå…¼å®¹å­—å…¸å’Œå¯¹è±¡æ ¼å¼ï¼‰
        if quick_candidates:
            if isinstance(quick_candidates[0], dict):
                quick_model = quick_candidates[0].get("model_name")
            else:
                quick_model = getattr(quick_candidates[0], "model_name", None)
        else:
            quick_model = None

        if deep_candidates:
            if isinstance(deep_candidates[0], dict):
                deep_model = deep_candidates[0].get("model_name")
            else:
                deep_model = getattr(deep_candidates[0], "model_name", None)
        else:
            deep_model = None

        # å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚çš„ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤
        if not quick_model or not deep_model:
            return self._get_default_models()

        logger.info(
            f"ğŸ¤– ä¸º {research_depth} åˆ†ææ¨èæ¨¡å‹: "
            f"quick={quick_model} (è§’è‰²:å¿«é€Ÿåˆ†æ), "
            f"deep={deep_model} (è§’è‰²:æ·±åº¦æ¨ç†)"
        )

        return quick_model, deep_model

    def _get_default_models(self) -> Tuple[str, str]:
        """è·å–é»˜è®¤æ¨¡å‹å¯¹"""
        try:
            quick_model = get_config_manager().get_quick_analysis_model()
            deep_model = get_config_manager().get_deep_analysis_model()

            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœè¿”å›çš„æ˜¯æ•´ä¸ªé…ç½®å¯¹è±¡ï¼Œå°è¯•æå– model_name
            if isinstance(quick_model, dict):
                logger.warning(
                    f"âš ï¸ quick_model æ˜¯é…ç½®å¯¹è±¡ï¼Œå°è¯•æå– model_name: {quick_model.get('model_name', 'qwen-turbo')}"
                )
                quick_model = quick_model.get("model_name", "qwen-turbo")

            if isinstance(deep_model, dict):
                logger.warning(
                    f"âš ï¸ deep_model æ˜¯é…ç½®å¯¹è±¡ï¼Œå°è¯•æå– model_name: {deep_model.get('model_name', 'qwen-plus')}"
                )
                deep_model = deep_model.get("model_name", "qwen-plus")

            # ç¡®ä¿è¿”å›çš„æ˜¯å­—ç¬¦ä¸²
            if not isinstance(quick_model, str):
                logger.warning(
                    f"âš ï¸ quick_model ç±»å‹é”™è¯¯: {type(quick_model)}ï¼Œä½¿ç”¨é»˜è®¤å€¼"
                )
                quick_model = "qwen-turbo"

            if not isinstance(deep_model, str):
                logger.warning(f"âš ï¸ deep_model ç±»å‹é”™è¯¯: {type(deep_model)}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                deep_model = "qwen-plus"

            logger.info(f"âœ… ä½¿ç”¨ç³»ç»Ÿé»˜è®¤æ¨¡å‹: quick={quick_model}, deep={deep_model}")
            return quick_model, deep_model
        except Exception as e:
            logger.error(f"âŒ è·å–é»˜è®¤æ¨¡å‹å¤±è´¥: {e}")
            return "qwen-turbo", "qwen-plus"

    def _recommend_model(self, model_type: str, min_level: int) -> str:
        """æ¨èæ»¡è¶³è¦æ±‚çš„æ¨¡å‹"""
        try:
            llm_configs = get_config_manager().get_llm_configs()
            for config in llm_configs:
                if (
                    config.enabled
                    and getattr(config, "capability_level", 2) >= min_level
                ):
                    display_name = config.model_display_name or config.model_name
                    return f"å»ºè®®ä½¿ç”¨: {display_name}"
        except Exception as e:
            logger.warning(f"æ¨èæ¨¡å‹å¤±è´¥: {e}")

        return "å»ºè®®å‡çº§æ¨¡å‹é…ç½®"


# å•ä¾‹
_model_capability_service = None


def get_model_capability_service() -> ModelCapabilityService:
    """è·å–æ¨¡å‹èƒ½åŠ›æœåŠ¡å•ä¾‹"""
    global _model_capability_service
    if _model_capability_service is None:
        _model_capability_service = ModelCapabilityService()
    return _model_capability_service
